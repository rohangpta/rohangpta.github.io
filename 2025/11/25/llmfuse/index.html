<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="Ronalds Vilcins - https://grohan.co"><title>Compressed Filesystems á la Language Models | Rohan Gupta</title><meta name=description content="Rohan Gupta's personal website"><meta name=twitter:card content="summary"><meta name=twitter:title content="Compressed Filesystems á la Language Models"><meta name=twitter:description content="Every systems engineer at some point in their journey yearns to write a filesystem. This sounds daunting at first - and writing a battle-tested filesystem is hard - but the minimal surface area for a &ldquo;working&rdquo; FS is surprisingly small, simple, and in-distribution for coding agents.
In fact, one of my smoke tests for new coding models is seeing how good of a filesystem they can one-shot! At some point, I had quite a few filesystems lying around - and coding models were getting pretty good - which made me wonder if the models were intelligent enough to actually model the filesystem engine itself?"><meta property="og:title" content="Compressed Filesystems á la Language Models"><meta property="og:description" content="Every systems engineer at some point in their journey yearns to write a filesystem. This sounds daunting at first - and writing a battle-tested filesystem is hard - but the minimal surface area for a &ldquo;working&rdquo; FS is surprisingly small, simple, and in-distribution for coding agents.
In fact, one of my smoke tests for new coding models is seeing how good of a filesystem they can one-shot! At some point, I had quite a few filesystems lying around - and coding models were getting pretty good - which made me wonder if the models were intelligent enough to actually model the filesystem engine itself?"><meta property="og:type" content="article"><meta property="og:url" content="https://grohan.co/2025/11/25/llmfuse/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-11-25T00:00:00+00:00"><meta property="article:modified_time" content="2025-11-25T00:00:00+00:00"><meta itemprop=name content="Compressed Filesystems á la Language Models"><meta itemprop=description content="Every systems engineer at some point in their journey yearns to write a filesystem. This sounds daunting at first - and writing a battle-tested filesystem is hard - but the minimal surface area for a &ldquo;working&rdquo; FS is surprisingly small, simple, and in-distribution for coding agents.
In fact, one of my smoke tests for new coding models is seeing how good of a filesystem they can one-shot! At some point, I had quite a few filesystems lying around - and coding models were getting pretty good - which made me wonder if the models were intelligent enough to actually model the filesystem engine itself?"><meta itemprop=datePublished content="2025-11-25T00:00:00+00:00"><meta itemprop=dateModified content="2025-11-25T00:00:00+00:00"><meta itemprop=wordCount content="2101"><meta itemprop=keywords content><link rel=canonical href=https://grohan.co/2025/11/25/llmfuse/><link rel=dns-prefetch href=https://www.google-analytics.com><link href=https://www.google-analytics.com rel=preconnect crossorigin><link rel=alternate type=application/atom+xml title="Rohan Gupta" href=https://grohan.coatom.xml><link rel=alternate type=application/json title="Rohan Gupta" href=https://grohan.cofeed.json><link rel=shortcuticon type=image/png href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNk+A8AAQUBAScY42YAAAAASUVORK5CYII="><script async src="https://www.googletagmanager.com/gtag/js?id=G-QPHTPNYLXE"></script>
<script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script>
<script>MathJax={tex:{displayMath:[["\\[","\\]"],["$$","$$"]],inlineMath:[["\\(","\\)"]]}}</script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-QPHTPNYLXE")</script><style>*,:after,:before{box-sizing:border-box;padding:0}body{font:1rem/1.5 Verdana,-apple-system,BlinkMacSystemFont,avenir next,avenir,helvetica,helvetica neue,ubuntu,roboto,noto,segoe ui,arial,sans-serif;text-rendering:optimizeSpeed}.posts hr{margin-top:0;margin-bottom:0;margin-right:.5rem;margin-left:.5rem;height:1px;border:none;border-bottom:1px #353535;flex:1 0 1rem}main{max-width:70ch;padding:2ch;margin:auto}a,body{color:#353535}::selection,a:focus,a:hover{background-color:#f0f0f0;color:#000}.meta{margin:0 0 2.5rem}.tags::before{content:"\2022";margin-left:1rem}code,pre{color:#353535;font-family:San Francisco Mono,Monaco,consolas,lucida console,dejavu sans mono,bitstream vera sans mono,monospace;font-size:normal;border:1px solid #353535;font-size:small}code{padding:.1rem;border:none}pre{padding:.5rem;overflow-x:auto}pre code{border:none}img{max-width:100%;border:1px solid #353535}hr{background:#353535;height:1px;border:0}ul{list-style-type:square}ul,ol{padding-left:1.2rem}header li,footer li{display:inline;text-transform:uppercase}header a,footer a{text-decoration:none}header ul,footer ul{justify-content:space-between;display:flex}[aria-current=page]{text-decoration:line-through}header,section,footer{padding:1rem 0}blockquote{border-left:5px solid #353535;padding-left:1rem}.posts ul,header ul,footer ul{list-style:none}.posts,header ul,footer ul{padding:0}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.7rem}.posts li a,.posts li div{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}.hash{opacity:.25;text-decoration:none}table{border-collapse:collapse;text-align:left;width:100%}table tr{background:#fff;border-bottom:1px solid}table th,table td{padding:10px 20px}</style><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","articleSection":"posts","name":"Compressed Filesystems á la Language Models","headline":"Compressed Filesystems á la Language Models","alternativeHeadline":"","description":"Every systems engineer at some point in their journey yearns to write a filesystem. This sounds daunting at first - and writing a battle-tested filesystem is hard - but the minimal surface area for a \u0026ldquo;working\u0026rdquo; FS is surprisingly small, simple, and in-distribution for coding agents.\nIn fact, one of my smoke tests for new coding models is seeing how good of a filesystem they can one-shot! At some point, I had quite a few filesystems lying around - and coding models were getting pretty good - which made me wonder if the models were intelligent enough to actually model the filesystem engine itself?","inLanguage":"en","isFamilyFriendly":"true","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/grohan.co\/2025\/11\/25\/llmfuse\/"},"author":{"@type":"Person","name":"Rohan Gupta"},"creator":{"@type":"Person","name":"Rohan Gupta"},"accountablePerson":{"@type":"Person","name":"Rohan Gupta"},"copyrightHolder":"Rohan Gupta","copyrightYear":"2025","dateCreated":"2025-11-25T00:00:00.00Z","datePublished":"2025-11-25T00:00:00.00Z","dateModified":"2025-11-25T00:00:00.00Z","publisher":{"@type":"Organization","name":"Rohan Gupta","url":"https://grohan.co","logo":{"@type":"ImageObject","url":"https:\/\/grohan.co\/images\/rohan.jpg","width":"32","height":"32"}},"image":"https://grohan.co/images/rohan.jpg","url":"https:\/\/grohan.co\/2025\/11\/25\/llmfuse\/","wordCount":"2101","genre":[],"keywords":[]}</script></head><body><main><header><nav><ul><li><a href=/posts aria-current=page>Writing</a></li><li><a href=/about>About</a></li><li><a href=/projects>Projects</a></li></ul></nav></header><hr><section><h2 itemprop="name headline">Compressed Filesystems á la Language Models</h2><p class=meta><time itemprop=datePublished datetime=2025-11-25>November 25, 2025</time> &bull;</p><span itemprop=articleBody><p>Every systems engineer at some point in their journey yearns to write
a filesystem. This sounds daunting at first - and writing a battle-tested filesystem <em>is</em> hard - but the minimal surface area for a &ldquo;working&rdquo; FS is surprisingly small, simple, and in-distribution for coding agents.</p><p>In fact, one of my smoke tests for new coding models is seeing how good of
a filesystem they can one-shot! At some point, I had quite a few filesystems lying around - and coding models were getting pretty good - which made me wonder if the models were intelligent enough to actually model the filesystem engine itself?</p><p>A filesystem is the perfect black-box API to model with wacky backends (see <a href=http://tom7.org/papers/murphy2022harder.pdf>&ldquo;Harder drives&rdquo;</a>), and besides the joy of training an LLM for fun - there were a few deeper truths about language models that I wanted to explore.</p><h1 id=training-a-filesystem>Training a filesystem <a href=#training-a-filesystem class=hash>#</a></h1><p>So I set upon training a filesystem. Building on top of one of my throwaway
FUSEs, a few rounds with Claude repurposed it to loopback against the host
with added logging, two things I needed to generate reference fine-tuning data:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span><span style=font-weight:700>class</span> <span style=font-weight:700>LoggingLoopbackFS</span>(LoggingMixIn, Operations):
</span></span><span style=display:flex><span>    <span style=font-style:italic>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=font-style:italic>    A loopback FUSE filesystem that logs all operations for training data.
</span></span></span><span style=display:flex><span><span style=font-style:italic>    
</span></span></span><span style=display:flex><span><span style=font-style:italic>    This implementation delegates all filesystem operations to a real directory
</span></span></span><span style=display:flex><span><span style=font-style:italic>    on the host filesystem, ensuring perfect semantic correctness while logging
</span></span></span><span style=display:flex><span><span style=font-style:italic>    every operation for LLM training data.
</span></span></span><span style=display:flex><span><span style=font-style:italic>    &#34;&#34;&#34;</span>
</span></span></code></pre></div><p>I then wrote a filesystem interaction simulator, which sampled various
operations against a sandboxed <code>LoggingLoopbackFS</code> to generate diverse FUSE
prompt/completion pairs. Concretely, I captured only the minimal set of <a href=https://github.com/rohangpta/llmfuse/blob/main/train/generate_data.py#L246>operations needed</a> for
R/W-ish capability (no open, xattrs, fsync etc).</p><p>Alongside the FUSE operation, I captured the full filesystem state at every
turn. I experimented with various formats, including an ASCII-art
representation, but ultimately settled on XML since it enforces prompt
boundaries clearly and had canonical parsers available.</p><p>With prompts including the FUSE operation + XML filesystem tree, the model learned two forms of completions:</p><ul><li>Reads (&lt;R>) requested the content / metadata as per the operation
(<code>getattr</code> / <code>readdir</code> / <code>read</code>)</li><li>Writes (&lt;W>) requested the model to output the full filesystem tree state,
after modification (<code>unlink</code> / <code>chmod</code> / <code>truncate</code> / <code>write</code>)</li></ul><p>Example prompt (read):</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback><span style=display:flex><span>&lt;R&gt;
</span></span><span style=display:flex><span>read(&#39;/usr14/log767.rs&#39;, size=4096, offset=0, fh=4) 
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>&lt;filesystem&gt;
</span></span><span style=display:flex><span>  &lt;directory path=&#34;/&#34; name=&#34;/&#34; mode=&#34;755&#34; owner=&#34;root&#34; group=&#34;root&#34;
</span></span><span style=display:flex><span>mtime=&#34;2025-01-01T00:00:00&#34;&gt;
</span></span><span style=display:flex><span>    &lt;directory path=&#34;usr14&#34; name=&#34;usr14&#34; mode=&#34;755&#34; owner=&#34;root&#34; group=&#34;root&#34;
</span></span><span style=display:flex><span>mtime=&#34;2025-01-01T00:00:00&#34;&gt;
</span></span><span style=display:flex><span>      &lt;file path=&#34;usr14/log767.rs&#34; name=&#34;log767.rs&#34; mode=&#34;644&#34; owner=&#34;root&#34;
</span></span><span style=display:flex><span>group=&#34;root&#34; mtime=&#34;2025-01-01T00:00:01&#34; size=&#34;276&#34;&gt;
</span></span><span style=display:flex><span>        &lt;body&gt;fn main() {
</span></span><span style=display:flex><span>    match process(7) {
</span></span><span style=display:flex><span>        Ok(result) =&amp;gt; println!(&#34;Result: {}&#34;, result),
</span></span><span style=display:flex><span>        Err(e) =&amp;gt; eprintln!(&#34;Error: {}&#34;, e),
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>&lt;/body&gt;
</span></span><span style=display:flex><span>      &lt;/file&gt;
</span></span><span style=display:flex><span>      &lt;file path=&#34;usr14/temp912.sh&#34; name=&#34;temp912.sh&#34; mode=&#34;644&#34; owner=&#34;root&#34;
</span></span><span style=display:flex><span>group=&#34;root&#34; mtime=&#34;2025-01-01T00:00:01&#34; size=&#34;268&#34;&gt;
</span></span><span style=display:flex><span>        &lt;body&gt;#!/bin/bash 
</span></span><span style=display:flex><span>         echo &#34;temp912&#34; || exit 1
</span></span><span style=display:flex><span>       &lt;/body&gt;
</span></span><span style=display:flex><span>      &lt;/file&gt;
</span></span><span style=display:flex><span>    &lt;/directory&gt;
</span></span><span style=display:flex><span>  &lt;/directory&gt;
</span></span><span style=display:flex><span>&lt;/filesystem&gt;
</span></span></code></pre></div><p>Completion:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-rs data-lang=rs><span style=display:flex><span><span style=font-weight:700>fn</span> main() {
</span></span><span style=display:flex><span>    <span style=font-weight:700>match</span> process(7) {
</span></span><span style=display:flex><span>        Ok(result) =&gt; println!(<span style=font-style:italic>&#34;Result: </span><span style=font-weight:700;font-style:italic>{}</span><span style=font-style:italic>&#34;</span>, result),
</span></span><span style=display:flex><span>        Err(e) =&gt; eprintln!(<span style=font-style:italic>&#34;Error: </span><span style=font-weight:700;font-style:italic>{}</span><span style=font-style:italic>&#34;</span>, e),
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><h3 id=fine-tuning>Fine-tuning <a href=#fine-tuning class=hash>#</a></h3><p>Once I had clean, representative, and diverse filesystem simulation data, actually running SFT was pretty straightforward on Modal. Over a few iteration cycles spread across nibbles of spare time, I ended up with ~98% accuracy on a hold-out eval after 8 epochs of SFT on a N=15000 dataset with Qwen3-4b.</p><p>Most of my time here was spent cleaning generated data and ensuring we represented every FUSE operation sufficiently + generated enough &ldquo;complex&rdquo; trees to learn on.</p><p>At this point, I wrote &mldr; possibly the smallest filesystem I&rsquo;ve seen&mldr; to give my model a spin in
the real world. Every FUSE operation was a passthrough to the LLM, for example:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span><span style=font-weight:700>class</span> <span style=font-weight:700>LLMFuse</span>(LoggingMixin, Operations):
</span></span><span style=display:flex><span>    ...
</span></span><span style=display:flex><span>    <span style=font-weight:700>def</span> chmod(self, path, mode):
</span></span><span style=display:flex><span>        <span style=font-style:italic>&#34;&#34;&#34;Change file permissions.&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>        response = self._query_llm_for_operation(<span style=font-style:italic>&#39;chmod&#39;</span>, path, mode=oct(mode))
</span></span><span style=display:flex><span>        <span style=font-weight:700>if</span> <span style=font-weight:700>not</span> self._handle_llm_response(response):
</span></span><span style=display:flex><span>            <span style=font-weight:700>raise</span> FuseOSError(ENOENT)
</span></span><span style=display:flex><span>        <span style=font-weight:700>return</span> 0
</span></span><span style=display:flex><span>    ...
</span></span></code></pre></div><p>Nice! I now had a mountable FUSE that was entirely &ldquo;implemented&rdquo; by a language
model. As you can see below, I was able to <code>ls</code> around it, <code>echo</code> into files, and <code>cat</code> them back out.</p><center><img src=/images/llmfuse_example.png width=800 height=250>
Poking around a Docker container with a mounted LLMFuse.</center><h1 id=compressing-the-filesystem>Compressing the filesystem <a href=#compressing-the-filesystem class=hash>#</a></h1><p>Perhaps the largest glaring inefficiency in this set up is the sheer verbosity
of the XML-based representation. I was using many bytes to represent attributes
and tree structure that could be encoded far more efficiently (~O(bits)) in a standard
C struct.</p><p>However, as I was fine-tuning on the XML filesystem tree representation, I was
baking in this very structure into the weights and probability distributions of my Qwen fork! If only there was a way to leverage this to compress state&mldr;</p><h2 id=two-sides-of-the-same-coin>Two sides of the same coin <a href=#two-sides-of-the-same-coin class=hash>#</a></h2><p>As it turns out, compression and AI are intimately related. Using LLMs to lossily
compress text is one of the most common applications, so it&rsquo;s not entirely
unintuitive. However, one researcher (Marcus Hutter) claimed back in 2006 that they are <em>equivalent</em> (and in fact <a href=http://prize.hutter1.net/hfaq.htm#about>bet $500K on this claim!</a>).</p><p>Presciently, Hutter appears to be absolutely right. His <code>enwik8</code> and <code>enwik9</code>&rsquo;s benchmark datasets are, today, best compressed by a <a href=https://bellard.org/ts_zip/>169M parameter LLM</a> (trained by none other than Fabrice Bellard in 2023).</p><p>That&rsquo;s a bit perplexing on the first glance. Surely LLM compression isn&rsquo;t reversible? What kind of voodoo magic was going on here?</p><h3 id=arithmetic-coding>Arithmetic coding <a href=#arithmetic-coding class=hash>#</a></h3><p>The algorithm that enables reversible compression using LLMs is called &ldquo;arithmetic coding&rdquo; and it builds upon a <a href=https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf>1948 result by Claude Shannon</a>.</p><p>Researchers at DeepMind (including Hutter himself) have <a href=https://arxiv.org/pdf/2309.10668>explained the math in
detail</a>, so I&rsquo;ll direct the most inquisitive of you readers there, but for a simplified understanding of what&rsquo;s going on, forget everything you might know about working with LLMs today. There&rsquo;s no prompting involved!</p><center><img src=/images/ac.png width=800 height=300></center><p>Let&rsquo;s assume the following is true for some predictive model \(M\)</p><ul><li>Lorem has first-word probability = 0.57.</li><li>Ipsum has second-word conditional probability = 0.67 (joint 0.38).</li><li>Dolor has a third word conditional probability = 0.5 (joint 0.19).</li></ul><p>&mldr;</p><p>so on and so forth until you reach the end of the string you want to compress and you end up with some &ldquo;final interval width&rdquo; \(P(m)\) on the real interval \([0,1]\) which represents your string.</p><p>Let&rsquo;s suppose in our example this turns out to be 0.012. We can represent this decimal in roughly \(- \log_{2}{P(m)} = 6.4\) bits, which is our final compression size.</p><p>There&rsquo;s a few elegant things about this algorithm:</p><ul><li><em>Any</em> number within this interval is uniquely determined by tracing the arithmetic coding algorithm through the specific probabilistic model&rsquo;s weights. &ldquo;Decoding&rdquo; is simply a retracing operation (see the line through the probability distributions above)</li><li>The inverse log relationship between predictive power \(P(m)\) and compression
pushes the burden of the &ldquo;hard compression problem&rdquo; to deep learning machinery which can encode high-dimensional text patterns within model weights, yielding far better compression ratios than deterministic algorithms.</li></ul><p>Sounds cool! But <strong>how good really</strong> is this compression? On comparing
arithmetic coding backed by <code>Qwen3-4B</code> against <code>gzip</code> for <a href=https://www.lipsum.com/><code>lipsum.txt</code></a>,
we already see pretty dramatic results:</p><table><thead><tr><th>Method</th><th>Size (bytes)</th><th>Compression Impact</th></tr></thead><tbody><tr><td>Original (plain)</td><td>446</td><td>—</td></tr><tr><td><code>gzip</code></td><td>298</td><td>~33% smaller</td></tr><tr><td><code>llmencode</code></td><td>13</td><td>~97% smaller</td></tr></tbody></table><p>(note: <a href=https://github.com/rohangpta/llmfuse/blob/main/llmencode/llmencode.py><code>llmencode</code></a> is my implementation of arithmetic coding)</p><p>22x better compression than <code>gzip</code> is pretty ridiculous! A caveat here is that <code>lipsum.txt</code> is heavily represented in training data, but 5-20x efficiency gains broadly hold for all text data that (looks like) it&rsquo;s been on the internet.</p><h2 id=self-compression>Self-compression <a href=#self-compression class=hash>#</a></h2><p>Now, back to our filesystem. The XML overhead we were worried about now can be
&ldquo;compressed away&rdquo; by the fine-tuned model. Using the same toy filesystem from
the Docker container demo above:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-xml data-lang=xml><span style=display:flex><span><span style=font-weight:700>&lt;filesystem&gt;</span>
</span></span><span style=display:flex><span>  <span style=font-weight:700>&lt;directory</span> path=<span style=font-style:italic>&#34;/&#34;</span> name=<span style=font-style:italic>&#34;/&#34;</span> mode=<span style=font-style:italic>&#34;755&#34;</span> owner=<span style=font-style:italic>&#34;root&#34;</span> group=<span style=font-style:italic>&#34;root&#34;</span> mtime=<span style=font-style:italic>&#34;2025-01-01T00:00:00&#34;</span><span style=font-weight:700>&gt;</span>
</span></span><span style=display:flex><span>    <span style=font-weight:700>&lt;directory</span> path=<span style=font-style:italic>&#34;testdir&#34;</span> name=<span style=font-style:italic>&#34;testdir&#34;</span> mode=<span style=font-style:italic>&#34;755&#34;</span> owner=<span style=font-style:italic>&#34;root&#34;</span> group=<span style=font-style:italic>&#34;root&#34;</span> mtime=<span style=font-style:italic>&#34;2025-01-01T00:00:00&#34;</span> <span style=font-weight:700>/&gt;</span>
</span></span><span style=display:flex><span>    <span style=font-weight:700>&lt;file</span> path=<span style=font-style:italic>&#34;testfile.txt&#34;</span> name=<span style=font-style:italic>&#34;testfile.txt&#34;</span> mode=<span style=font-style:italic>&#34;644&#34;</span> owner=<span style=font-style:italic>&#34;root&#34;</span> group=<span style=font-style:italic>&#34;root&#34;</span> mtime=<span style=font-style:italic>&#34;2025-01-01T00:00:01&#34;</span> size=<span style=font-style:italic>&#34;14&#34;</span><span style=font-weight:700>&gt;</span>
</span></span><span style=display:flex><span>      <span style=font-weight:700>&lt;body&gt;</span>hello llmfuse
</span></span><span style=display:flex><span><span style=font-weight:700>&lt;/body&gt;</span>
</span></span><span style=display:flex><span>    <span style=font-weight:700>&lt;/file&gt;</span>
</span></span><span style=display:flex><span>  <span style=font-weight:700>&lt;/directory&gt;</span>
</span></span><span style=display:flex><span><span style=font-weight:700>&lt;/filesystem&gt;</span>
</span></span></code></pre></div><table><thead><tr><th>Model</th><th>Original (bytes)</th><th>Compressed (bytes)</th><th>Ratio</th></tr></thead><tbody><tr><td>Base Qwen3-4B</td><td>394</td><td>38</td><td>10.4x</td></tr><tr><td>Fine-tuned Qwen3-4B</td><td>394</td><td>21</td><td>18.8x</td></tr></tbody></table><p>The fine-tuned model achieves <strong>44.7% better compression</strong> on XML filesystem
trees - the very format it was trained to predict. This is the &ldquo;self-compression&rdquo;
effect: by baking the XML structure into the model weights during fine-tuning,
the arithmetic coder can represent that structure in fewer bits.</p><p>Self-compression in filesystems isn&rsquo;t a novel concept. For example, there exists the
<a href=https://docs.kernel.org/filesystems/squashfs.html><code>squashfs</code></a> tool (created in 2002) to create R/O compressed filesystems. Squashfs compresses
files, inodes, and directories together, not unlike what we&rsquo;re doing here!</p><p>Under the hood, <code>squashfs</code> just wraps <code>gzip</code>/<code>zstd</code>/your favourite compression
algorithm. So for plain-text data, <code>squashfs</code> compression stats pale in the face of <code>llmfuse</code>:</p><table><thead><tr><th>Method</th><th>Compressed Size</th><th>Notes</th></tr></thead><tbody><tr><td>squashfs (gzip)</td><td>171 bytes</td><td>gzip-compressed file contents, inodes, directory tables</td></tr><tr><td>llmfuse (fine-tuned)</td><td>21 bytes</td><td>Arithmetic coded XML state</td></tr></tbody></table><p>For the same filesystem tree (one directory, one 14-byte text file), llmfuse
achieves <strong>~8x better compression</strong> than squashfs (see methodology in appendix).</p><p>The difference comes down to <code>llmencode</code> being far better than <code>gzip</code> on
text data + XML structure - especially when the model has been fine-tuned on exactly
that structure.</p><h1 id=conclusion>Conclusion <a href=#conclusion class=hash>#</a></h1><p>What started off as a little experiment mostly to get my hands dirty with
training and inference evolved into a full blown <a href=https://xkcd.com/356/>nerd
snipe</a> and intellectual adventure. Thanks for making it
this far!</p><p>I entirely recognize that this is a &ldquo;toy&rdquo;
experiment under a very specific setup; with that said, the numbers above are pretty eye-popping, and the question I&rsquo;ve been trying to answer as I write this up is: does this have any real-world potential?</p><p>Of course, in the short term, there&rsquo;s a whole host of caveats: you need an
LLM, likely a GPU, all your data is in the context window (which we know scales
poorly), and this only works on text data.</p><p>Still, it&rsquo;s intriguing to wonder whether the very engines that will likely
dominate all &ldquo;text generation&rdquo; going forward can be used to compress their own
data? Perhaps in a distant future, where running LLMs at the edge makes sense, or for specific kinds of workflows where data is read very infrequently.</p><p>Overall, I&rsquo;m grateful to Peyton at <a href=https://modal.com>Modal</a> for the compute credits. Running
a somewhat unconventional experiment like this wouldn&rsquo;t have been possible
without full control over the training and inference code, and extremely
tedious without the simplicity of running ML infra on Modal! It&rsquo;s truly awesome
to be able to just <code>modal deploy</code> and get my own private inference endpoints,
or just <code>modal run</code> to prototype some code on the cloud.</p><h1 id=appendix>Appendix <a href=#appendix class=hash>#</a></h1><h2 id=source-code>Source Code <a href=#source-code class=hash>#</a></h2><p>All of the source code for this experiment, particularly <code>llmfuse</code> and
<code>llmencode</code> are <a href=https://github.com/rohangpta/llmfuse>open-sourced</a> under MIT.</p><p><code>llmencode</code> is abstracted into a CLI utility that you can run locally.
Inference on 4B models is slow, but entirely possible on consumer hardware.
I prototyped most of this code by running on a 2021 MacBook Pro, before
productionizing on Modal.</p><p>A fun experiment / party trick to identify how &ldquo;common&rdquo; a certain
string is in training data is to look at its <code>llmencode</code> compression ratio!</p><h2 id=squashfs-comparison-methodology>SquashFS comparison methodology <a href=#squashfs-comparison-methodology class=hash>#</a></h2><p>The raw <code>.sqsh</code> file is 4096 bytes due to block alignment padding. To find the
actual compressed size, I used <code>xxd</code> to inspect the binary and found the last
non-zero byte at offset 266 (267 bytes total). Subtracting the fixed 96-byte
superblock header gives us 171 bytes of actual gzip-compressed content -
everything needed to reconstruct the filesystem.</p><h2 id=compression-as-a-metric>Compression as a metric <a href=#compression-as-a-metric class=hash>#</a></h2><p>It&rsquo;s equally interesting to think about compression as a metric. An angle I&rsquo;d
considered is doing some kind of RL on the arithmetic coded compression number itself.</p><p><del>Is that simply equivalent to the pre-training objective (due to the prediction-compression duality)? Or does the &ldquo;sequence-level&rdquo; objective add something more&mldr; interesting to the mix. Please reach out if you have thoughts!</del></p><p><strong>EDIT:</strong></p><p>As it turns out - it is indeed equivalent to the pre-training objective!</p><p><strong>Pre-training</strong> aims to maximize the probability of training data. For a sequence \(x = (x_1, x_2, \ldots, x_T)\), we decompose via the chain rule:</p><div>$$P(x) = \prod_{t=1}^{T} P(x_t \mid x_{< t})$$</div><p>Taking logarithms (for numerical stability and to convert products to sums):</p><div>$$\log P(x) = \sum_{t=1}^{T} \log P(x_t \mid x_{< t})$$</div><p>Maximizing log-probability is equivalent to minimizing its negative — the <strong>negative log-likelihood</strong>:</p><div>$$\mathcal{L}_{\text{NLL}}(\theta) = -\sum_{t=1}^{T} \log P_\theta(x_t \mid x_{< t})$$</div><p><strong>Arithmetic coding</strong> maps the entire sequence \(x\) to an interval on \([0,1]\) of width equal to its joint probability \(P(x)\). To uniquely specify a point within an interval of width \(P(x)\), we require \(-\log_2 P(x)\) bits.</p><p>Expanding this using the chain rule from above gives us the sum:</p><p>$$L_{\text{compressed}}(x) = -\log_2 P(x) = -\log_2 \prod_{t=1}^{T} P_\theta(x_t \mid x_{&lt; t}) = -\sum_{t=1}^{T} \log_2 P_\theta(x_t \mid x_{&lt; t})$$</p><p>These differ only by logarithm base. Since \(\log_2 P = \frac{\ln P}{\ln 2}\), and \(\frac{1}{\ln 2} \approx 1.44\) is a positive constant:</p><div>$$\arg\min_\theta \mathcal{L}_{\text{NLL}} = \arg\min_\theta L_{\text{compressed}}$$</div><p>The same \(\theta^*\) minimizes both. \(\square\)</p><p>Beyond resolving the RL question, there&rsquo;s something quietly beautiful about this framing. &ldquo;minimize negative log-likelihood&rdquo; and &ldquo;minimize cross-entropy&rdquo; are abstractions that require complex statistical machinery to parse (to grok the &ldquo;pre-training objective&rdquo;). But compression is simple, and simple is better than complex.</p></span></section><hr><footer><nav><ul><li>© 2026</li></ul></nav></footer></main></body></html>