<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="Ronalds Vilcins - https://grohan.co"><title>Reverse Engineering Claude's Token Counter | Rohan Gupta</title><meta name=description content="How I reverse-engineered a large Claude vocabulary from count_tokens and built a fast local token estimator."><meta name=twitter:card content="summary"><meta name=twitter:title content="Reverse Engineering Claude's Token Counter"><meta name=twitter:description content="How I reverse-engineered a large Claude vocabulary from count_tokens and built a fast local token estimator."><meta property="og:title" content="Reverse Engineering Claude's Token Counter"><meta property="og:description" content="How I reverse-engineered a large Claude vocabulary from count_tokens and built a fast local token estimator."><meta property="og:type" content="article"><meta property="og:url" content="https://grohan.co/2026/02/10/ctoc/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2026-02-10T00:00:00+00:00"><meta property="article:modified_time" content="2026-02-10T00:00:00+00:00"><meta itemprop=name content="Reverse Engineering Claude's Token Counter"><meta itemprop=description content="How I reverse-engineered a large Claude vocabulary from count_tokens and built a fast local token estimator."><meta itemprop=datePublished content="2026-02-10T00:00:00+00:00"><meta itemprop=dateModified content="2026-02-10T00:00:00+00:00"><meta itemprop=wordCount content="1346"><meta itemprop=keywords content="coding agents,nlp,"><link rel=canonical href=https://grohan.co/2026/02/10/ctoc/><link rel=dns-prefetch href=https://www.google-analytics.com><link href=https://www.google-analytics.com rel=preconnect crossorigin><link rel=alternate type=application/atom+xml title="Rohan Gupta" href=https://grohan.coatom.xml><link rel=alternate type=application/json title="Rohan Gupta" href=https://grohan.cofeed.json><link rel=shortcuticon type=image/png href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNk+A8AAQUBAScY42YAAAAASUVORK5CYII="><script async src="https://www.googletagmanager.com/gtag/js?id=G-QPHTPNYLXE"></script>
<script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script>
<script>MathJax={tex:{displayMath:[["\\[","\\]"],["$$","$$"]],inlineMath:[["\\(","\\)"]]}}</script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-QPHTPNYLXE")</script><style>*,:after,:before{box-sizing:border-box;padding:0}body{font:1rem/1.5 Verdana,-apple-system,BlinkMacSystemFont,avenir next,avenir,helvetica,helvetica neue,ubuntu,roboto,noto,segoe ui,arial,sans-serif;text-rendering:optimizeSpeed}.posts hr{margin-top:0;margin-bottom:0;margin-right:.5rem;margin-left:.5rem;height:1px;border:none;border-bottom:1px #353535;flex:1 0 1rem}main{max-width:70ch;padding:2ch;margin:auto}a,body{color:#353535}::selection,a:focus,a:hover{background-color:#f0f0f0;color:#000}.meta{margin:0 0 2.5rem}.tags::before{content:"\2022";margin-left:1rem}code,pre{color:#353535;font-family:San Francisco Mono,Monaco,consolas,lucida console,dejavu sans mono,bitstream vera sans mono,monospace;font-size:normal;border:1px solid #353535;font-size:small}code{padding:.1rem;border:none}pre{padding:.5rem;overflow-x:auto}pre code{border:none}img{max-width:100%;border:1px solid #353535}hr{background:#353535;height:1px;border:0}ul{list-style-type:square}ul,ol{padding-left:1.2rem}header li,footer li{display:inline;text-transform:uppercase}header a,footer a{text-decoration:none}header ul,footer ul{justify-content:space-between;display:flex}[aria-current=page]{text-decoration:line-through}header,section,footer{padding:1rem 0}blockquote{border-left:5px solid #353535;padding-left:1rem}.posts ul,header ul,footer ul{list-style:none}.posts,header ul,footer ul{padding:0}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.7rem}.posts li a,.posts li div{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}.hash{opacity:.25;text-decoration:none}table{border-collapse:collapse;text-align:left;width:100%}table tr{background:#fff;border-bottom:1px solid}table th,table td{padding:10px 20px}</style><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","articleSection":"posts","name":"Reverse Engineering Claude\u0027s Token Counter","headline":"Reverse Engineering Claude\u0027s Token Counter","alternativeHeadline":"","description":"How I reverse-engineered a large Claude vocabulary from count_tokens and built a fast local token estimator.","inLanguage":"en","isFamilyFriendly":"true","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/grohan.co\/2026\/02\/10\/ctoc\/"},"author":{"@type":"Person","name":"Rohan Gupta"},"creator":{"@type":"Person","name":"Rohan Gupta"},"accountablePerson":{"@type":"Person","name":"Rohan Gupta"},"copyrightHolder":"Rohan Gupta","copyrightYear":"2026","dateCreated":"2026-02-10T00:00:00.00Z","datePublished":"2026-02-10T00:00:00.00Z","dateModified":"2026-02-10T00:00:00.00Z","publisher":{"@type":"Organization","name":"Rohan Gupta","url":"https://grohan.co","logo":{"@type":"ImageObject","url":"https:\/\/grohan.co\/images\/rohan.jpg","width":"32","height":"32"}},"image":"https://grohan.co/images/rohan.jpg","url":"https:\/\/grohan.co\/2026\/02\/10\/ctoc\/","wordCount":"1346","genre":["coding agents","nlp"],"keywords":["coding agents","nlp"]}</script></head><body><main><header><nav><ul><li><a href=/posts aria-current=page>Writing</a></li><li><a href=/about>About</a></li><li><a href=/projects>Projects</a></li></ul></nav></header><hr><section><h2 itemprop="name headline">Reverse Engineering Claude's Token Counter</h2><p class=meta><time itemprop=datePublished datetime=2026-02-10>February 10, 2026</time> &bull;
<a href=/tags/coding-agents>coding agents</a>, <a href=/tags/nlp>nlp</a></p><span itemprop=articleBody><p>Claude 3+ doesn&rsquo;t ship with an open tokenizer.</p><p>If you&rsquo;re building coding agents, that&rsquo;s a practical problem. You either call Anthropic&rsquo;s <a href=https://docs.anthropic.com/en/api/messages-count-tokens><code>count_tokens</code></a> for everything (slow, online, awkward) or use a proxy estimator (<code>tiktoken</code>, <code>wc -c // {3,4}</code>) and accept large systematic error - up to 20–50% for Claude models. As agents increasingly self-manage context, they need a fast, local way to see token usage across a directory — <a href=https://github.com/AlDanial/cloc><code>cloc</code></a> (count lines of code), but for tokens.</p><p>So I reverse-engineered a large chunk of Claude&rsquo;s vocabulary from the <code>count_tokens</code> API and built <a href=https://github.com/rohangpta/ctoc/tree/main><code>ctoc</code></a>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback><span style=display:flex><span>➜  ctoc git:(main) bazel-bin/ctoc --by-file .
</span></span><span style=display:flex><span>--------------------------------------------------------------------------------
</span></span><span style=display:flex><span>File                                Ext                tokens
</span></span><span style=display:flex><span>--------------------------------------------------------------------------------
</span></span><span style=display:flex><span>./MODULE.bazel.lock                 .lock              22,906
</span></span><span style=display:flex><span>./ctoc.cc                           .cc                 4,870
</span></span><span style=display:flex><span>./REPORT.md                         .md                 4,290
</span></span><span style=display:flex><span>./gen_vocab.py                      .py                   962
</span></span><span style=display:flex><span>./README.md                         .md                   473
</span></span><span style=display:flex><span>./BUILD.bazel                       .bazel                246
</span></span><span style=display:flex><span>./MODULE.bazel                      .bazel                153
</span></span><span style=display:flex><span>./020020-huh-isk                    .dat                   22
</span></span><span style=display:flex><span>--------------------------------------------------------------------------------
</span></span><span style=display:flex><span>SUM (8 files)                                          33,922
</span></span><span style=display:flex><span>--------------------------------------------------------------------------------
</span></span></code></pre></div><p><code>ctoc</code> is an offline estimator of Claude 4.x&rsquo;s <code>count_tokens()</code> API that lands at ~96% accuracy. It&rsquo;s backed by a 36,495-token verified vocabulary (greedy tokenizer), and the code/vocabulary are <a href=https://github.com/rohangpta/ctoc/tree/main>open source</a>.</p><h2 id=a-primer-on-bpe-tokenization>A primer on BPE tokenization <a href=#a-primer-on-bpe-tokenization class=hash>#</a></h2><p>Tokenizers these days mostly use the <a href=https://en.wikipedia.org/wiki/Byte-pair_encoding>BPE algorithm</a>. BPE is &ldquo;trained&rdquo; by using a fixed/private corpus of data to create a table of merge rules of frequent tokens.</p><p>Say the merge table (in priority order) is:</p><ol><li>t + h → th</li><li>th + e → the</li><li>i + s → is</li></ol><p>Tokenizing the input &ldquo;this&rdquo;:</p><ul><li>Start: [t, h, i, s]</li><li>Rule 1: [th, i, s] — merged t+h</li><li>Rule 2: no &ldquo;th&rdquo;+&ldquo;e&rdquo; pair — skip</li><li>Rule 3: [th, is] — merged i+s</li><li>Done: [&ldquo;th&rdquo;, &ldquo;is&rdquo;]</li></ul><p>That&rsquo;s a toy intuition, not a full encoder spec: practical BPE implementations track merge priorities over adjacent pairs as well.</p><p>Now I&rsquo;m not <em>certain</em> Anthropic uses BPE, but given that most of the industry has converged on it, it&rsquo;s a reasonable bet.</p><p>This means that without this &ldquo;merge table&rdquo; or private corpus we can&rsquo;t <em>exactly</em> re-create the tokenization.</p><p>But if we can recover the vocabulary, we don&rsquo;t need the merge table — a greedy longest-match over the known tokens should approximate BPE&rsquo;s token counts closely enough.</p><h2 id=probing-count_tokens-to-identify-vocabulary>Probing count_tokens to identify vocabulary <a href=#probing-count_tokens-to-identify-vocabulary class=hash>#</a></h2><p>We need to do two things to reverse engineer Claude&rsquo;s vocabulary: verify whether a candidate string is a single token, and decompose longer strings to extract tokens we haven&rsquo;t seen yet.</p><h3 id=single-token-verification>Single-token verification <a href=#single-token-verification class=hash>#</a></h3><p>The naive check — send a candidate to <code>count_tokens</code>, see if you get 1 — doesn&rsquo;t work because the API wraps inputs in chat framing. Raw counts include a roughly constant overhead that varies by the type of the first character (7, 8, 9+ for letters, digits, Unicode respectively - what&rsquo;s going on here, Anthropic?).</p><p>Sandwich counting fixes this: wrap every probe between markers that we know are single tokens, and subtract the known baseline.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=font-weight:700>def</span> count_tokens(text: str) -&gt; int:
</span></span><span style=display:flex><span>    marker = <span style=font-style:italic>&#34;</span><span style=font-weight:700;font-style:italic>\u00A7</span><span style=font-style:italic>&#34;</span>
</span></span><span style=display:flex><span>    base = raw_count(marker + marker)
</span></span><span style=display:flex><span>    <span style=font-weight:700>return</span> raw_count(marker + text + marker) - base
</span></span></code></pre></div><p>With this, <code>count_tokens(candidate) == 1</code> reliably tells us whether a candidate is a single token.</p><h3 id=decomposing-strings-into-tokens>Decomposing strings into tokens <a href=#decomposing-strings-into-tokens class=hash>#</a></h3><p>Verification handles candidates we already suspect are single tokens. But when we encounter a longer string - from a dataset, from another tokenizer&rsquo;s vocab - we need a way to extract the individual tokens inside it.</p><p>The approach: scan for a position <code>i</code> where <code>count(s[:i]) == 1</code> or <code>count(s[i:]) == 1</code>. That peels off one confirmed token from either end. Recurse on the remainder until the string is fully decomposed (or you hit a chunk that can&rsquo;t be broken down further).</p><p>So we have the machinery: sandwich counting to verify single tokens, iterative peeling to decompose arbitrary strings. But decompose <em>what</em>?</p><p>Brute-forcing all possible byte strings of length 1-64 is astronomically large, and our decomposition is linear per string, so even clever generation strategies hit the API wall fast (2000/min rate limit). Early experiments with generated data and public datasets proved to be intractable very quickly, though we did pull out a few confirmed tokens as proof of life.</p><h3 id=cross-tokenizer-mining>Cross-tokenizer mining <a href=#cross-tokenizer-mining class=hash>#</a></h3><p>I needed a way to narrow the search space to something that would run in days instead of centuries.</p><p>After some back-and-forth with Claude, I arrived on cross-tokenizer mining: decode vocab items from existing BPE tokenizers (tiktoken, HuggingFace models), test each against Claude with sandwich counting, keep only single-token hits.</p><p>Tokenizers trained on overlapping internet/programming corpora converge on many of the same subwords. This isn&rsquo;t surprising if you think about it from a compression angle, and the Anthropic-specific tokens are likely not dominant in the distribution.</p><table><thead><tr><th>Source</th><th>Vocab size</th><th>Hit rate</th><th>New tokens found</th></tr></thead><tbody><tr><td>tiktoken cl100k_base (GPT-4)</td><td>100K</td><td>~46%</td><td>~29,000</td></tr><tr><td>tiktoken o200k_base (GPT-4o)</td><td>200K</td><td>~15%</td><td>~4,000</td></tr><tr><td>11 HuggingFace US + China models</td><td>150-250K each</td><td>2-74%</td><td>~3,800</td></tr></tbody></table><p>Sorting multilingual candidates by cross-tokenizer frequency (tokens appearing in more tokenizers checked first) gave a 74% hit rate on the first 1,000 candidates, declining to ~2% by 20K.</p><h3 id=long-tail>Long tail <a href=#long-tail class=hash>#</a></h3><p>The long-tail vocabulary recoveries here were interesting:</p><ul><li>Re-checking all digit-starting candidates recovered ~1,006 tokens — almost all 3-digit numbers (<code>916</code>, <code>030</code>, <code>271</code>) are single tokens.</li><li>BPE creates single tokens for specific lengths of repeated characters (<code>=</code>, <code>-</code>, etc.) up to length 64 with non-monotonic patterns: <code>"=" * 7</code> is 2 tokens but <code>"=" * 8</code> is 1 (likely a BPE merge-order artifact — these lengths matter for markdown fences and separator lines). Space sequences 1-16, tab sequences 1-4, and newline variants are each single tokens - critical for code indentation accuracy (and token efficiency!)</li></ul><h2 id=results>Results <a href=#results class=hash>#</a></h2><p>The full extraction took 3 days and ~277K API calls.</p><h3 id=estimator-quality-greedy-longest-match>Estimator quality (greedy longest-match) <a href=#estimator-quality-greedy-longest-match class=hash>#</a></h3><table><thead><tr><th>Corpus</th><th style=text-align:right>Efficiency ratio (<code>API tokens / greedy tokens</code>)</th></tr></thead><tbody><tr><td>Python source (9 files)</td><td style=text-align:right>96.1%</td></tr><tr><td>Mixed code + docs (9 files)</td><td style=text-align:right>95.1%</td></tr><tr><td>English prose (5 samples)</td><td style=text-align:right>99.2%</td></tr></tbody></table><p>Per-file variance is low: individual files land within ~2% of their corpus mean, and the estimator consistently over-segments (predicts more tokens than API), which is desirable for conservative context budgeting (read the Appendix to understand <em>why</em> we get this close).</p><h3 id=conclusion>Conclusion <a href=#conclusion class=hash>#</a></h3><p>The final outcome of this project - <code>ctoc</code> - is deliberately boring software that just runs a greedy tokenization algorithm on top of our discovered vocabulary. Unknown bytes fall back to 1 token. Fast enough to run as a preflight check in local workflows, or for a coding agent to run as a subprocess.</p><p>If you care about exact counts for billing-critical paths, use Anthropic&rsquo;s API. If you care about fast, parallelizable, local (self)context management, this could be good enough (and certainly has scope to close the 3-4% gap).</p><p>As always, this depends on current (tested with Claude 4.x) API behavior and can drift if Anthropic changes tokenizer internals.</p><hr><h2 id=appendix>Appendix <a href=#appendix class=hash>#</a></h2><h3 id=why-does-greedy-tokenization-do-well>Why does greedy tokenization do well? <a href=#why-does-greedy-tokenization-do-well class=hash>#</a></h3><p>What turned out to be empirically true but not obvious - greedy longest-match and BPE merge-order produce <em>different segmentations</em>. Why do the token <em>counts</em> nearly converge?</p><p>There&rsquo;s a reasonable theoretical picture:</p><p>BPE vocabularies are built for greedy compression. <a href=https://aclanthology.org/2023.findings-acl.38/>Zouhar et al. (2023)</a> formalise BPE training as greedy submodular maximisation of a compression utility. The vocabulary is constructed bottom-up: every multi-character token was formed by merging two shorter tokens that are also in the vocabulary. This hierarchical structure means the longest token at any position tends to be the one BPE would also select.</p><p>Byte-level completeness prevents dead ends. BPE vocabularies always include single-byte fallback tokens, so a left-to-right greedy pass can never paint itself into a corner. This eliminates the primary failure mode of greedy algorithms on arbitrary dictionaries.</p><p>BPE training also has an implicit left-to-right bias. <a href=https://aclanthology.org/2025.emnlp-main.1775/>Sawada & Goyal (2025)</a> evaluate merge-list-free left-to-right greedy encoding and find it is often comparable to standard merge-based tokenization, with improvements on some tasks and modest degradations on others.</p><p>And where greedy and BPE do disagree, the result is typically a rearrangement of boundaries, not a net increase in tokens. <code>['hell', 'ooo']</code> vs <code>['hello', 'oo']</code> — different boundaries, same count.</p><p>The upshot: for BPE-family tokenizers with byte fallback, greedy and merge-order counting often converge closely in practice when vocabulary coverage is high. The 4-5% gap in <code>ctoc</code> is plausibly missing tokens forcing over-segmentation. If the vocabulary were complete, greedy counting would likely be even closer to API counts.</p></span></section><hr><footer><nav><ul><li>© 2026</li></ul></nav></footer></main></body></html>