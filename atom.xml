<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Rohan Gupta</title><description>Rohan Gupta's personal website</description><link>https://grohan.co</link><language>en</language><copyright>Copyright 2026, Ronalds Vilcins</copyright><lastBuildDate>Tue, 10 Feb 2026 00:00:00 +0000</lastBuildDate><generator>Hugo - gohugo.io</generator><docs>http://cyber.harvard.edu/rss/rss.html</docs><atom:link href="https://ronaldsvilcins.com/atom.xml" rel="self" type="application/atom+xml"/><item><title>Reverse Engineering Claude's Token Counter</title><link>https://grohan.co/2026/02/10/ctoc/</link><description>&lt;p>Claude 3+ doesn&amp;rsquo;t ship with an open tokenizer.&lt;/p>
&lt;p>If you&amp;rsquo;re building coding agents, that&amp;rsquo;s a practical problem. You either call Anthropic&amp;rsquo;s &lt;a href="https://docs.anthropic.com/en/api/messages-count-tokens">&lt;code>count_tokens&lt;/code>&lt;/a> for everything (slow, online, awkward) or use a proxy estimator (&lt;code>tiktoken&lt;/code>, &lt;code>wc -c // {3,4}&lt;/code>) and accept large systematic error - up to 20–50% for Claude models. As agents increasingly self-manage context, they need a fast, local way to see token usage across a directory — &lt;a href="https://github.com/AlDanial/cloc">&lt;code>cloc&lt;/code>&lt;/a> (count lines of code), but for tokens.&lt;/p>
&lt;p>So I reverse-engineered a large chunk of Claude&amp;rsquo;s vocabulary from the &lt;code>count_tokens&lt;/code> API and built &lt;a href="https://github.com/rohangpta/ctoc/tree/main">&lt;code>ctoc&lt;/code>&lt;/a>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-fallback" data-lang="fallback">&lt;span style="display:flex;">&lt;span>➜ ctoc git:(main) bazel-bin/ctoc --by-file .
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>--------------------------------------------------------------------------------
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>File Ext tokens
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>--------------------------------------------------------------------------------
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>./MODULE.bazel.lock .lock 22,906
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>./ctoc.cc .cc 4,870
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>./REPORT.md .md 4,290
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>./gen_vocab.py .py 962
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>./README.md .md 473
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>./BUILD.bazel .bazel 246
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>./MODULE.bazel .bazel 153
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>./020020-huh-isk .dat 22
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>--------------------------------------------------------------------------------
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>SUM (8 files) 33,922
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>--------------------------------------------------------------------------------
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;code>ctoc&lt;/code> is an offline estimator of Claude 4.x&amp;rsquo;s &lt;code>count_tokens()&lt;/code> API that lands at ~96% accuracy. It&amp;rsquo;s backed by a 36,495-token verified vocabulary (greedy tokenizer), and the code/vocabulary are &lt;a href="https://github.com/rohangpta/ctoc/tree/main">open source&lt;/a>.&lt;/p>
&lt;h2 id="a-primer-on-bpe-tokenization">A primer on BPE tokenization &lt;a href="#a-primer-on-bpe-tokenization" class="hash">#&lt;/a>&lt;/h2>
&lt;p>Tokenizers these days mostly use the &lt;a href="https://en.wikipedia.org/wiki/Byte-pair_encoding">BPE algorithm&lt;/a>. BPE is &amp;ldquo;trained&amp;rdquo; by using a fixed/private corpus of data to create a table of merge rules of frequent tokens.&lt;/p>
&lt;p>Say the merge table (in priority order) is:&lt;/p>
&lt;ol>
&lt;li>t + h → th&lt;/li>
&lt;li>th + e → the&lt;/li>
&lt;li>i + s → is&lt;/li>
&lt;/ol>
&lt;p>Tokenizing the input &amp;ldquo;this&amp;rdquo;:&lt;/p>
&lt;ul>
&lt;li>Start: [t, h, i, s]&lt;/li>
&lt;li>Rule 1: [th, i, s] — merged t+h&lt;/li>
&lt;li>Rule 2: no &amp;ldquo;th&amp;rdquo;+&amp;ldquo;e&amp;rdquo; pair — skip&lt;/li>
&lt;li>Rule 3: [th, is] — merged i+s&lt;/li>
&lt;li>Done: [&amp;ldquo;th&amp;rdquo;, &amp;ldquo;is&amp;rdquo;]&lt;/li>
&lt;/ul>
&lt;p>That&amp;rsquo;s a toy intuition, not a full encoder spec: practical BPE implementations track merge priorities over adjacent pairs as well.&lt;/p>
&lt;p>Now I&amp;rsquo;m not &lt;em>certain&lt;/em> Anthropic uses BPE, but given that most of the industry has converged on it, it&amp;rsquo;s a reasonable bet.&lt;/p>
&lt;p>This means that without this &amp;ldquo;merge table&amp;rdquo; or private corpus we can&amp;rsquo;t &lt;em>exactly&lt;/em> re-create the tokenization.&lt;/p>
&lt;p>But if we can recover the vocabulary, we don&amp;rsquo;t need the merge table — a greedy longest-match over the known tokens should approximate BPE&amp;rsquo;s token counts closely enough.&lt;/p>
&lt;h2 id="probing-count_tokens-to-identify-vocabulary">Probing count_tokens to identify vocabulary &lt;a href="#probing-count_tokens-to-identify-vocabulary" class="hash">#&lt;/a>&lt;/h2>
&lt;p>We need to do two things to reverse engineer Claude&amp;rsquo;s vocabulary: verify whether a candidate string is a single token, and decompose longer strings to extract tokens we haven&amp;rsquo;t seen yet.&lt;/p>
&lt;h3 id="single-token-verification">Single-token verification &lt;a href="#single-token-verification" class="hash">#&lt;/a>&lt;/h3>
&lt;p>The naive check — send a candidate to &lt;code>count_tokens&lt;/code>, see if you get 1 — doesn&amp;rsquo;t work because the API wraps inputs in chat framing. Raw counts include a roughly constant overhead that varies by the type of the first character (7, 8, 9+ for letters, digits, Unicode respectively - what&amp;rsquo;s going on here, Anthropic?).&lt;/p>
&lt;p>Sandwich counting fixes this: wrap every probe between markers that we know are single tokens, and subtract the known baseline.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="font-weight:bold">def&lt;/span> count_tokens(text: str) -&amp;gt; int:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> marker = &lt;span style="font-style:italic">&amp;#34;&lt;/span>&lt;span style="font-weight:bold;font-style:italic">\u00A7&lt;/span>&lt;span style="font-style:italic">&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> base = raw_count(marker + marker)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="font-weight:bold">return&lt;/span> raw_count(marker + text + marker) - base
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>With this, &lt;code>count_tokens(candidate) == 1&lt;/code> reliably tells us whether a candidate is a single token.&lt;/p>
&lt;h3 id="decomposing-strings-into-tokens">Decomposing strings into tokens &lt;a href="#decomposing-strings-into-tokens" class="hash">#&lt;/a>&lt;/h3>
&lt;p>Verification handles candidates we already suspect are single tokens. But when we encounter a longer string - from a dataset, from another tokenizer&amp;rsquo;s vocab - we need a way to extract the individual tokens inside it.&lt;/p>
&lt;p>The approach: scan for a position &lt;code>i&lt;/code> where &lt;code>count(s[:i]) == 1&lt;/code> or &lt;code>count(s[i:]) == 1&lt;/code>. That peels off one confirmed token from either end. Recurse on the remainder until the string is fully decomposed (or you hit a chunk that can&amp;rsquo;t be broken down further).&lt;/p>
&lt;p>So we have the machinery: sandwich counting to verify single tokens, iterative peeling to decompose arbitrary strings. But decompose &lt;em>what&lt;/em>?&lt;/p>
&lt;p>Brute-forcing all possible byte strings of length 1-64 is astronomically large, and our decomposition is linear per string, so even clever generation strategies hit the API wall fast (2000/min rate limit). Early experiments with generated data and public datasets proved to be intractable very quickly, though we did pull out a few confirmed tokens as proof of life.&lt;/p>
&lt;h3 id="cross-tokenizer-mining">Cross-tokenizer mining &lt;a href="#cross-tokenizer-mining" class="hash">#&lt;/a>&lt;/h3>
&lt;p>I needed a way to narrow the search space to something that would run in days instead of centuries.&lt;/p>
&lt;p>After some back-and-forth with Claude, I arrived on cross-tokenizer mining: decode vocab items from existing BPE tokenizers (tiktoken, HuggingFace models), test each against Claude with sandwich counting, keep only single-token hits.&lt;/p>
&lt;p>Tokenizers trained on overlapping internet/programming corpora converge on many of the same subwords. This isn&amp;rsquo;t surprising if you think about it from a compression angle, and the Anthropic-specific tokens are likely not dominant in the distribution.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Source&lt;/th>
&lt;th>Vocab size&lt;/th>
&lt;th>Hit rate&lt;/th>
&lt;th>New tokens found&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>tiktoken cl100k_base (GPT-4)&lt;/td>
&lt;td>100K&lt;/td>
&lt;td>~46%&lt;/td>
&lt;td>~29,000&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>tiktoken o200k_base (GPT-4o)&lt;/td>
&lt;td>200K&lt;/td>
&lt;td>~15%&lt;/td>
&lt;td>~4,000&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>11 HuggingFace US + China models&lt;/td>
&lt;td>150-250K each&lt;/td>
&lt;td>2-74%&lt;/td>
&lt;td>~3,800&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Sorting multilingual candidates by cross-tokenizer frequency (tokens appearing in more tokenizers checked first) gave a 74% hit rate on the first 1,000 candidates, declining to ~2% by 20K.&lt;/p>
&lt;h3 id="long-tail">Long tail &lt;a href="#long-tail" class="hash">#&lt;/a>&lt;/h3>
&lt;p>The long-tail vocabulary recoveries here were interesting:&lt;/p>
&lt;ul>
&lt;li>Re-checking all digit-starting candidates recovered ~1,006 tokens — almost all 3-digit numbers (&lt;code>916&lt;/code>, &lt;code>030&lt;/code>, &lt;code>271&lt;/code>) are single tokens.&lt;/li>
&lt;li>BPE creates single tokens for specific lengths of repeated characters (&lt;code>=&lt;/code>, &lt;code>-&lt;/code>, etc.) up to length 64 with non-monotonic patterns: &lt;code>&amp;quot;=&amp;quot; * 7&lt;/code> is 2 tokens but &lt;code>&amp;quot;=&amp;quot; * 8&lt;/code> is 1 (likely a BPE merge-order artifact — these lengths matter for markdown fences and separator lines). Space sequences 1-16, tab sequences 1-4, and newline variants are each single tokens - critical for code indentation accuracy (and token efficiency!)&lt;/li>
&lt;/ul>
&lt;h2 id="results">Results &lt;a href="#results" class="hash">#&lt;/a>&lt;/h2>
&lt;p>The full extraction took 3 days and ~277K API calls.&lt;/p>
&lt;h3 id="estimator-quality-greedy-longest-match">Estimator quality (greedy longest-match) &lt;a href="#estimator-quality-greedy-longest-match" class="hash">#&lt;/a>&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Corpus&lt;/th>
&lt;th style="text-align:right">Efficiency ratio (&lt;code>API tokens / greedy tokens&lt;/code>)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Python source (9 files)&lt;/td>
&lt;td style="text-align:right">96.1%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Mixed code + docs (9 files)&lt;/td>
&lt;td style="text-align:right">95.1%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>English prose (5 samples)&lt;/td>
&lt;td style="text-align:right">99.2%&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Per-file variance is low: individual files land within ~2% of their corpus mean, and the estimator consistently over-segments (predicts more tokens than API), which is desirable for conservative context budgeting (read the Appendix to understand &lt;em>why&lt;/em> we get this close).&lt;/p>
&lt;h3 id="conclusion">Conclusion &lt;a href="#conclusion" class="hash">#&lt;/a>&lt;/h3>
&lt;p>The final outcome of this project - &lt;code>ctoc&lt;/code> - is deliberately boring software that just runs a greedy tokenization algorithm on top of our discovered vocabulary. Unknown bytes fall back to 1 token. Fast enough to run as a preflight check in local workflows, or for a coding agent to run as a subprocess.&lt;/p>
&lt;p>If you care about exact counts for billing-critical paths, use Anthropic&amp;rsquo;s API. If you care about fast, parallelizable, local (self)context management, this could be good enough (and certainly has scope to close the 3-4% gap).&lt;/p>
&lt;p>As always, this depends on current (tested with Claude 4.x) API behavior and can drift if Anthropic changes tokenizer internals.&lt;/p>
&lt;hr>
&lt;h2 id="appendix">Appendix &lt;a href="#appendix" class="hash">#&lt;/a>&lt;/h2>
&lt;h3 id="why-does-greedy-tokenization-do-well">Why does greedy tokenization do well? &lt;a href="#why-does-greedy-tokenization-do-well" class="hash">#&lt;/a>&lt;/h3>
&lt;p>What turned out to be empirically true but not obvious - greedy longest-match and BPE merge-order produce &lt;em>different segmentations&lt;/em>. Why do the token &lt;em>counts&lt;/em> nearly converge?&lt;/p>
&lt;p>There&amp;rsquo;s a reasonable theoretical picture:&lt;/p>
&lt;p>BPE vocabularies are built for greedy compression. &lt;a href="https://aclanthology.org/2023.findings-acl.38/">Zouhar et al. (2023)&lt;/a> formalise BPE training as greedy submodular maximisation of a compression utility. The vocabulary is constructed bottom-up: every multi-character token was formed by merging two shorter tokens that are also in the vocabulary. This hierarchical structure means the longest token at any position tends to be the one BPE would also select.&lt;/p>
&lt;p>Byte-level completeness prevents dead ends. BPE vocabularies always include single-byte fallback tokens, so a left-to-right greedy pass can never paint itself into a corner. This eliminates the primary failure mode of greedy algorithms on arbitrary dictionaries.&lt;/p>
&lt;p>BPE training also has an implicit left-to-right bias. &lt;a href="https://aclanthology.org/2025.emnlp-main.1775/">Sawada &amp;amp; Goyal (2025)&lt;/a> evaluate merge-list-free left-to-right greedy encoding and find it is often comparable to standard merge-based tokenization, with improvements on some tasks and modest degradations on others.&lt;/p>
&lt;p>And where greedy and BPE do disagree, the result is typically a rearrangement of boundaries, not a net increase in tokens. &lt;code>['hell', 'ooo']&lt;/code> vs &lt;code>['hello', 'oo']&lt;/code> — different boundaries, same count.&lt;/p>
&lt;p>The upshot: for BPE-family tokenizers with byte fallback, greedy and merge-order counting often converge closely in practice when vocabulary coverage is high. The 4-5% gap in &lt;code>ctoc&lt;/code> is plausibly missing tokens forcing over-segmentation. If the vocabulary were complete, greedy counting would likely be even closer to API counts.&lt;/p></description><author>ronalds.vilcins@gmail.com (Ronalds Vilcins)</author><guid>https://grohan.co/2026/02/10/ctoc/</guid><pubDate>Tue, 10 Feb 2026 00:00:00 +0000</pubDate></item><item><title>Compressed Filesystems á la Language Models</title><link>https://grohan.co/2025/11/25/llmfuse/</link><description>&lt;p>Every systems engineer at some point in their journey yearns to write
a filesystem. This sounds daunting at first - and writing a battle-tested filesystem &lt;em>is&lt;/em> hard - but the minimal surface area for a &amp;ldquo;working&amp;rdquo; FS is surprisingly small, simple, and in-distribution for coding agents.&lt;/p>
&lt;p>In fact, one of my smoke tests for new coding models is seeing how good of
a filesystem they can one-shot! At some point, I had quite a few filesystems lying around - and coding models were getting pretty good - which made me wonder if the models were intelligent enough to actually model the filesystem engine itself?&lt;/p>
&lt;p>A filesystem is the perfect black-box API to model with wacky backends (see &lt;a href="http://tom7.org/papers/murphy2022harder.pdf">&amp;ldquo;Harder drives&amp;rdquo;&lt;/a>), and besides the joy of training an LLM for fun - there were a few deeper truths about language models that I wanted to explore.&lt;/p>
&lt;h1 id="training-a-filesystem">Training a filesystem &lt;a href="#training-a-filesystem" class="hash">#&lt;/a>&lt;/h1>
&lt;p>So I set upon training a filesystem. Building on top of one of my throwaway
FUSEs, a few rounds with Claude repurposed it to loopback against the host
with added logging, two things I needed to generate reference fine-tuning data:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-py" data-lang="py">&lt;span style="display:flex;">&lt;span>&lt;span style="font-weight:bold">class&lt;/span> &lt;span style="font-weight:bold">LoggingLoopbackFS&lt;/span>(LoggingMixIn, Operations):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="font-style:italic">&amp;#34;&amp;#34;&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="font-style:italic"> A loopback FUSE filesystem that logs all operations for training data.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="font-style:italic">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="font-style:italic"> This implementation delegates all filesystem operations to a real directory
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="font-style:italic"> on the host filesystem, ensuring perfect semantic correctness while logging
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="font-style:italic"> every operation for LLM training data.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="font-style:italic"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>I then wrote a filesystem interaction simulator, which sampled various
operations against a sandboxed &lt;code>LoggingLoopbackFS&lt;/code> to generate diverse FUSE
prompt/completion pairs. Concretely, I captured only the minimal set of &lt;a href="https://github.com/rohangpta/llmfuse/blob/main/train/generate_data.py#L246">operations needed&lt;/a> for
R/W-ish capability (no open, xattrs, fsync etc).&lt;/p>
&lt;p>Alongside the FUSE operation, I captured the full filesystem state at every
turn. I experimented with various formats, including an ASCII-art
representation, but ultimately settled on XML since it enforces prompt
boundaries clearly and had canonical parsers available.&lt;/p>
&lt;p>With prompts including the FUSE operation + XML filesystem tree, the model learned two forms of completions:&lt;/p>
&lt;ul>
&lt;li>Reads (&amp;lt;R&amp;gt;) requested the content / metadata as per the operation
(&lt;code>getattr&lt;/code> / &lt;code>readdir&lt;/code> / &lt;code>read&lt;/code>)&lt;/li>
&lt;li>Writes (&amp;lt;W&amp;gt;) requested the model to output the full filesystem tree state,
after modification (&lt;code>unlink&lt;/code> / &lt;code>chmod&lt;/code> / &lt;code>truncate&lt;/code> / &lt;code>write&lt;/code>)&lt;/li>
&lt;/ul>
&lt;p>Example prompt (read):&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-fallback" data-lang="fallback">&lt;span style="display:flex;">&lt;span>&amp;lt;R&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>read(&amp;#39;/usr14/log767.rs&amp;#39;, size=4096, offset=0, fh=4)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>---
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&amp;lt;filesystem&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;lt;directory path=&amp;#34;/&amp;#34; name=&amp;#34;/&amp;#34; mode=&amp;#34;755&amp;#34; owner=&amp;#34;root&amp;#34; group=&amp;#34;root&amp;#34;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>mtime=&amp;#34;2025-01-01T00:00:00&amp;#34;&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;lt;directory path=&amp;#34;usr14&amp;#34; name=&amp;#34;usr14&amp;#34; mode=&amp;#34;755&amp;#34; owner=&amp;#34;root&amp;#34; group=&amp;#34;root&amp;#34;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>mtime=&amp;#34;2025-01-01T00:00:00&amp;#34;&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;lt;file path=&amp;#34;usr14/log767.rs&amp;#34; name=&amp;#34;log767.rs&amp;#34; mode=&amp;#34;644&amp;#34; owner=&amp;#34;root&amp;#34;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>group=&amp;#34;root&amp;#34; mtime=&amp;#34;2025-01-01T00:00:01&amp;#34; size=&amp;#34;276&amp;#34;&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;lt;body&amp;gt;fn main() {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> match process(7) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Ok(result) =&amp;amp;gt; println!(&amp;#34;Result: {}&amp;#34;, result),
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Err(e) =&amp;amp;gt; eprintln!(&amp;#34;Error: {}&amp;#34;, e),
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&amp;lt;/body&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;lt;/file&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;lt;file path=&amp;#34;usr14/temp912.sh&amp;#34; name=&amp;#34;temp912.sh&amp;#34; mode=&amp;#34;644&amp;#34; owner=&amp;#34;root&amp;#34;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>group=&amp;#34;root&amp;#34; mtime=&amp;#34;2025-01-01T00:00:01&amp;#34; size=&amp;#34;268&amp;#34;&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;lt;body&amp;gt;#!/bin/bash
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> echo &amp;#34;temp912&amp;#34; || exit 1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;lt;/body&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;lt;/file&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;lt;/directory&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;lt;/directory&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&amp;lt;/filesystem&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Completion:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-rs" data-lang="rs">&lt;span style="display:flex;">&lt;span>&lt;span style="font-weight:bold">fn&lt;/span> main() {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="font-weight:bold">match&lt;/span> process(7) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Ok(result) =&amp;gt; println!(&lt;span style="font-style:italic">&amp;#34;Result: &lt;/span>&lt;span style="font-weight:bold;font-style:italic">{}&lt;/span>&lt;span style="font-style:italic">&amp;#34;&lt;/span>, result),
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Err(e) =&amp;gt; eprintln!(&lt;span style="font-style:italic">&amp;#34;Error: &lt;/span>&lt;span style="font-weight:bold;font-style:italic">{}&lt;/span>&lt;span style="font-style:italic">&amp;#34;&lt;/span>, e),
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="fine-tuning">Fine-tuning &lt;a href="#fine-tuning" class="hash">#&lt;/a>&lt;/h3>
&lt;p>Once I had clean, representative, and diverse filesystem simulation data, actually running SFT was pretty straightforward on Modal. Over a few iteration cycles spread across nibbles of spare time, I ended up with ~98% accuracy on a hold-out eval after 8 epochs of SFT on a N=15000 dataset with Qwen3-4b.&lt;/p>
&lt;p>Most of my time here was spent cleaning generated data and ensuring we represented every FUSE operation sufficiently + generated enough &amp;ldquo;complex&amp;rdquo; trees to learn on.&lt;/p>
&lt;p>At this point, I wrote &amp;hellip; possibly the smallest filesystem I&amp;rsquo;ve seen&amp;hellip; to give my model a spin in
the real world. Every FUSE operation was a passthrough to the LLM, for example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-py" data-lang="py">&lt;span style="display:flex;">&lt;span>&lt;span style="font-weight:bold">class&lt;/span> &lt;span style="font-weight:bold">LLMFuse&lt;/span>(LoggingMixin, Operations):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="font-weight:bold">def&lt;/span> chmod(self, path, mode):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="font-style:italic">&amp;#34;&amp;#34;&amp;#34;Change file permissions.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> response = self._query_llm_for_operation(&lt;span style="font-style:italic">&amp;#39;chmod&amp;#39;&lt;/span>, path, mode=oct(mode))
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="font-weight:bold">if&lt;/span> &lt;span style="font-weight:bold">not&lt;/span> self._handle_llm_response(response):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="font-weight:bold">raise&lt;/span> FuseOSError(ENOENT)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="font-weight:bold">return&lt;/span> 0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Nice! I now had a mountable FUSE that was entirely &amp;ldquo;implemented&amp;rdquo; by a language
model. As you can see below, I was able to &lt;code>ls&lt;/code> around it, &lt;code>echo&lt;/code> into files, and &lt;code>cat&lt;/code> them back out.&lt;/p>
&lt;center>
&lt;img src =/images/llmfuse_example.png width="800" height="250"/>
Poking around a Docker container with a mounted LLMFuse.
&lt;/center>
&lt;h1 id="compressing-the-filesystem">Compressing the filesystem &lt;a href="#compressing-the-filesystem" class="hash">#&lt;/a>&lt;/h1>
&lt;p>Perhaps the largest glaring inefficiency in this set up is the sheer verbosity
of the XML-based representation. I was using many bytes to represent attributes
and tree structure that could be encoded far more efficiently (~O(bits)) in a standard
C struct.&lt;/p>
&lt;p>However, as I was fine-tuning on the XML filesystem tree representation, I was
baking in this very structure into the weights and probability distributions of my Qwen fork! If only there was a way to leverage this to compress state&amp;hellip;&lt;/p>
&lt;h2 id="two-sides-of-the-same-coin">Two sides of the same coin &lt;a href="#two-sides-of-the-same-coin" class="hash">#&lt;/a>&lt;/h2>
&lt;p>As it turns out, compression and AI are intimately related. Using LLMs to lossily
compress text is one of the most common applications, so it&amp;rsquo;s not entirely
unintuitive. However, one researcher (Marcus Hutter) claimed back in 2006 that they are &lt;em>equivalent&lt;/em> (and in fact &lt;a href="http://prize.hutter1.net/hfaq.htm#about">bet $500K on this claim!&lt;/a>).&lt;/p>
&lt;p>Presciently, Hutter appears to be absolutely right. His &lt;code>enwik8&lt;/code> and &lt;code>enwik9&lt;/code>&amp;rsquo;s benchmark datasets are, today, best compressed by a &lt;a href="https://bellard.org/ts_zip/">169M parameter LLM&lt;/a> (trained by none other than Fabrice Bellard in 2023).&lt;/p>
&lt;p>That&amp;rsquo;s a bit perplexing on the first glance. Surely LLM compression isn&amp;rsquo;t reversible? What kind of voodoo magic was going on here?&lt;/p>
&lt;h3 id="arithmetic-coding">Arithmetic coding &lt;a href="#arithmetic-coding" class="hash">#&lt;/a>&lt;/h3>
&lt;p>The algorithm that enables reversible compression using LLMs is called &amp;ldquo;arithmetic coding&amp;rdquo; and it builds upon a &lt;a href="https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf">1948 result by Claude Shannon&lt;/a>.&lt;/p>
&lt;p>Researchers at DeepMind (including Hutter himself) have &lt;a href="https://arxiv.org/pdf/2309.10668">explained the math in
detail&lt;/a>, so I&amp;rsquo;ll direct the most inquisitive of you readers there, but for a simplified understanding of what&amp;rsquo;s going on, forget everything you might know about working with LLMs today. There&amp;rsquo;s no prompting involved!&lt;/p>
&lt;center>
&lt;img src =/images/ac.png width="800" height="300"/>
&lt;/center>
&lt;p>Let&amp;rsquo;s assume the following is true for some predictive model \(M\)&lt;/p>
&lt;ul>
&lt;li>Lorem has first-word probability = 0.57.&lt;/li>
&lt;li>Ipsum has second-word conditional probability = 0.67 (joint 0.38).&lt;/li>
&lt;li>Dolor has a third word conditional probability = 0.5 (joint 0.19).&lt;/li>
&lt;/ul>
&lt;p>&amp;hellip;&lt;/p>
&lt;p>so on and so forth until you reach the end of the string you want to compress and you end up with some &amp;ldquo;final interval width&amp;rdquo; \(P(m)\) on the real interval \([0,1]\) which represents your string.&lt;/p>
&lt;p>Let&amp;rsquo;s suppose in our example this turns out to be 0.012. We can represent this decimal in roughly \(- \log_{2}{P(m)} = 6.4\) bits, which is our final compression size.&lt;/p>
&lt;p>There&amp;rsquo;s a few elegant things about this algorithm:&lt;/p>
&lt;ul>
&lt;li>&lt;em>Any&lt;/em> number within this interval is uniquely determined by tracing the arithmetic coding algorithm through the specific probabilistic model&amp;rsquo;s weights. &amp;ldquo;Decoding&amp;rdquo; is simply a retracing operation (see the line through the probability distributions above)&lt;/li>
&lt;li>The inverse log relationship between predictive power \(P(m)\) and compression
pushes the burden of the &amp;ldquo;hard compression problem&amp;rdquo; to deep learning machinery which can encode high-dimensional text patterns within model weights, yielding far better compression ratios than deterministic algorithms.&lt;/li>
&lt;/ul>
&lt;p>Sounds cool! But &lt;strong>how good really&lt;/strong> is this compression? On comparing
arithmetic coding backed by &lt;code>Qwen3-4B&lt;/code> against &lt;code>gzip&lt;/code> for &lt;a href="https://www.lipsum.com/">&lt;code>lipsum.txt&lt;/code>&lt;/a>,
we already see pretty dramatic results:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Method&lt;/th>
&lt;th>Size (bytes)&lt;/th>
&lt;th>Compression Impact&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Original (plain)&lt;/td>
&lt;td>446&lt;/td>
&lt;td>—&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>gzip&lt;/code>&lt;/td>
&lt;td>298&lt;/td>
&lt;td>~33% smaller&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>llmencode&lt;/code>&lt;/td>
&lt;td>13&lt;/td>
&lt;td>~97% smaller&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>(note: &lt;a href="https://github.com/rohangpta/llmfuse/blob/main/llmencode/llmencode.py">&lt;code>llmencode&lt;/code>&lt;/a> is my implementation of arithmetic coding)&lt;/p>
&lt;p>22x better compression than &lt;code>gzip&lt;/code> is pretty ridiculous! A caveat here is that &lt;code>lipsum.txt&lt;/code> is heavily represented in training data, but 5-20x efficiency gains broadly hold for all text data that (looks like) it&amp;rsquo;s been on the internet.&lt;/p>
&lt;h2 id="self-compression">Self-compression &lt;a href="#self-compression" class="hash">#&lt;/a>&lt;/h2>
&lt;p>Now, back to our filesystem. The XML overhead we were worried about now can be
&amp;ldquo;compressed away&amp;rdquo; by the fine-tuned model. Using the same toy filesystem from
the Docker container demo above:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-xml" data-lang="xml">&lt;span style="display:flex;">&lt;span>&lt;span style="font-weight:bold">&amp;lt;filesystem&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="font-weight:bold">&amp;lt;directory&lt;/span> path=&lt;span style="font-style:italic">&amp;#34;/&amp;#34;&lt;/span> name=&lt;span style="font-style:italic">&amp;#34;/&amp;#34;&lt;/span> mode=&lt;span style="font-style:italic">&amp;#34;755&amp;#34;&lt;/span> owner=&lt;span style="font-style:italic">&amp;#34;root&amp;#34;&lt;/span> group=&lt;span style="font-style:italic">&amp;#34;root&amp;#34;&lt;/span> mtime=&lt;span style="font-style:italic">&amp;#34;2025-01-01T00:00:00&amp;#34;&lt;/span>&lt;span style="font-weight:bold">&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="font-weight:bold">&amp;lt;directory&lt;/span> path=&lt;span style="font-style:italic">&amp;#34;testdir&amp;#34;&lt;/span> name=&lt;span style="font-style:italic">&amp;#34;testdir&amp;#34;&lt;/span> mode=&lt;span style="font-style:italic">&amp;#34;755&amp;#34;&lt;/span> owner=&lt;span style="font-style:italic">&amp;#34;root&amp;#34;&lt;/span> group=&lt;span style="font-style:italic">&amp;#34;root&amp;#34;&lt;/span> mtime=&lt;span style="font-style:italic">&amp;#34;2025-01-01T00:00:00&amp;#34;&lt;/span> &lt;span style="font-weight:bold">/&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="font-weight:bold">&amp;lt;file&lt;/span> path=&lt;span style="font-style:italic">&amp;#34;testfile.txt&amp;#34;&lt;/span> name=&lt;span style="font-style:italic">&amp;#34;testfile.txt&amp;#34;&lt;/span> mode=&lt;span style="font-style:italic">&amp;#34;644&amp;#34;&lt;/span> owner=&lt;span style="font-style:italic">&amp;#34;root&amp;#34;&lt;/span> group=&lt;span style="font-style:italic">&amp;#34;root&amp;#34;&lt;/span> mtime=&lt;span style="font-style:italic">&amp;#34;2025-01-01T00:00:01&amp;#34;&lt;/span> size=&lt;span style="font-style:italic">&amp;#34;14&amp;#34;&lt;/span>&lt;span style="font-weight:bold">&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="font-weight:bold">&amp;lt;body&amp;gt;&lt;/span>hello llmfuse
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="font-weight:bold">&amp;lt;/body&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="font-weight:bold">&amp;lt;/file&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="font-weight:bold">&amp;lt;/directory&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="font-weight:bold">&amp;lt;/filesystem&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>Original (bytes)&lt;/th>
&lt;th>Compressed (bytes)&lt;/th>
&lt;th>Ratio&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Base Qwen3-4B&lt;/td>
&lt;td>394&lt;/td>
&lt;td>38&lt;/td>
&lt;td>10.4x&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Fine-tuned Qwen3-4B&lt;/td>
&lt;td>394&lt;/td>
&lt;td>21&lt;/td>
&lt;td>18.8x&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>The fine-tuned model achieves &lt;strong>44.7% better compression&lt;/strong> on XML filesystem
trees - the very format it was trained to predict. This is the &amp;ldquo;self-compression&amp;rdquo;
effect: by baking the XML structure into the model weights during fine-tuning,
the arithmetic coder can represent that structure in fewer bits.&lt;/p>
&lt;p>Self-compression in filesystems isn&amp;rsquo;t a novel concept. For example, there exists the
&lt;a href="https://docs.kernel.org/filesystems/squashfs.html">&lt;code>squashfs&lt;/code>&lt;/a> tool (created in 2002) to create R/O compressed filesystems. Squashfs compresses
files, inodes, and directories together, not unlike what we&amp;rsquo;re doing here!&lt;/p>
&lt;p>Under the hood, &lt;code>squashfs&lt;/code> just wraps &lt;code>gzip&lt;/code>/&lt;code>zstd&lt;/code>/your favourite compression
algorithm. So for plain-text data, &lt;code>squashfs&lt;/code> compression stats pale in the face of &lt;code>llmfuse&lt;/code>:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Method&lt;/th>
&lt;th>Compressed Size&lt;/th>
&lt;th>Notes&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>squashfs (gzip)&lt;/td>
&lt;td>171 bytes&lt;/td>
&lt;td>gzip-compressed file contents, inodes, directory tables&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>llmfuse (fine-tuned)&lt;/td>
&lt;td>21 bytes&lt;/td>
&lt;td>Arithmetic coded XML state&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>For the same filesystem tree (one directory, one 14-byte text file), llmfuse
achieves &lt;strong>~8x better compression&lt;/strong> than squashfs (see methodology in appendix).&lt;/p>
&lt;p>The difference comes down to &lt;code>llmencode&lt;/code> being far better than &lt;code>gzip&lt;/code> on
text data + XML structure - especially when the model has been fine-tuned on exactly
that structure.&lt;/p>
&lt;h1 id="conclusion">Conclusion &lt;a href="#conclusion" class="hash">#&lt;/a>&lt;/h1>
&lt;p>What started off as a little experiment mostly to get my hands dirty with
training and inference evolved into a full blown &lt;a href="https://xkcd.com/356/">nerd
snipe&lt;/a> and intellectual adventure. Thanks for making it
this far!&lt;/p>
&lt;p>I entirely recognize that this is a &amp;ldquo;toy&amp;rdquo;
experiment under a very specific setup; with that said, the numbers above are pretty eye-popping, and the question I&amp;rsquo;ve been trying to answer as I write this up is: does this have any real-world potential?&lt;/p>
&lt;p>Of course, in the short term, there&amp;rsquo;s a whole host of caveats: you need an
LLM, likely a GPU, all your data is in the context window (which we know scales
poorly), and this only works on text data.&lt;/p>
&lt;p>Still, it&amp;rsquo;s intriguing to wonder whether the very engines that will likely
dominate all &amp;ldquo;text generation&amp;rdquo; going forward can be used to compress their own
data? Perhaps in a distant future, where running LLMs at the edge makes sense, or for specific kinds of workflows where data is read very infrequently.&lt;/p>
&lt;p>Overall, I&amp;rsquo;m grateful to Peyton at &lt;a href="https://modal.com">Modal&lt;/a> for the compute credits. Running
a somewhat unconventional experiment like this wouldn&amp;rsquo;t have been possible
without full control over the training and inference code, and extremely
tedious without the simplicity of running ML infra on Modal! It&amp;rsquo;s truly awesome
to be able to just &lt;code>modal deploy&lt;/code> and get my own private inference endpoints,
or just &lt;code>modal run&lt;/code> to prototype some code on the cloud.&lt;/p>
&lt;h1 id="appendix">Appendix &lt;a href="#appendix" class="hash">#&lt;/a>&lt;/h1>
&lt;h2 id="source-code">Source Code &lt;a href="#source-code" class="hash">#&lt;/a>&lt;/h2>
&lt;p>All of the source code for this experiment, particularly &lt;code>llmfuse&lt;/code> and
&lt;code>llmencode&lt;/code> are &lt;a href="https://github.com/rohangpta/llmfuse">open-sourced&lt;/a> under MIT.&lt;/p>
&lt;p>&lt;code>llmencode&lt;/code> is abstracted into a CLI utility that you can run locally.
Inference on 4B models is slow, but entirely possible on consumer hardware.
I prototyped most of this code by running on a 2021 MacBook Pro, before
productionizing on Modal.&lt;/p>
&lt;p>A fun experiment / party trick to identify how &amp;ldquo;common&amp;rdquo; a certain
string is in training data is to look at its &lt;code>llmencode&lt;/code> compression ratio!&lt;/p>
&lt;h2 id="squashfs-comparison-methodology">SquashFS comparison methodology &lt;a href="#squashfs-comparison-methodology" class="hash">#&lt;/a>&lt;/h2>
&lt;p>The raw &lt;code>.sqsh&lt;/code> file is 4096 bytes due to block alignment padding. To find the
actual compressed size, I used &lt;code>xxd&lt;/code> to inspect the binary and found the last
non-zero byte at offset 266 (267 bytes total). Subtracting the fixed 96-byte
superblock header gives us 171 bytes of actual gzip-compressed content -
everything needed to reconstruct the filesystem.&lt;/p>
&lt;h2 id="compression-as-a-metric">Compression as a metric &lt;a href="#compression-as-a-metric" class="hash">#&lt;/a>&lt;/h2>
&lt;p>It&amp;rsquo;s equally interesting to think about compression as a metric. An angle I&amp;rsquo;d
considered is doing some kind of RL on the arithmetic coded compression number itself.&lt;/p>
&lt;p>&lt;del>Is that simply equivalent to the pre-training objective (due to the prediction-compression duality)? Or does the &amp;ldquo;sequence-level&amp;rdquo; objective add something more&amp;hellip; interesting to the mix. Please reach out if you have thoughts!&lt;/del>&lt;/p>
&lt;p>&lt;strong>EDIT:&lt;/strong>&lt;/p>
&lt;p>As it turns out - it is indeed equivalent to the pre-training objective!&lt;/p>
&lt;p>&lt;strong>Pre-training&lt;/strong> aims to maximize the probability of training data. For a sequence \(x = (x_1, x_2, \ldots, x_T)\), we decompose via the chain rule:&lt;/p>
&lt;div>
$$P(x) = \prod_{t=1}^{T} P(x_t \mid x_{&lt; t})$$
&lt;/div>
&lt;p>Taking logarithms (for numerical stability and to convert products to sums):&lt;/p>
&lt;div>
$$\log P(x) = \sum_{t=1}^{T} \log P(x_t \mid x_{&lt; t})$$
&lt;/div>
&lt;p>Maximizing log-probability is equivalent to minimizing its negative — the &lt;strong>negative log-likelihood&lt;/strong>:&lt;/p>
&lt;div>
$$\mathcal{L}_{\text{NLL}}(\theta) = -\sum_{t=1}^{T} \log P_\theta(x_t \mid x_{&lt; t})$$
&lt;/div>
&lt;p>&lt;strong>Arithmetic coding&lt;/strong> maps the entire sequence \(x\) to an interval on \([0,1]\) of width equal to its joint probability \(P(x)\). To uniquely specify a point within an interval of width \(P(x)\), we require \(-\log_2 P(x)\) bits.&lt;/p>
&lt;p>Expanding this using the chain rule from above gives us the sum:&lt;/p>
&lt;p>$$L_{\text{compressed}}(x) = -\log_2 P(x) = -\log_2 \prod_{t=1}^{T} P_\theta(x_t \mid x_{&amp;lt; t}) = -\sum_{t=1}^{T} \log_2 P_\theta(x_t \mid x_{&amp;lt; t})$$&lt;/p>
&lt;p>These differ only by logarithm base. Since \(\log_2 P = \frac{\ln P}{\ln 2}\), and \(\frac{1}{\ln 2} \approx 1.44\) is a positive constant:&lt;/p>
&lt;div>
$$\arg\min_\theta \mathcal{L}_{\text{NLL}} = \arg\min_\theta L_{\text{compressed}}$$
&lt;/div>
&lt;p>The same \(\theta^*\) minimizes both. \(\square\)&lt;/p>
&lt;p>Beyond resolving the RL question, there&amp;rsquo;s something quietly beautiful about this framing. In my view, viewing the pre-training objective as compression is a simple way to &amp;ldquo;grok&amp;rdquo; the math.&lt;/p></description><author>ronalds.vilcins@gmail.com (Ronalds Vilcins)</author><guid>https://grohan.co/2025/11/25/llmfuse/</guid><pubDate>Tue, 25 Nov 2025 00:00:00 +0000</pubDate></item><item><title>Technical Debt is Entropy In Software</title><link>https://grohan.co/2024/12/27/entropy/</link><description>&lt;h1 id="entropy">Entropy &lt;a href="#entropy" class="hash">#&lt;/a>&lt;/h1>
&lt;p>Entropy is the ultimate boss battle [1]. As the reason why ice melts, why tires
burst, and why ink diffuses &amp;ndash; thermodynamic entropy is a fact of the
physical world, sharply following the arrow of time [2].&lt;/p>
&lt;p>The Second Law of Thermodynamics states&lt;/p>
&lt;blockquote>
&lt;p>For an isolated system, entropy will either increase or remain the same over time; decreases require exporting entropy to the environment.&lt;/p>
&lt;/blockquote>
&lt;p>There&amp;rsquo;s something about inevitability that I think is fascinating, especially
when you can see it unfold in front of you &amp;ndash; my
favourite visualisation is below (thanks Gemini).&lt;/p>
&lt;center>&lt;img src =/images/diffusion.png width="550" height="400"/>&lt;/center>
&lt;p>This picture displays the three stages of entropy:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Infancy&lt;/strong>: when the ink enters the water&lt;/li>
&lt;li>&lt;strong>Expansion&lt;/strong>: when the ink diffuses through water&lt;/li>
&lt;li>&lt;strong>Maturity&lt;/strong>: when the ink and water have fully merged&lt;/li>
&lt;/ul>
&lt;p>So, how does entropy grow? Academics and practitioners alike believe that
entropy follows an S-curve [4], with its three stages likened to those of ink diffusing in water.&lt;/p>
&lt;center>&lt;img src =/images/scurve.png width="550" height="400"/> &lt;/center>
&lt;p>This is strikingly similar to the business lifecycle curve above! Indeed, prior
art agrees that &lt;strong>business and technology lifecycles are overlaid entropy curves&lt;/strong>
[5].&lt;/p>
&lt;p>Entropy in business is largely a representation of diffusion of a
particular product, driven by the forces of supply and demand. While entropy is
often likened to &amp;ldquo;disorder&amp;rdquo;, I like to use &amp;ldquo;disruption&amp;rdquo; - permeation of new (ink)
into old (water). It is neither good nor bad, simply inevitable.&lt;/p>
&lt;h2 id="properties-of-entropy">Properties of Entropy &lt;a href="#properties-of-entropy" class="hash">#&lt;/a>&lt;/h2>
&lt;h3 id="statistical-entropy">Statistical Entropy &lt;a href="#statistical-entropy" class="hash">#&lt;/a>&lt;/h3>
&lt;p>Since we are talking about software and not atoms, let&amp;rsquo;s turn to information theory to
understand the information contained in software programs.&lt;/p>
&lt;p>In information theory, Shannon&amp;rsquo;s entropy is, for a random
variable \( X \) distributed according to \( p: (x \in \mathcal{X})
\rightarrow [0, 1] \):&lt;/p>
&lt;p>$$ H(X) = - \sum_{x \in \mathcal{X}} p(x)\log(p(x)) $$&lt;/p>
&lt;p>While this may look obscure at first, the formulation begets two properties:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>Property 1.&lt;/strong> The number of possible states that a system can have is
generally proportional to the total entropy in a system. A dice with 6 sides has
more entropy than one with 4 sides.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Property 2.&lt;/strong> Higher entropy is correlated with a higher presence of tail events.
Gaussians and exponentials are maximum entropy distributions (under
certain statistical conditions [3]), but they are light-tailed.
Many real-world systems are heavier-tailed (Taleb-style), so tail risk
is larger than Gaussian intuition suggests [16].&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h3 id="complexity">Complexity &lt;a href="#complexity" class="hash">#&lt;/a>&lt;/h3>
&lt;p>Complexity is entropy&amp;rsquo;s first cousin. Formally we&amp;rsquo;ll use
Kolmogorov complexity:&lt;/p>
&lt;blockquote>
&lt;p>\(K(o)\) for an object \(o\) is the length of the shortest program that produces the
object as output.&lt;/p>
&lt;/blockquote>
&lt;p>In other words, it measures how &amp;ldquo;compressible&amp;rdquo; something is.&lt;/p>
&lt;p>Now - how does entropy relate to complexity? Modis posits the following relation [4][15]:&lt;/p>
&lt;blockquote>
&lt;p>Complexity is the time derivative of entropy. Given that entropy is an
S-curve, complexity is roughly normally distributed.&lt;/p>
&lt;p>With time, complexity increases until a peak and decreases after.&lt;/p>
&lt;/blockquote>
&lt;p>To see this: let&amp;rsquo;s consider Scott Aaronson&amp;rsquo;s lucid example around cream dissolving
into coffee [6].&lt;/p>
&lt;p>His research empirically calculated a complexity score using the &lt;code>gzip&lt;/code> compression
file size of pictures of cream melting in the coffee.&lt;/p>
&lt;center>&lt;img src =/images/coffee_complexity.png width="413" height="300"/>&lt;/center>
&lt;p>The picture above represents snapshots of the coffee cup at the three phases of entropy. As the cream and coffee begin to mix, complexity increases until a maximum
before decreasing as the mixture becomes saturated and homogeneous.&lt;/p>
&lt;p>It is simple to see why the first and
last image can be more easily compressed (i.e are less complex) compared to the
image in the middle.&lt;/p>
&lt;h1 id="where-does-software-fit-into-this">Where Does Software Fit Into This? &lt;a href="#where-does-software-fit-into-this" class="hash">#&lt;/a>&lt;/h1>
&lt;p>Let&amp;rsquo;s start by quantifying where software is today in its business lifecycle /
entropy curve.&lt;/p>
&lt;center>&lt;img src =/images/software_today.png width="550" height="400"/>&lt;/center>
&lt;p>As in the graph above, I think we are around (X,Y) today.&lt;/p>
&lt;p>Why?&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Software is still custom-made / productised. SaaS &amp;ldquo;exploded&amp;rdquo; but hasn&amp;rsquo;t permeated all
industries. Digital modernisation efforts continue to be in higher
demand than available supply, indicating superlinear business growth or relative
convexity &amp;ndash; &lt;em>we are close to the central inflection point&lt;/em>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Software complexity is close to peaking. For starters, the
cloud has driven the successful commoditisation of infrastructure. We now have
the ability to &lt;em>run software&lt;/em> cheaply and easily.&lt;/p>
&lt;p>What&amp;rsquo;s left is to reduce the complexity of
the specification of software (language and layers of
abstraction) that runs on said infrastructure. &lt;em>Prediction&lt;/em>: LLMs are statistical
program compression tools [12] and will do exactly this.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h1 id="tech-debt-as-complexity">Tech Debt as Complexity &lt;a href="#tech-debt-as-complexity" class="hash">#&lt;/a>&lt;/h1>
&lt;p>For a particular business, technological modernisation is largely bottlenecked by iteration speed to a desired solution.
I view tech debt as the maintenance and complexity related resistance to change that
causes this bottleneck.&lt;/p>
&lt;p>To be clear, every problem that software solves has some theoretical baseline level of
&amp;ldquo;complexity&amp;rdquo; needed to meet its specification. Technical debt is simply the add-on
difference in complexity between the ideal and real world solutions.&lt;/p>
&lt;p>So what causes complexity? The sources of complexity that I&amp;rsquo;ve found to
be the most concise yet accurate are below [7]:&lt;/p>
&lt;h3 id="complexity-via-obscurity">Complexity via obscurity &lt;a href="#complexity-via-obscurity" class="hash">#&lt;/a>&lt;/h3>
&lt;p>Poorly designed software abstractions (obscurity) generate more complexity - &amp;ldquo;garbage in, garbage
out&amp;rdquo;.&lt;/p>
&lt;p>The core issue with &amp;ldquo;obscure&amp;rdquo; abstractions is that they are uncompressed representations of state.
Their interfaces are inherently complex, where they should instead be
simple and hide deep complexity [11].&lt;/p>
&lt;p>Often times, obscurity appears due to constraints imposed on systems (&amp;ldquo;make this
code accept XYZ data format&amp;rdquo;). Local
optimisations lead to global API changes that introduce obscurity and state bloat
[17]. Refactors / complexity regulation measures don&amp;rsquo;t fit into deadlines and this bloat
almost always compounds (Property 1).&lt;/p>
&lt;h3 id="complexity-via-dense-dependency-graphs">Complexity via dense dependency graphs &lt;a href="#complexity-via-dense-dependency-graphs" class="hash">#&lt;/a>&lt;/h3>
&lt;p>Software that depends on A LOT of other software is more prone to bugs,
vulnerabilities and maintenance overhead. This kind of
technical debt introduces some obscurity but also &amp;ldquo;tail-risk&amp;rdquo; around software (Property
2).&lt;/p>
&lt;p>The number of failures due to weaknesses
in the open source parts of a software supply chain increased by 650% between
2020 and 2021 [8]. At the same time, OSS adoption has been growing 70% YoY [9], bringing with it
increasingly public vulnerabilities like in &lt;code>log4j&lt;/code>, &lt;code>xz&lt;/code>, &lt;code>OpenSSH&lt;/code> etc.&lt;/p>
&lt;p>Dependencies can also reflect a level of &amp;ldquo;obscurity&amp;rdquo; in software that may not
justify their added risk. Leftpad is a great example here! [18]&lt;/p>
&lt;p>What isn&amp;rsquo;t captured by mere probabilities is how this &amp;ldquo;tail-risk&amp;rdquo; can
manifest as devastatingly high-impact Black Swan events [14]: massive cybercrimes affecting data
protection and financial security. Software outage affecting the
global economy. We&amp;rsquo;ve all seen them play out.&lt;/p>
&lt;h1 id="conclusion">Conclusion &lt;a href="#conclusion" class="hash">#&lt;/a>&lt;/h1>
&lt;center>&lt;img src =/images/complexity_entropy.png width="550" height="400"/>&lt;/center>
&lt;p>To recap, entropy in software tracks the expansion of software with time and complexity
is the derivative of entropy. Due to the S-curve shape of entropy
and thus &amp;ldquo;normal&amp;rdquo; shape of its derivative, &amp;ldquo;complexity begets
complexity&amp;rdquo; until a certain point in time, after which complexity reduces.&lt;/p>
&lt;p>I argue that due to market conditions: namely the state of SaaS as a product /
custom-made offering and the commoditisation of infrastructure (cloud) that we are at the middle of the S-curve. i.e., we are at the &lt;strong>peak
of software complexity / tech debt growth&lt;/strong>. Intuitively, in the business “experimental -&amp;gt; custom -&amp;gt; product -&amp;gt; commodity” lifecycle,
software is at the custom/product intersection, which is somewhere in the middle.&lt;/p>
&lt;p>While the &lt;em>total amount&lt;/em> of code in the world will keep growing, in order to sustain healthy
growth, demand for &amp;ldquo;software regulators&amp;rdquo; will rise to address growing
maintenance and lifecycle management costs. In order to get from product to
commodity and therefore fully permeate society, operating software needs to be
far simpler and
cheaper.&lt;/p>
&lt;p>Economically, McKinsey estimates that tech debt accounts for 40% of IT balance
sheets and up to 50% of developer time [13]. It shows up as a vicious cycle that
organisations increasingly have a harder time escaping from (remember:
complexity begets complexity).&lt;/p>
&lt;center>&lt;img src =/images/complexity_cycle.svgz width="550" height="400"/>&lt;/center>
&lt;p>A report on software quality in 2022 attaches a $2.4 trillion price tag
to technical debt in the form of poor quality and overly complex software [8].
However, if you interchange loosely with &amp;ldquo;technological opportunity cost&amp;rdquo;, the
real business value is of course far
higher:&lt;/p>
&lt;blockquote>
&lt;p>The total technological debt includes entirely &amp;ldquo;untapped&amp;rdquo; digital
transformation in industries (??T) + the canonical &amp;ldquo;poorly tapped&amp;rdquo; form of
tech debt (2.4T).&lt;/p>
&lt;/blockquote>
&lt;p>A lot of words to say: there&amp;rsquo;s a whole lot left to do here.&lt;/p>
&lt;h1 id="footnotes-and-references">Footnotes and References &lt;a href="#footnotes-and-references" class="hash">#&lt;/a>&lt;/h1>
&lt;p>[1] &lt;a href="https://x.com/elonmusk/status/1090689205586472960">https://x.com/elonmusk/status/1090689205586472960&lt;/a>&lt;/p>
&lt;p>[2] &lt;a href="https://en.wikipedia.org/wiki/Entropy_as_an_arrow_of_time">https://en.wikipedia.org/wiki/Entropy_as_an_arrow_of_time&lt;/a>&lt;/p>
&lt;p>[3] &lt;a href="https://pillowlab.princeton.edu/teaching/statneuro2018/slides/notes08_infotheory.pdf">https://pillowlab.princeton.edu/teaching/statneuro2018/slides/notes08_infotheory.pdf&lt;/a>&lt;/p>
&lt;p>[4] &lt;a href="https://arxiv.org/abs/2410.10844">https://arxiv.org/abs/2410.10844&lt;/a>&lt;/p>
&lt;p>[5] &lt;a href="http://www.growth-dynamics.com/articles/Forecasting_Complexity.pdf">http://www.growth-dynamics.com/articles/Forecasting_Complexity.pdf&lt;/a>&lt;/p>
&lt;p>[6] &lt;a href="https://arxiv.org/abs/1405.6903">https://arxiv.org/abs/1405.6903&lt;/a>&lt;/p>
&lt;p>[7] &lt;a href="https://books.google.com/books/about/A_Philosophy_of_Software_Design.html?id=hkfEzgEACAAJ&amp;amp;source=kp_book_description">https://books.google.com/books/about/A_Philosophy_of_Software_Design.html?id=hkfEzgEACAAJ&amp;amp;source=kp_book_description&lt;/a>&lt;/p>
&lt;p>[8] &lt;a href="https://www.it-cisq.org/wp-content/uploads/sites/6/2022/11/CPSQ-Report-Nov-22-2.pdf">https://www.it-cisq.org/wp-content/uploads/sites/6/2022/11/CPSQ-Report-Nov-22-2.pdf&lt;/a>&lt;/p>
&lt;p>[9] &lt;a href="https://www.sonatype.com/blog/the-scale-of-open-source-growth-challenges-and-key-insights">https://www.sonatype.com/blog/the-scale-of-open-source-growth-challenges-and-key-insights&lt;/a>&lt;/p>
&lt;p>[10] &lt;a href="https://en.wikipedia.org/wiki/Lindy_effect">https://en.wikipedia.org/wiki/Lindy_effect&lt;/a>&lt;/p>
&lt;p>[11] &lt;a href="https://en.wikipedia.org/wiki/Everything_is_a_file">https://en.wikipedia.org/wiki/Everything_is_a_file&lt;/a>&lt;/p>
&lt;p>[12] &lt;a href="https://arxiv.org/abs/2309.10668">https://arxiv.org/abs/2309.10668&lt;/a>&lt;/p>
&lt;p>[13] &lt;a href="https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/breaking-technical-debts-vicious-cycle-to-modernize-your-business">https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/breaking-technical-debts-vicious-cycle-to-modernize-your-business&lt;/a>&lt;/p>
&lt;p>[14] &lt;a href="https://en.wikipedia.org/wiki/Black_swan_theory">https://en.wikipedia.org/wiki/Black_swan_theory&lt;/a>&lt;/p>
&lt;p>[15] Theodore Modis presents an excellent information-theoretic analysis of the relation between complexity
and entropy: his claim held true over a 20 year prediction (2002-2022) [4] (&lt;em>a Lindy trend&lt;/em> [10]).&lt;/p>
&lt;p>[16] &lt;a href="https://arxiv.org/pdf/1412.7647">https://arxiv.org/pdf/1412.7647&lt;/a>&lt;/p>
&lt;p>[17] &lt;a href="https://www.hillelwayne.com/post/complexity-constraints/">https://www.hillelwayne.com/post/complexity-constraints/&lt;/a>&lt;/p>
&lt;p>[18] &lt;a href="https://en.wikipedia.org/wiki/Npm_left-pad_incident">https://en.wikipedia.org/wiki/Npm_left-pad_incident&lt;/a>&lt;/p>
&lt;h1 id="appendix-three-hard-problems">Appendix: Three Hard Problems &lt;a href="#appendix-three-hard-problems" class="hash">#&lt;/a>&lt;/h1>
&lt;p>I&amp;rsquo;ve spent a lot of words talking about a problem and tying it to theory and predictions. What does taming complexity look like? How do we roll the ball down
the peak of the hill?&lt;/p>
&lt;p>At the risk of being tongue-in-cheek,
I&amp;rsquo;ll start by saying I think the solution
will involve solving the &amp;ldquo;three hardest problems&amp;rdquo; in computer science.&lt;/p>
&lt;h3 id="naming">Naming &lt;a href="#naming" class="hash">#&lt;/a>&lt;/h3>
&lt;p>Naming standards solve the literal naming problem. Today, standards
work - they just suffer from enforcement and distribution problems. Because of this, the shapes of
software abstractions haven&amp;rsquo;t been globally standardised. It&amp;rsquo;s a problem famous
enough to warrant its own &lt;a href="https://xkcd.com/927/">obligatory XKCD&lt;/a>.&lt;/p>
&lt;p>Because of where we are on the software S-curve, I suspect the move from product to commodity will
entail hiding the naming problem with higher-order abstractions. As people move
up an abstraction layer, user-defined names will simply matter less.&lt;/p>
&lt;h3 id="caching">Caching &lt;a href="#caching" class="hash">#&lt;/a>&lt;/h3>
&lt;p>What&amp;rsquo;s hard about caching isn&amp;rsquo;t maintaining caching
infrastructure, but rather defining the correct cache key and invalidation
policy. This isn&amp;rsquo;t impossible - it again just requires precise and
well-defined behaviour. The CPU caches are a great success story here - they
just needed maturity in the CPU/memory interface.&lt;/p>
&lt;ul>
&lt;li>With cacheability, you get reproducibility, determinism, and more general
fungibility of building blocks. With fungibility of building blocks - you
get commodity-like properties!&lt;/li>
&lt;li>Declarative specifications of components are closely tied to cacheability -
a statement which traces the entire history of software infrastructure
growth and commoditisation (Kubernetes, Docker, Terraform, Nix, Bazel etc).&lt;/li>
&lt;/ul>
&lt;h3 id="off-by-one-errors">Off by one errors &lt;a href="#off-by-one-errors" class="hash">#&lt;/a>&lt;/h3>
&lt;p>These simply represent human hallucinations. Execution
driven feedback. i.e unit test cases encapsulating abstractions &amp;ldquo;solve&amp;rdquo; this well.&lt;/p>
&lt;h3 id="prove-it">Prove it? &lt;a href="#prove-it" class="hash">#&lt;/a>&lt;/h3>
&lt;p>I won&amp;rsquo;t elaborate too much on &amp;ldquo;solutions&amp;rdquo; since I have various
hypotheses-in-testing that need iteration (as opposed to more ideation).&lt;/p>
&lt;p>If you made it this far, clearly some of this was interesting to you. Let&amp;rsquo;s chat! &lt;a href="mailto:rohangupta883@gmail.com">Email&lt;/a> &lt;a href="https://x.com/rohangupta_">X&lt;/a>&lt;/p></description><author>ronalds.vilcins@gmail.com (Ronalds Vilcins)</author><guid>https://grohan.co/2024/12/27/entropy/</guid><pubDate>Fri, 27 Dec 2024 00:00:00 +0000</pubDate></item><item><title>Factorials &amp; Fun with Vim</title><link>https://grohan.co/2023/08/10/vim/</link><description>&lt;p>I recently hit a somewhat important milestone in my life: one year as a Vim
user. Despite its steep learning curve, I used to think that mastering Vim
was mostly about habit formation. I swore by the
cheatsheets and believed I would be golden. After all, a
text editor couldn&amp;rsquo;t be too terribly complex, right?&lt;/p>
&lt;p>However, I soon realised how &lt;em>little&lt;/em> I knew about Vim and how much there
really was to know! In writing this post, I&amp;rsquo;d like to demonstrate (by
way of example) some of
the features of Vim that I found interesting; this is by no means a
comprehensive guide to Vim&amp;rsquo;s capabilities (I would be lying if I said I knew half of what Vim
has to offer), but perhaps a newfound &lt;a href="https://xkcd.com/378/">taste of power&lt;/a>
for the green, fresh-off-VS Code individual looking to dabble with the dark arts.&lt;/p>
&lt;h1 id="computing-factorials">Computing Factorials &lt;a href="#computing-factorials" class="hash">#&lt;/a>&lt;/h1>
&lt;p>This post has a simple goal: computing the factorial. A classic exercise for any
new programmer &amp;ndash; simple to understand, but broad in terms of concepts covered.
More formally, we&amp;rsquo;ll define our problem as writing a Vim routine that takes an input (ex. 5!) and spits out the factorial (120 in our example).&lt;/p>
&lt;p>As a preface, I&amp;rsquo;ll be referencing a variety of Vim features in this post, but I&amp;rsquo;m explicitly choosing not to explain their usage in detail. Instead, I&amp;rsquo;ve tried to link resources that explain syntax and functionality better than I can. This is not a tutorial as much as it is a showcase.&lt;/p>
&lt;h2 id="state-transitions-and-math">State Transitions and Math &lt;a href="#state-transitions-and-math" class="hash">#&lt;/a>&lt;/h2>
&lt;p>To break our problem down a bit, we need a way to multiply (and decrement) numbers in Vim, store the results somewhere, and then repeat that process N times. Sounds simple enough?&lt;/p>
&lt;p>Thankfully, we don&amp;rsquo;t have to implement multiplication from scratch. Vim&amp;rsquo;s got us covered here with the &lt;a href="https://stackoverflow.com/questions/7027741/what-is-the-purpose-of-the-expression-register">expression register&lt;/a>, noted by &amp;ldquo;=&amp;rdquo;. You can access this in insert mode using &lt;code>&amp;lt;C-r&amp;gt;=&lt;/code> and enter any expression you&amp;rsquo;d like. The expression register is clever enough to do basic math operations (addition, subtraction, multiplication, division), but not the factorial &amp;ndash; unfortunately
our life isn&amp;rsquo;t &lt;em>that&lt;/em> easy.&lt;/p>
&lt;p>As an example, you could enter insert mode and input &lt;code>&amp;lt;C-r&amp;gt;=10*7&lt;/code> and the text under your cursor will be filled with 70, neat enough for simple calculations on the fly.&lt;/p>
&lt;p>So we&amp;rsquo;ve got a way to multiply numbers, now we need to do this recursively as
well as store intermediate state somewhere. Let&amp;rsquo;s tackle the state problem
first; for the sake of demonstration, we&amp;rsquo;ll do the most naive thing I can think
of to store our intermediate state: write to text.&lt;/p>
&lt;p>Consider a program with the following states&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-fallback" data-lang="fallback">&lt;span style="display:flex;">&lt;span>7!
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>6!7
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>5!42
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>4!210
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>5040
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The contents after the ! store the intermediate state of the program, while leaving the contents before valid for recursion. We now effectively have program memory equal to the size of the text buffer we&amp;rsquo;re working with!&lt;/p>
&lt;p>With our states defined, we&amp;rsquo;ve now got to think about how exactly we transition between these states. This is where we can introduce the Vim &lt;a href="https://vim.fandom.com/wiki/Search_and_replace">substitute&lt;/a> command, which matches against patterns and replaces them according to a set of rules.&lt;/p>
&lt;p>The command below matches on a number &lt;code>a&lt;/code> followed by ! and replaces it with &lt;code>(a-1)!a&lt;/code>. This is our base case for the recursion. We use the &lt;code>.&lt;/code> operator to concatenate our expressions (using the expression register) together. We&amp;rsquo;re also able to use &lt;code>submatch(0)&lt;/code> which simply matches against everything captured.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-fallback" data-lang="fallback">&lt;span style="display:flex;">&lt;span>:s/\d\+!/\=submatch(0) - 1 . &amp;#34;!&amp;#34; . submatch(0)/
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Next, we need our &amp;ldquo;recursive case&amp;rdquo;, matching on &lt;code>a!b&lt;/code> and replacing with &lt;code>a-1!b*a&lt;/code>. Note the introduction of capturing groups below! Previously, we only matched on one set of characters but now we have 2 distinct operations, which is also why we use indices 1 and 2 for &lt;code>submatch&lt;/code>.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-fallback" data-lang="fallback">&lt;span style="display:flex;">&lt;span>:s/\(\d\+\)!\(\d\+\)/\=submatch(1) - 1 . &amp;#34;!&amp;#34; . submatch(1) \* submatch(2)/
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Fantastic! This will take 6!7 and map it to 5!42, and so on.&lt;/p>
&lt;h2 id="orchestration">Orchestration &lt;a href="#orchestration" class="hash">#&lt;/a>&lt;/h2>
&lt;p>We&amp;rsquo;ve got all the state transformations we need now, but we still need a way to somehow orchestrate them together in a single command. This includes finding a way to programmatically recurse on our substitution expression a variable number of times. Sounds like a lot, but Vim&amp;rsquo;s again got us covered with an idiomatic tool: &lt;a href="https://vim.fandom.com/wiki/Macros">the macro&lt;/a>, which is essentially a way to record sequences of edits and apply them in one shot.&lt;/p>
&lt;p>Our script now looks like this (we record the macro in the &lt;code>@b&lt;/code> register):&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-fallback" data-lang="fallback">&lt;span style="display:flex;">&lt;span>V
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>:s/\d\+!/\=submatch(0) - 1 . &amp;#34;!&amp;#34; . submatch(0)/
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>qb
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>V
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>:s/\(\d\+\)!\(\d\+\)/\=submatch(1) - 1 . &amp;#34;!&amp;#34; . submatch(1) \* submatch(2)/
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>q
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Nearly done, now all we need is a way to find the number of times to execute the macro. Until now, we&amp;rsquo;ve been on a purist edit-only streak, but let&amp;rsquo;s instead introduce some &lt;em>real&lt;/em> programming concepts (variables!?).&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-fallback" data-lang="fallback">&lt;span style="display:flex;">&lt;span>let i = matchstr(getline(&amp;#39;.&amp;#39;), &amp;#39;\d\+&amp;#39;)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>execute &amp;#34;normal! &amp;#34; . i . &amp;#34;@b&amp;#34;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>These two lines will execute our macro &lt;code>i&lt;/code> times, where &lt;code>i&lt;/code> is simply the value of the number we want to compute the factorial of.&lt;/p>
&lt;p>Finally, this will leave us with 1!5040 (in the case of 7!). We need to perform just a bit of cleanup, since we have some extra information in the line (corresponding to our state), which we can do with the following substitution logic:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-fallback" data-lang="fallback">&lt;span style="display:flex;">&lt;span>:s/.\*!\(\d\+\)/\1/
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>or alternatively, using &lt;a href="https://vim.fandom.com/wiki/Power_of_g">globals&lt;/a> (note the use of the Vim exclusive look-ahead &lt;code>\ze&lt;/code>)&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-fallback" data-lang="fallback">&lt;span style="display:flex;">&lt;span>:g/.\*!\ze/normal! df!
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>And that&amp;rsquo;s it! Combining all of these expressions together gives us a general factorial machine.&lt;/p>
&lt;p>&lt;strong>Note&lt;/strong>: I&amp;rsquo;m certainly aware of neater/shorter/better ways to do the same thing
but I wanted to go through as many different &amp;ldquo;Vim trix&amp;rdquo; as I could in a post. If
you&amp;rsquo;re looking to optimise for neatness and flex your Vim muscles, you might want to check out &lt;a href="https://vimgolf.com">Vim Golf&lt;/a>. I&amp;rsquo;m convinced some of the folks on there are not human.&lt;/p>
&lt;h2 id="means-to-an-end">Means to an End &lt;a href="#means-to-an-end" class="hash">#&lt;/a>&lt;/h2>
&lt;p>The purpose of this post is not to encourage the reader to leave
their trusted calculator behind in favour of Vim, but instead to view a
multi-stage programming problem under the lens of powerful, expressive edits.&lt;/p>
&lt;p>Vim, like many other programming tools, is primarily a means to an end. It
simply happens to be powerful enough to do almost anything under the sun (if you are determined and/or
crazy enough).&lt;/p>
&lt;p>What&amp;rsquo;s also remarkable to me is how Vim (circa 1991) has squarely stood its ground in the everchanging landscape of software development tools. Perhaps this is a testament to the timeless quality of design
decisions made &amp;ndash; a seemingly rare feat in the fast-paced modern world of
software. There&amp;rsquo;s a long way to go before I consider myself an expert at Vim, and I&amp;rsquo;m not
sure if there truly is an end in sight; but perhaps that&amp;rsquo;s what makes it a fun
tool to use?&lt;/p>
&lt;p>&lt;em>P.S.&lt;/em>: I write fondly about Vim but I should mention that I&amp;rsquo;m actually
an Emacs user. Plot twist, I know. My editor of choice is &lt;a href="https://github.com/doomemacs/doomemacs">Doom Emacs&lt;/a> which
is a smooth, Space-centric Emacs config with Vim emulation. Why fight over
Emacs vs Vim when you can have both?&lt;/p>
&lt;h2 id="further-reading">Further Reading &lt;a href="#further-reading" class="hash">#&lt;/a>&lt;/h2>
&lt;p>I realise as I write this post that a lot of what I&amp;rsquo;ve written about involves Vim regex.
Here&amp;rsquo;s an &lt;a href="https://dev.to/iggredible/learning-vim-regex-26ep">article&lt;/a> I found
online that does a good job explaining it.&lt;/p>
&lt;p>I&amp;rsquo;m often reminded of &lt;a href="https://stackoverflow.com/questions/1218390/what-is-your-most-productive-shortcut-with-vim/1220118#1220118">this legendary
answer&lt;/a>
which captures the essence of what I think makes Vim so enduringly good:
powerful expressivity combined with a simplistic grammar.&lt;/p>
&lt;p>Lastly, here is a &lt;a href="https://j11g.com/2023/08/07/the-legacy-of-bram-moolenaar/">well-written epitaph&lt;/a> to Bram Moolenar (the creator of Vim).&lt;/p></description><author>ronalds.vilcins@gmail.com (Ronalds Vilcins)</author><guid>https://grohan.co/2023/08/10/vim/</guid><pubDate>Thu, 10 Aug 2023 00:00:00 +0000</pubDate></item><item><title>Halfway There</title><link>https://grohan.co/2022/06/15/halfway/</link><description>&lt;p>College. Times of fun, frolic, and of course, learning. For a couple days now,
I&amp;rsquo;ve been thinking about the past 2 years of my life, focusing on the most
meaningful aspects. I needed a place to write my reflections down, so
I thought perhaps this would make for a fitting first blog post. So here&amp;rsquo;s a
couple shoddily jotted down thoughts on what I think have been my most
significant takeaways from school, so far.&lt;/p>
&lt;h1 id="school-still-sucks-but-less">School still sucks, but less &lt;a href="#school-still-sucks-but-less" class="hash">#&lt;/a>&lt;/h1>
&lt;p>I used to absolutely despise highschool. Part of it was because I was a lazy
slob who wanted to play RuneScape every waking hour but mostly because I didn&amp;rsquo;t
like what I was meant to do in school in order to do well. For example, the
Computer Science curriculum tested us on some pretty jank stuff. We&amp;rsquo;re talking
definitions, case studies, and the most basic OOP. I&amp;rsquo;d spend hours the morning
of the exam rote learning the names of 7 layers of the OSI model, but not what
they &lt;em>actually&lt;/em> were useful for.&lt;/p>
&lt;p>When I got to college, I was hoping that I would be, for the most part, doing
things that I enjoyed, and those things would directly correlate to how well I
was doing. This turned out to be initially true! I felt pretty academically
satisfied after my first year of school, but (to my unfortunate realization) it
just happened to be that intro classes at Penn are run especially well. Not only
do they teach you real CS fundamentals, but they also have fun, knowledgeable
folks on staff and run a great curriculum that kept me engaged with the
material. (aside: I was pleasantly surprised to learn early freshman year that
CIS 1xx staff are pretty tight-knit. They even do BYOs &amp;ndash; I know, not what you&amp;rsquo;d
think when I mention CS course staff)&lt;/p>
&lt;p>As I progressed to some of the upper division classes, I was still learning
interesting things, but it felt like I was increasingly falling into the habit
of autodidactism. For example, I took &lt;strong>CIS 521: Artificial Intelligence&lt;/strong>
sophomore fall. The class was, by all metrics, good, but there was no sense of
community or engagement as in the intro classes. Crucially, it &lt;em>could be done
async and had annoying assignments&lt;/em>. The in-person equivalent just didn&amp;rsquo;t seem
valuable enough to go to. I could just Google any questions I had and, well,
kind of figure things out as I go. This left me thinking &amp;ndash; if I really cared
about the material, I could have just as easily put my head down and grinded out
the class (which was basically a bunch of videos) in a couple weeks, skipping
over the poorly designed bits. I ended up attending zero (0) live lectures for
the class &amp;ndash; I don&amp;rsquo;t even know what room it was in.&lt;/p>
&lt;p>Is that the way I like learning? I&amp;rsquo;m not sure. But one thing is for sure: it&amp;rsquo;s
convenient. Since Fall 2021, I&amp;rsquo;ve been going to almost no classes, because it is
simply more convenient to learn material at home, at my own pace. There&amp;rsquo;s
probably some joke out there about 1.5x lectures being the new norm, but at this
point it&amp;rsquo;s reality. I have been &lt;em>living&lt;/em> on 1.5x and lecture notes.&lt;/p>
&lt;p>While the convenience is certainly nice, it&amp;rsquo;s unfulfilling. I had hoped college
would be more of the interactive, community-oriented experience I had in my
intro classes. For the few upper division classes that do offer this, great. But
for the others, if someone were to download the Canvas page and send it over to
me to cherry-pick modules, I think I&amp;rsquo;d have a better time than having to go
through administrative nonsense, annoying homeworks, and even more annoying
exams &amp;ndash; I&amp;rsquo;m taking most of this upper level stuff for fun anyways.&lt;/p>
&lt;p>For the most part, school isn&amp;rsquo;t the one &amp;ldquo;teaching&amp;rdquo; me, I&amp;rsquo;m learning things
myself. But with the added overhead of whatever the course staff decides is
going to make up my grade. Let me do this stuff on my own, or run classes better
and make people want to show up!&lt;/p>
&lt;!--- markdown auto fill mode --->
&lt;h1 id="i-actually-like-what-i-_do_-now">I actually like what I &lt;em>do&lt;/em> now &lt;a href="#i-actually-like-what-i-_do_-now" class="hash">#&lt;/a>&lt;/h1>
&lt;p>I don&amp;rsquo;t like the question: what are your hobbies? A hobby sounds far too casual
for me. I&amp;rsquo;m the type of person who finds one thing and then &lt;em>does&lt;/em> it
obsessively. In highschool, I played a lot of RuneScape. Like a lot. ~300 full
days worth of RuneScape. You can do the math yourself, but that&amp;rsquo;s a large number
of hours. (fun fact: I only told my mother this number after I got
into college, she still wasn&amp;rsquo;t amused). RuneScape was, truly, what I &lt;em>did&lt;/em> &amp;ndash; the thing
I would think about on the hour long bus ride back from school, and often sneak
late at night to do instead of whatever homework I had due.&lt;/p>
&lt;p>But it wasn&amp;rsquo;t the most practical or interesting (to most people). Indeed, it&amp;rsquo;s
pretty difficult to sit down at dinner with someone and explain why a game
that&amp;rsquo;s older than I am and not very well known was so captivating to me. So,
well, if someone asked me my hobbies were or what I &lt;em>did&lt;/em>, I&amp;rsquo;d look them in the
eye, lie through my teeth, and say that I was into Computer Science. But, at
the time (in highschool) that wasn&amp;rsquo;t really true.&lt;/p>
&lt;p>For context, my first legitimate introduction to real-world programming was the summer
after sophomore year. I was unquestioningly forced to go make use of myself at
my mother&amp;rsquo;s hospital&amp;rsquo;s IT department, where I was tasked with building some
nonsense webapp using everyone&amp;rsquo;s favourite web framework: Django. I didn&amp;rsquo;t know
the first thing about web programming so I began prowling through Django
documentation. I didn&amp;rsquo;t really understand everything, but I understood enough to
combine with code from GitHub and StackOverFlow and piece together something
that just about worked.&lt;/p>
&lt;p>The problem was: I didn&amp;rsquo;t really &lt;em>care&lt;/em> about understanding more, or learning
about the things I was working with. For example, I didn&amp;rsquo;t know what &lt;code>HTTP&lt;/code> was,
just that you could do a bunch of operations with it, like &lt;code>GET&lt;/code> and &lt;code>POST&lt;/code>, and
those operations did exactly what you&amp;rsquo;d think they did. Instead, I wanted to
play RuneScape, or laze around, or watch some show, or eat Indian McDonald&amp;rsquo;s
&lt;a href="https://mcdonaldsblog.in/wp-content/uploads/2016/11/mexican-fries.jpg">Mexican Cheesy
Fries&lt;/a> (I
miss this a lot). Coding was alright, I could do it if I was really asked to,
and I wasn&amp;rsquo;t terrible at it, but it didn&amp;rsquo;t really make me tick.&lt;/p>
&lt;p>Flash-forward to freshman fall at Penn. I come in as a Cognitive Science major.
I mean, brains seemed cool. I half-assed a Coursera course on ML sometime senior
year of highschool, so AI also seemed cool. I&amp;rsquo;d also worked with computers. It
seemed natural to study Cognitive Science, but I wasn&amp;rsquo;t the guy with a plan. On
a whim, I saw that &lt;a href="https://pennlabs.org">Penn Labs&lt;/a> used Django, and it seemed
somewhat interesting, so I thought I&amp;rsquo;d give it a shot. By some miracle, things
work out and I get in. In a week&amp;rsquo;s time, I realize that I&amp;rsquo;ve been lying to
myself.&lt;/p>
&lt;p>Everyone at the club seemed so genuinely passionate about computers, it was nuts
&amp;ndash; they actually cared about how things worked. The folks I worked with actually knew
what &lt;code>HTTP&lt;/code> was, and that was the least of the things they knew about. Strange
words like &lt;code>DevOps, Docker, Kubernetes&lt;/code> would float around in Slack, and I&amp;rsquo;d
have no idea what they meant. Debugging wasn&amp;rsquo;t just Googling and copypasting on
loop, but actually trying things meaningfully. I felt pretty out of place &amp;ndash; a
small fish in a big pond &amp;ndash; but I liked it!&lt;/p>
&lt;p>Not only was I learning a lot, but the enthusiasm for programming was rubbing
off on me. Experiencing this alongside the intro CS classes I talked about
above, I actually began to enjoy computer science. I found myself Googling
random things about Django in my free time, searching up about tools like
&lt;code>pipenv&lt;/code>, and doing things that the younger me would never have done. I was
experiencing a shift in the thing I &lt;em>did&lt;/em>: from RuneScape to coding.&lt;/p>
&lt;p>Things end up panning out after: I transfer to &lt;a href="https://www.nets.upenn.edu/">NETS&lt;/a>, get
more involved with Labs, learn about those strange words I mentioned above through
&lt;a href="https://cis188.org/">CIS 188&lt;/a>, and generally immerse myself in the programming
world. However, looking back, it&amp;rsquo;s funny how things work out sometimes &amp;ndash; I&amp;rsquo;ve often reflected on the fact
that the 10 person IT department in a hospital in Bangalore, India just happened
to use Django as their backend, which is perhaps the only reason I thought I&amp;rsquo;d
have a shot at Penn Labs, and consequently end up growing as much as I did.&lt;/p>
&lt;p>To conclude, I&amp;rsquo;d like to note that the title is a little misleading, I&amp;rsquo;ve always
liked what I &lt;strong>really&lt;/strong> did, but I definitely did fake it for a bit. Now, when
I&amp;rsquo;m at a dinner, and I&amp;rsquo;m prompted about my hobbies, it&amp;rsquo;s a little more
gratifying to say that I like programming &amp;ndash; because I know it is for real.&lt;/p></description><author>ronalds.vilcins@gmail.com (Ronalds Vilcins)</author><guid>https://grohan.co/2022/06/15/halfway/</guid><pubDate>Wed, 15 Jun 2022 00:00:00 +0000</pubDate></item></channel></rss>