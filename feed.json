{"version":"https://jsonfeed.org/version/1","title":"Rohan Gupta","home_page_url":"https://grohan.co","feed_url":"https://ronaldsvilcins.com/feed.json","description":"Rohan Gupta's personal website","icon":"https://ronaldsvilcins.com/assets/apple-touch-icon.png","favicon":"https://ronaldsvilcins.com/assets/favicon.ico","expired":false,"author":{"name":"Ronalds Vilcins","url":"https://ronaldsvilcins.com/"},"items":[{"id":"2e78d0762da7d602b3ce4bf0e822149b52bdf3b0","title":"Reverse Engineering Claude's Token Counter","summary":"How I reverse-engineered a large Claude vocabulary from count_tokens and built a fast local token estimator.","content_text":"Claude 3+ doesn\u0026rsquo;t ship with an open tokenizer.\nIf you\u0026rsquo;re building coding agents, that\u0026rsquo;s a practical problem. You either call Anthropic\u0026rsquo;s count_tokens for everything (slow, online, awkward) or use a proxy estimator (tiktoken, wc -c // {3,4}) and accept large systematic error - up to 20–50% for Claude models. As agents increasingly self-manage context, they need a fast, local way to see token usage across a directory — cloc (count lines of code), but for tokens.\nSo I reverse-engineered a large chunk of Claude\u0026rsquo;s vocabulary from the count_tokens API and built ctoc:\n➜ ctoc git:(main) bazel-bin/ctoc --by-file . -------------------------------------------------------------------------------- File Ext tokens -------------------------------------------------------------------------------- ./MODULE.bazel.lock .lock 22,906 ./ctoc.cc .cc 4,870 ./REPORT.md .md 4,290 ./gen_vocab.py .py 962 ./README.md .md 473 ./BUILD.bazel .bazel 246 ./MODULE.bazel .bazel 153 ./020020-huh-isk .dat 22 -------------------------------------------------------------------------------- SUM (8 files) 33,922 -------------------------------------------------------------------------------- ctoc is an offline estimator of Claude 4.x\u0026rsquo;s count_tokens() API that lands at ~96% accuracy. It\u0026rsquo;s backed by a 36,495-token verified vocabulary (greedy tokenizer), and the code/vocabulary are open source.\nA primer on BPE tokenization # Tokenizers these days mostly use the BPE algorithm. BPE is \u0026ldquo;trained\u0026rdquo; by using a fixed/private corpus of data to create a table of merge rules of frequent tokens.\nSay the merge table (in priority order) is:\nt + h → th th + e → the i + s → is Tokenizing the input \u0026ldquo;this\u0026rdquo;:\nStart: [t, h, i, s] Rule 1: [th, i, s] — merged t+h Rule 2: no \u0026ldquo;th\u0026rdquo;+\u0026ldquo;e\u0026rdquo; pair — skip Rule 3: [th, is] — merged i+s Done: [\u0026ldquo;th\u0026rdquo;, \u0026ldquo;is\u0026rdquo;] That\u0026rsquo;s a toy intuition, not a full encoder spec: practical BPE implementations track merge priorities over adjacent pairs as well.\nNow I\u0026rsquo;m not certain Anthropic uses BPE, but given that most of the industry has converged on it, it\u0026rsquo;s a reasonable bet.\nThis means that without this \u0026ldquo;merge table\u0026rdquo; or private corpus we can\u0026rsquo;t exactly re-create the tokenization.\nBut if we can recover the vocabulary, we don\u0026rsquo;t need the merge table — a greedy longest-match over the known tokens should approximate BPE\u0026rsquo;s token counts closely enough.\nProbing count_tokens to identify vocabulary # We need to do two things to reverse engineer Claude\u0026rsquo;s vocabulary: verify whether a candidate string is a single token, and decompose longer strings to extract tokens we haven\u0026rsquo;t seen yet.\nSingle-token verification # The naive check — send a candidate to count_tokens, see if you get 1 — doesn\u0026rsquo;t work because the API wraps inputs in chat framing. Raw counts include a roughly constant overhead that varies by the type of the first character (7, 8, 9+ for letters, digits, Unicode respectively - what\u0026rsquo;s going on here, Anthropic?).\nSandwich counting fixes this: wrap every probe between markers that we know are single tokens, and subtract the known baseline.\ndef count_tokens(text: str) -\u0026gt; int: marker = \u0026#34;\\u00A7\u0026#34; base = raw_count(marker + marker) return raw_count(marker + text + marker) - base With this, count_tokens(candidate) == 1 reliably tells us whether a candidate is a single token.\nDecomposing strings into tokens # Verification handles candidates we already suspect are single tokens. But when we encounter a longer string - from a dataset, from another tokenizer\u0026rsquo;s vocab - we need a way to extract the individual tokens inside it.\nThe approach: scan for a position i where count(s[:i]) == 1 or count(s[i:]) == 1. That peels off one confirmed token from either end. Recurse on the remainder until the string is fully decomposed (or you hit a chunk that can\u0026rsquo;t be broken down further).\nSo we have the machinery: sandwich counting to verify single tokens, iterative peeling to decompose arbitrary strings. But decompose what?\nBrute-forcing all possible byte strings of length 1-64 is astronomically large, and our decomposition is linear per string, so even clever generation strategies hit the API wall fast (2000/min rate limit). Early experiments with generated data and public datasets proved to be intractable very quickly, though we did pull out a few confirmed tokens as proof of life.\nCross-tokenizer mining # I needed a way to narrow the search space to something that would run in days instead of centuries.\nAfter some back-and-forth with Claude, I arrived on cross-tokenizer mining: decode vocab items from existing BPE tokenizers (tiktoken, HuggingFace models), test each against Claude with sandwich counting, keep only single-token hits.\nTokenizers trained on overlapping internet/programming corpora converge on many of the same subwords. This isn\u0026rsquo;t surprising if you think about it from a compression angle, and the Anthropic-specific tokens are likely not dominant in the distribution.\nSource Vocab size Hit rate New tokens found tiktoken cl100k_base (GPT-4) 100K ~46% ~29,000 tiktoken o200k_base (GPT-4o) 200K ~15% ~4,000 11 HuggingFace US + China models 150-250K each 2-74% ~3,800 Sorting multilingual candidates by cross-tokenizer frequency (tokens appearing in more tokenizers checked first) gave a 74% hit rate on the first 1,000 candidates, declining to ~2% by 20K.\nLong tail # The long-tail vocabulary recoveries here were interesting:\nRe-checking all digit-starting candidates recovered ~1,006 tokens — almost all 3-digit numbers (916, 030, 271) are single tokens. BPE creates single tokens for specific lengths of repeated characters (=, -, etc.) up to length 64 with non-monotonic patterns: \u0026quot;=\u0026quot; * 7 is 2 tokens but \u0026quot;=\u0026quot; * 8 is 1 (likely a BPE merge-order artifact — these lengths matter for markdown fences and separator lines). Space sequences 1-16, tab sequences 1-4, and newline variants are each single tokens - critical for code indentation accuracy (and token efficiency!) Results # The full extraction took 3 days and ~277K API calls.\nEstimator quality (greedy longest-match) # Corpus Efficiency ratio (API tokens / greedy tokens) Python source (9 files) 96.1% Mixed code + docs (9 files) 95.1% English prose (5 samples) 99.2% Per-file variance is low: individual files land within ~2% of their corpus mean, and the estimator consistently over-segments (predicts more tokens than API), which is desirable for conservative context budgeting (read the Appendix to understand why we get this close).\nConclusion # The final outcome of this project - ctoc - is deliberately boring software that just runs a greedy tokenization algorithm on top of our discovered vocabulary. Unknown bytes fall back to 1 token. Fast enough to run as a preflight check in local workflows, or for a coding agent to run as a subprocess.\nIf you care about exact counts for billing-critical paths, use Anthropic\u0026rsquo;s API. If you care about fast, parallelizable, local (self)context management, this could be good enough (and certainly has scope to close the 3-4% gap).\nAs always, this depends on current (tested with Claude 4.x) API behavior and can drift if Anthropic changes tokenizer internals.\nAppendix # Why does greedy tokenization do well? # What turned out to be empirically true but not obvious - greedy longest-match and BPE merge-order produce different segmentations. Why do the token counts nearly converge?\nThere\u0026rsquo;s a reasonable theoretical picture:\nBPE vocabularies are built for greedy compression. Zouhar et al. (2023) formalise BPE training as greedy submodular maximisation of a compression utility. The vocabulary is constructed bottom-up: every multi-character token was formed by merging two shorter tokens that are also in the vocabulary. This hierarchical structure means the longest token at any position tends to be the one BPE would also select.\nByte-level completeness prevents dead ends. BPE vocabularies always include single-byte fallback tokens, so a left-to-right greedy pass can never paint itself into a corner. This eliminates the primary failure mode of greedy algorithms on arbitrary dictionaries.\nBPE training also has an implicit left-to-right bias. Sawada \u0026amp; Goyal (2025) evaluate merge-list-free left-to-right greedy encoding and find it is often comparable to standard merge-based tokenization, with improvements on some tasks and modest degradations on others.\nAnd where greedy and BPE do disagree, the result is typically a rearrangement of boundaries, not a net increase in tokens. ['hell', 'ooo'] vs ['hello', 'oo'] — different boundaries, same count.\nThe upshot: for BPE-family tokenizers with byte fallback, greedy and merge-order counting often converge closely in practice when vocabulary coverage is high. The 4-5% gap in ctoc is plausibly missing tokens forcing over-segmentation. If the vocabulary were complete, greedy counting would likely be even closer to API counts.\n","content_html":"\u003cp\u003eClaude 3+ doesn\u0026rsquo;t ship with an open tokenizer.\u003c/p\u003e\n\u003cp\u003eIf you\u0026rsquo;re building coding agents, that\u0026rsquo;s a practical problem. You either call Anthropic\u0026rsquo;s \u003ca href=\"https://docs.anthropic.com/en/api/messages-count-tokens\"\u003e\u003ccode\u003ecount_tokens\u003c/code\u003e\u003c/a\u003e for everything (slow, online, awkward) or use a proxy estimator (\u003ccode\u003etiktoken\u003c/code\u003e, \u003ccode\u003ewc -c // {3,4}\u003c/code\u003e) and accept large systematic error - up to 20–50% for Claude models. As agents increasingly self-manage context, they need a fast, local way to see token usage across a directory — \u003ca href=\"https://github.com/AlDanial/cloc\"\u003e\u003ccode\u003ecloc\u003c/code\u003e\u003c/a\u003e (count lines of code), but for tokens.\u003c/p\u003e\n\u003cp\u003eSo I reverse-engineered a large chunk of Claude\u0026rsquo;s vocabulary from the \u003ccode\u003ecount_tokens\u003c/code\u003e API and built \u003ca href=\"https://github.com/rohangpta/ctoc/tree/main\"\u003e\u003ccode\u003ectoc\u003c/code\u003e\u003c/a\u003e:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-fallback\" data-lang=\"fallback\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e➜  ctoc git:(main) bazel-bin/ctoc --by-file .\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e--------------------------------------------------------------------------------\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eFile                                Ext                tokens\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e--------------------------------------------------------------------------------\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e./MODULE.bazel.lock                 .lock              22,906\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e./ctoc.cc                           .cc                 4,870\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e./REPORT.md                         .md                 4,290\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e./gen_vocab.py                      .py                   962\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e./README.md                         .md                   473\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e./BUILD.bazel                       .bazel                246\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e./MODULE.bazel                      .bazel                153\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e./020020-huh-isk                    .dat                   22\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e--------------------------------------------------------------------------------\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eSUM (8 files)                                          33,922\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e--------------------------------------------------------------------------------\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003ccode\u003ectoc\u003c/code\u003e is an offline estimator of Claude 4.x\u0026rsquo;s \u003ccode\u003ecount_tokens()\u003c/code\u003e API that lands at ~96% accuracy. It\u0026rsquo;s backed by a 36,495-token verified vocabulary (greedy tokenizer), and the code/vocabulary are \u003ca href=\"https://github.com/rohangpta/ctoc/tree/main\"\u003eopen source\u003c/a\u003e.\u003c/p\u003e\n\u003ch2 id=\"a-primer-on-bpe-tokenization\"\u003eA primer on BPE tokenization \u003ca href=\"#a-primer-on-bpe-tokenization\" class=\"hash\"\u003e#\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eTokenizers these days mostly use the \u003ca href=\"https://en.wikipedia.org/wiki/Byte-pair_encoding\"\u003eBPE algorithm\u003c/a\u003e. BPE is \u0026ldquo;trained\u0026rdquo; by using a fixed/private corpus of data to create a table of merge rules of frequent tokens.\u003c/p\u003e\n\u003cp\u003eSay the merge table (in priority order) is:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003et + h → th\u003c/li\u003e\n\u003cli\u003eth + e → the\u003c/li\u003e\n\u003cli\u003ei + s → is\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eTokenizing the input \u0026ldquo;this\u0026rdquo;:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eStart:  [t, h, i, s]\u003c/li\u003e\n\u003cli\u003eRule 1: [th, i, s]       — merged t+h\u003c/li\u003e\n\u003cli\u003eRule 2: no \u0026ldquo;th\u0026rdquo;+\u0026ldquo;e\u0026rdquo; pair — skip\u003c/li\u003e\n\u003cli\u003eRule 3: [th, is]         — merged i+s\u003c/li\u003e\n\u003cli\u003eDone:   [\u0026ldquo;th\u0026rdquo;, \u0026ldquo;is\u0026rdquo;]\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThat\u0026rsquo;s a toy intuition, not a full encoder spec: practical BPE implementations track merge priorities over adjacent pairs as well.\u003c/p\u003e\n\u003cp\u003eNow I\u0026rsquo;m not \u003cem\u003ecertain\u003c/em\u003e Anthropic uses BPE, but given that most of the industry has converged on it, it\u0026rsquo;s a reasonable bet.\u003c/p\u003e\n\u003cp\u003eThis means that without this \u0026ldquo;merge table\u0026rdquo; or private corpus we can\u0026rsquo;t \u003cem\u003eexactly\u003c/em\u003e re-create the tokenization.\u003c/p\u003e\n\u003cp\u003eBut if we can recover the vocabulary, we don\u0026rsquo;t need the merge table — a greedy longest-match over the known tokens should approximate BPE\u0026rsquo;s token counts closely enough.\u003c/p\u003e\n\u003ch2 id=\"probing-count_tokens-to-identify-vocabulary\"\u003eProbing count_tokens to identify vocabulary \u003ca href=\"#probing-count_tokens-to-identify-vocabulary\" class=\"hash\"\u003e#\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eWe need to do two things to reverse engineer Claude\u0026rsquo;s vocabulary: verify whether a candidate string is a single token, and decompose longer strings to extract tokens we haven\u0026rsquo;t seen yet.\u003c/p\u003e\n\u003ch3 id=\"single-token-verification\"\u003eSingle-token verification \u003ca href=\"#single-token-verification\" class=\"hash\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eThe naive check — send a candidate to \u003ccode\u003ecount_tokens\u003c/code\u003e, see if you get 1 — doesn\u0026rsquo;t work because the API wraps inputs in chat framing. Raw counts include a roughly constant overhead that varies by the type of the first character (7, 8, 9+ for letters, digits, Unicode respectively - what\u0026rsquo;s going on here, Anthropic?).\u003c/p\u003e\n\u003cp\u003eSandwich counting fixes this: wrap every probe between markers that we know are single tokens, and subtract the known baseline.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"font-weight:bold\"\u003edef\u003c/span\u003e count_tokens(text: str) -\u0026gt; int:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    marker = \u003cspan style=\"font-style:italic\"\u003e\u0026#34;\u003c/span\u003e\u003cspan style=\"font-weight:bold;font-style:italic\"\u003e\\u00A7\u003c/span\u003e\u003cspan style=\"font-style:italic\"\u003e\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    base = raw_count(marker + marker)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"font-weight:bold\"\u003ereturn\u003c/span\u003e raw_count(marker + text + marker) - base\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eWith this, \u003ccode\u003ecount_tokens(candidate) == 1\u003c/code\u003e reliably tells us whether a candidate is a single token.\u003c/p\u003e\n\u003ch3 id=\"decomposing-strings-into-tokens\"\u003eDecomposing strings into tokens \u003ca href=\"#decomposing-strings-into-tokens\" class=\"hash\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eVerification handles candidates we already suspect are single tokens. But when we encounter a longer string - from a dataset, from another tokenizer\u0026rsquo;s vocab - we need a way to extract the individual tokens inside it.\u003c/p\u003e\n\u003cp\u003eThe approach: scan for a position \u003ccode\u003ei\u003c/code\u003e where \u003ccode\u003ecount(s[:i]) == 1\u003c/code\u003e or \u003ccode\u003ecount(s[i:]) == 1\u003c/code\u003e. That peels off one confirmed token from either end. Recurse on the remainder until the string is fully decomposed (or you hit a chunk that can\u0026rsquo;t be broken down further).\u003c/p\u003e\n\u003cp\u003eSo we have the machinery: sandwich counting to verify single tokens, iterative peeling to decompose arbitrary strings. But decompose \u003cem\u003ewhat\u003c/em\u003e?\u003c/p\u003e\n\u003cp\u003eBrute-forcing all possible byte strings of length 1-64 is astronomically large, and our decomposition is linear per string, so even clever generation strategies hit the API wall fast (2000/min rate limit). Early experiments with generated data and public datasets proved to be intractable very quickly, though we did pull out a few confirmed tokens as proof of life.\u003c/p\u003e\n\u003ch3 id=\"cross-tokenizer-mining\"\u003eCross-tokenizer mining \u003ca href=\"#cross-tokenizer-mining\" class=\"hash\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eI needed a way to narrow the search space to something that would run in days instead of centuries.\u003c/p\u003e\n\u003cp\u003eAfter some back-and-forth with Claude, I arrived on cross-tokenizer mining: decode vocab items from existing BPE tokenizers (tiktoken, HuggingFace models), test each against Claude with sandwich counting, keep only single-token hits.\u003c/p\u003e\n\u003cp\u003eTokenizers trained on overlapping internet/programming corpora converge on many of the same subwords. This isn\u0026rsquo;t surprising if you think about it from a compression angle, and the Anthropic-specific tokens are likely not dominant in the distribution.\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eSource\u003c/th\u003e\n\u003cth\u003eVocab size\u003c/th\u003e\n\u003cth\u003eHit rate\u003c/th\u003e\n\u003cth\u003eNew tokens found\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003etiktoken cl100k_base (GPT-4)\u003c/td\u003e\n\u003ctd\u003e100K\u003c/td\u003e\n\u003ctd\u003e~46%\u003c/td\u003e\n\u003ctd\u003e~29,000\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003etiktoken o200k_base (GPT-4o)\u003c/td\u003e\n\u003ctd\u003e200K\u003c/td\u003e\n\u003ctd\u003e~15%\u003c/td\u003e\n\u003ctd\u003e~4,000\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e11 HuggingFace US + China models\u003c/td\u003e\n\u003ctd\u003e150-250K each\u003c/td\u003e\n\u003ctd\u003e2-74%\u003c/td\u003e\n\u003ctd\u003e~3,800\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eSorting multilingual candidates by cross-tokenizer frequency (tokens appearing in more tokenizers checked first) gave a 74% hit rate on the first 1,000 candidates, declining to ~2% by 20K.\u003c/p\u003e\n\u003ch3 id=\"long-tail\"\u003eLong tail \u003ca href=\"#long-tail\" class=\"hash\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eThe long-tail vocabulary recoveries here were interesting:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eRe-checking all digit-starting candidates recovered ~1,006 tokens — almost all 3-digit numbers (\u003ccode\u003e916\u003c/code\u003e, \u003ccode\u003e030\u003c/code\u003e, \u003ccode\u003e271\u003c/code\u003e) are single tokens.\u003c/li\u003e\n\u003cli\u003eBPE creates single tokens for specific lengths of repeated characters (\u003ccode\u003e=\u003c/code\u003e, \u003ccode\u003e-\u003c/code\u003e, etc.) up to length 64 with non-monotonic patterns: \u003ccode\u003e\u0026quot;=\u0026quot; * 7\u003c/code\u003e is 2 tokens but \u003ccode\u003e\u0026quot;=\u0026quot; * 8\u003c/code\u003e is 1 (likely a BPE merge-order artifact — these lengths matter for markdown fences and separator lines). Space sequences 1-16, tab sequences 1-4, and newline variants are each single tokens - critical for code indentation accuracy (and token efficiency!)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"results\"\u003eResults \u003ca href=\"#results\" class=\"hash\"\u003e#\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eThe full extraction took 3 days and ~277K API calls.\u003c/p\u003e\n\u003ch3 id=\"estimator-quality-greedy-longest-match\"\u003eEstimator quality (greedy longest-match) \u003ca href=\"#estimator-quality-greedy-longest-match\" class=\"hash\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eCorpus\u003c/th\u003e\n\u003cth style=\"text-align:right\"\u003eEfficiency ratio (\u003ccode\u003eAPI tokens / greedy tokens\u003c/code\u003e)\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003ePython source (9 files)\u003c/td\u003e\n\u003ctd style=\"text-align:right\"\u003e96.1%\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eMixed code + docs (9 files)\u003c/td\u003e\n\u003ctd style=\"text-align:right\"\u003e95.1%\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eEnglish prose (5 samples)\u003c/td\u003e\n\u003ctd style=\"text-align:right\"\u003e99.2%\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003ePer-file variance is low: individual files land within ~2% of their corpus mean, and the estimator consistently over-segments (predicts more tokens than API), which is desirable for conservative context budgeting (read the Appendix to understand \u003cem\u003ewhy\u003c/em\u003e we get this close).\u003c/p\u003e\n\u003ch3 id=\"conclusion\"\u003eConclusion \u003ca href=\"#conclusion\" class=\"hash\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eThe final outcome of this project - \u003ccode\u003ectoc\u003c/code\u003e - is deliberately boring software that just runs a greedy tokenization algorithm on top of our discovered vocabulary. Unknown bytes fall back to 1 token. Fast enough to run as a preflight check in local workflows, or for a coding agent to run as a subprocess.\u003c/p\u003e\n\u003cp\u003eIf you care about exact counts for billing-critical paths, use Anthropic\u0026rsquo;s API. If you care about fast, parallelizable, local (self)context management, this could be good enough (and certainly has scope to close the 3-4% gap).\u003c/p\u003e\n\u003cp\u003eAs always, this depends on current (tested with Claude 4.x) API behavior and can drift if Anthropic changes tokenizer internals.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"appendix\"\u003eAppendix \u003ca href=\"#appendix\" class=\"hash\"\u003e#\u003c/a\u003e\u003c/h2\u003e\n\u003ch3 id=\"why-does-greedy-tokenization-do-well\"\u003eWhy does greedy tokenization do well? \u003ca href=\"#why-does-greedy-tokenization-do-well\" class=\"hash\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eWhat turned out to be empirically true but not obvious - greedy longest-match and BPE merge-order produce \u003cem\u003edifferent segmentations\u003c/em\u003e. Why do the token \u003cem\u003ecounts\u003c/em\u003e nearly converge?\u003c/p\u003e\n\u003cp\u003eThere\u0026rsquo;s a reasonable theoretical picture:\u003c/p\u003e\n\u003cp\u003eBPE vocabularies are built for greedy compression. \u003ca href=\"https://aclanthology.org/2023.findings-acl.38/\"\u003eZouhar et al. (2023)\u003c/a\u003e formalise BPE training as greedy submodular maximisation of a compression utility. The vocabulary is constructed bottom-up: every multi-character token was formed by merging two shorter tokens that are also in the vocabulary. This hierarchical structure means the longest token at any position tends to be the one BPE would also select.\u003c/p\u003e\n\u003cp\u003eByte-level completeness prevents dead ends. BPE vocabularies always include single-byte fallback tokens, so a left-to-right greedy pass can never paint itself into a corner. This eliminates the primary failure mode of greedy algorithms on arbitrary dictionaries.\u003c/p\u003e\n\u003cp\u003eBPE training also has an implicit left-to-right bias. \u003ca href=\"https://aclanthology.org/2025.emnlp-main.1775/\"\u003eSawada \u0026amp; Goyal (2025)\u003c/a\u003e evaluate merge-list-free left-to-right greedy encoding and find it is often comparable to standard merge-based tokenization, with improvements on some tasks and modest degradations on others.\u003c/p\u003e\n\u003cp\u003eAnd where greedy and BPE do disagree, the result is typically a rearrangement of boundaries, not a net increase in tokens. \u003ccode\u003e['hell', 'ooo']\u003c/code\u003e vs \u003ccode\u003e['hello', 'oo']\u003c/code\u003e — different boundaries, same count.\u003c/p\u003e\n\u003cp\u003eThe upshot: for BPE-family tokenizers with byte fallback, greedy and merge-order counting often converge closely in practice when vocabulary coverage is high. The 4-5% gap in \u003ccode\u003ectoc\u003c/code\u003e is plausibly missing tokens forcing over-segmentation. If the vocabulary were complete, greedy counting would likely be even closer to API counts.\u003c/p\u003e\n","url":"https://grohan.co/2026/02/10/ctoc/","image":"https://grohan.cophotos/<no value>","banner_image":"https://grohan.cophotos/<no value>","date_published":"10026-10-09T20:1010:00+00:00","date_modified":"10026-10-09T20:1010:00+00:00","author":{"name":"Ronalds Vilcins","url":"https://ronaldsvilcins.com/"}},{"id":"549babcbca490c7fa92cbb47c37c93427c9a99d7","title":"Compressed Filesystems á la Language Models","summary":"","content_text":"Every systems engineer at some point in their journey yearns to write a filesystem. This sounds daunting at first - and writing a battle-tested filesystem is hard - but the minimal surface area for a \u0026ldquo;working\u0026rdquo; FS is surprisingly small, simple, and in-distribution for coding agents.\nIn fact, one of my smoke tests for new coding models is seeing how good of a filesystem they can one-shot! At some point, I had quite a few filesystems lying around - and coding models were getting pretty good - which made me wonder if the models were intelligent enough to actually model the filesystem engine itself?\nA filesystem is the perfect black-box API to model with wacky backends (see \u0026ldquo;Harder drives\u0026rdquo;), and besides the joy of training an LLM for fun - there were a few deeper truths about language models that I wanted to explore.\nTraining a filesystem # So I set upon training a filesystem. Building on top of one of my throwaway FUSEs, a few rounds with Claude repurposed it to loopback against the host with added logging, two things I needed to generate reference fine-tuning data:\nclass LoggingLoopbackFS(LoggingMixIn, Operations): \u0026#34;\u0026#34;\u0026#34; A loopback FUSE filesystem that logs all operations for training data. This implementation delegates all filesystem operations to a real directory on the host filesystem, ensuring perfect semantic correctness while logging every operation for LLM training data. \u0026#34;\u0026#34;\u0026#34; I then wrote a filesystem interaction simulator, which sampled various operations against a sandboxed LoggingLoopbackFS to generate diverse FUSE prompt/completion pairs. Concretely, I captured only the minimal set of operations needed for R/W-ish capability (no open, xattrs, fsync etc).\nAlongside the FUSE operation, I captured the full filesystem state at every turn. I experimented with various formats, including an ASCII-art representation, but ultimately settled on XML since it enforces prompt boundaries clearly and had canonical parsers available.\nWith prompts including the FUSE operation + XML filesystem tree, the model learned two forms of completions:\nReads (\u0026lt;R\u0026gt;) requested the content / metadata as per the operation (getattr / readdir / read) Writes (\u0026lt;W\u0026gt;) requested the model to output the full filesystem tree state, after modification (unlink / chmod / truncate / write) Example prompt (read):\n\u0026lt;R\u0026gt; read(\u0026#39;/usr14/log767.rs\u0026#39;, size=4096, offset=0, fh=4) --- \u0026lt;filesystem\u0026gt; \u0026lt;directory path=\u0026#34;/\u0026#34; name=\u0026#34;/\u0026#34; mode=\u0026#34;755\u0026#34; owner=\u0026#34;root\u0026#34; group=\u0026#34;root\u0026#34; mtime=\u0026#34;2025-01-01T00:00:00\u0026#34;\u0026gt; \u0026lt;directory path=\u0026#34;usr14\u0026#34; name=\u0026#34;usr14\u0026#34; mode=\u0026#34;755\u0026#34; owner=\u0026#34;root\u0026#34; group=\u0026#34;root\u0026#34; mtime=\u0026#34;2025-01-01T00:00:00\u0026#34;\u0026gt; \u0026lt;file path=\u0026#34;usr14/log767.rs\u0026#34; name=\u0026#34;log767.rs\u0026#34; mode=\u0026#34;644\u0026#34; owner=\u0026#34;root\u0026#34; group=\u0026#34;root\u0026#34; mtime=\u0026#34;2025-01-01T00:00:01\u0026#34; size=\u0026#34;276\u0026#34;\u0026gt; \u0026lt;body\u0026gt;fn main() { match process(7) { Ok(result) =\u0026amp;gt; println!(\u0026#34;Result: {}\u0026#34;, result), Err(e) =\u0026amp;gt; eprintln!(\u0026#34;Error: {}\u0026#34;, e), } \u0026lt;/body\u0026gt; \u0026lt;/file\u0026gt; \u0026lt;file path=\u0026#34;usr14/temp912.sh\u0026#34; name=\u0026#34;temp912.sh\u0026#34; mode=\u0026#34;644\u0026#34; owner=\u0026#34;root\u0026#34; group=\u0026#34;root\u0026#34; mtime=\u0026#34;2025-01-01T00:00:01\u0026#34; size=\u0026#34;268\u0026#34;\u0026gt; \u0026lt;body\u0026gt;#!/bin/bash echo \u0026#34;temp912\u0026#34; || exit 1 \u0026lt;/body\u0026gt; \u0026lt;/file\u0026gt; \u0026lt;/directory\u0026gt; \u0026lt;/directory\u0026gt; \u0026lt;/filesystem\u0026gt; Completion:\nfn main() { match process(7) { Ok(result) =\u0026gt; println!(\u0026#34;Result: {}\u0026#34;, result), Err(e) =\u0026gt; eprintln!(\u0026#34;Error: {}\u0026#34;, e), } } Fine-tuning # Once I had clean, representative, and diverse filesystem simulation data, actually running SFT was pretty straightforward on Modal. Over a few iteration cycles spread across nibbles of spare time, I ended up with ~98% accuracy on a hold-out eval after 8 epochs of SFT on a N=15000 dataset with Qwen3-4b.\nMost of my time here was spent cleaning generated data and ensuring we represented every FUSE operation sufficiently + generated enough \u0026ldquo;complex\u0026rdquo; trees to learn on.\nAt this point, I wrote \u0026hellip; possibly the smallest filesystem I\u0026rsquo;ve seen\u0026hellip; to give my model a spin in the real world. Every FUSE operation was a passthrough to the LLM, for example:\nclass LLMFuse(LoggingMixin, Operations): ... def chmod(self, path, mode): \u0026#34;\u0026#34;\u0026#34;Change file permissions.\u0026#34;\u0026#34;\u0026#34; response = self._query_llm_for_operation(\u0026#39;chmod\u0026#39;, path, mode=oct(mode)) if not self._handle_llm_response(response): raise FuseOSError(ENOENT) return 0 ... Nice! I now had a mountable FUSE that was entirely \u0026ldquo;implemented\u0026rdquo; by a language model. As you can see below, I was able to ls around it, echo into files, and cat them back out.\nPoking around a Docker container with a mounted LLMFuse. Compressing the filesystem # Perhaps the largest glaring inefficiency in this set up is the sheer verbosity of the XML-based representation. I was using many bytes to represent attributes and tree structure that could be encoded far more efficiently (~O(bits)) in a standard C struct.\nHowever, as I was fine-tuning on the XML filesystem tree representation, I was baking in this very structure into the weights and probability distributions of my Qwen fork! If only there was a way to leverage this to compress state\u0026hellip;\nTwo sides of the same coin # As it turns out, compression and AI are intimately related. Using LLMs to lossily compress text is one of the most common applications, so it\u0026rsquo;s not entirely unintuitive. However, one researcher (Marcus Hutter) claimed back in 2006 that they are equivalent (and in fact bet $500K on this claim!).\nPresciently, Hutter appears to be absolutely right. His enwik8 and enwik9\u0026rsquo;s benchmark datasets are, today, best compressed by a 169M parameter LLM (trained by none other than Fabrice Bellard in 2023).\nThat\u0026rsquo;s a bit perplexing on the first glance. Surely LLM compression isn\u0026rsquo;t reversible? What kind of voodoo magic was going on here?\nArithmetic coding # The algorithm that enables reversible compression using LLMs is called \u0026ldquo;arithmetic coding\u0026rdquo; and it builds upon a 1948 result by Claude Shannon.\nResearchers at DeepMind (including Hutter himself) have explained the math in detail, so I\u0026rsquo;ll direct the most inquisitive of you readers there, but for a simplified understanding of what\u0026rsquo;s going on, forget everything you might know about working with LLMs today. There\u0026rsquo;s no prompting involved!\nLet\u0026rsquo;s assume the following is true for some predictive model \\(M\\)\nLorem has first-word probability = 0.57. Ipsum has second-word conditional probability = 0.67 (joint 0.38). Dolor has a third word conditional probability = 0.5 (joint 0.19). \u0026hellip;\nso on and so forth until you reach the end of the string you want to compress and you end up with some \u0026ldquo;final interval width\u0026rdquo; \\(P(m)\\) on the real interval \\([0,1]\\) which represents your string.\nLet\u0026rsquo;s suppose in our example this turns out to be 0.012. We can represent this decimal in roughly \\(- \\log_{2}{P(m)} = 6.4\\) bits, which is our final compression size.\nThere\u0026rsquo;s a few elegant things about this algorithm:\nAny number within this interval is uniquely determined by tracing the arithmetic coding algorithm through the specific probabilistic model\u0026rsquo;s weights. \u0026ldquo;Decoding\u0026rdquo; is simply a retracing operation (see the line through the probability distributions above) The inverse log relationship between predictive power \\(P(m)\\) and compression pushes the burden of the \u0026ldquo;hard compression problem\u0026rdquo; to deep learning machinery which can encode high-dimensional text patterns within model weights, yielding far better compression ratios than deterministic algorithms. Sounds cool! But how good really is this compression? On comparing arithmetic coding backed by Qwen3-4B against gzip for lipsum.txt, we already see pretty dramatic results:\nMethod Size (bytes) Compression Impact Original (plain) 446 — gzip 298 ~33% smaller llmencode 13 ~97% smaller (note: llmencode is my implementation of arithmetic coding)\n22x better compression than gzip is pretty ridiculous! A caveat here is that lipsum.txt is heavily represented in training data, but 5-20x efficiency gains broadly hold for all text data that (looks like) it\u0026rsquo;s been on the internet.\nSelf-compression # Now, back to our filesystem. The XML overhead we were worried about now can be \u0026ldquo;compressed away\u0026rdquo; by the fine-tuned model. Using the same toy filesystem from the Docker container demo above:\n\u0026lt;filesystem\u0026gt; \u0026lt;directory path=\u0026#34;/\u0026#34; name=\u0026#34;/\u0026#34; mode=\u0026#34;755\u0026#34; owner=\u0026#34;root\u0026#34; group=\u0026#34;root\u0026#34; mtime=\u0026#34;2025-01-01T00:00:00\u0026#34;\u0026gt; \u0026lt;directory path=\u0026#34;testdir\u0026#34; name=\u0026#34;testdir\u0026#34; mode=\u0026#34;755\u0026#34; owner=\u0026#34;root\u0026#34; group=\u0026#34;root\u0026#34; mtime=\u0026#34;2025-01-01T00:00:00\u0026#34; /\u0026gt; \u0026lt;file path=\u0026#34;testfile.txt\u0026#34; name=\u0026#34;testfile.txt\u0026#34; mode=\u0026#34;644\u0026#34; owner=\u0026#34;root\u0026#34; group=\u0026#34;root\u0026#34; mtime=\u0026#34;2025-01-01T00:00:01\u0026#34; size=\u0026#34;14\u0026#34;\u0026gt; \u0026lt;body\u0026gt;hello llmfuse \u0026lt;/body\u0026gt; \u0026lt;/file\u0026gt; \u0026lt;/directory\u0026gt; \u0026lt;/filesystem\u0026gt; Model Original (bytes) Compressed (bytes) Ratio Base Qwen3-4B 394 38 10.4x Fine-tuned Qwen3-4B 394 21 18.8x The fine-tuned model achieves 44.7% better compression on XML filesystem trees - the very format it was trained to predict. This is the \u0026ldquo;self-compression\u0026rdquo; effect: by baking the XML structure into the model weights during fine-tuning, the arithmetic coder can represent that structure in fewer bits.\nSelf-compression in filesystems isn\u0026rsquo;t a novel concept. For example, there exists the squashfs tool (created in 2002) to create R/O compressed filesystems. Squashfs compresses files, inodes, and directories together, not unlike what we\u0026rsquo;re doing here!\nUnder the hood, squashfs just wraps gzip/zstd/your favourite compression algorithm. So for plain-text data, squashfs compression stats pale in the face of llmfuse:\nMethod Compressed Size Notes squashfs (gzip) 171 bytes gzip-compressed file contents, inodes, directory tables llmfuse (fine-tuned) 21 bytes Arithmetic coded XML state For the same filesystem tree (one directory, one 14-byte text file), llmfuse achieves ~8x better compression than squashfs (see methodology in appendix).\nThe difference comes down to llmencode being far better than gzip on text data + XML structure - especially when the model has been fine-tuned on exactly that structure.\nConclusion # What started off as a little experiment mostly to get my hands dirty with training and inference evolved into a full blown nerd snipe and intellectual adventure. Thanks for making it this far!\nI entirely recognize that this is a \u0026ldquo;toy\u0026rdquo; experiment under a very specific setup; with that said, the numbers above are pretty eye-popping, and the question I\u0026rsquo;ve been trying to answer as I write this up is: does this have any real-world potential?\nOf course, in the short term, there\u0026rsquo;s a whole host of caveats: you need an LLM, likely a GPU, all your data is in the context window (which we know scales poorly), and this only works on text data.\nStill, it\u0026rsquo;s intriguing to wonder whether the very engines that will likely dominate all \u0026ldquo;text generation\u0026rdquo; going forward can be used to compress their own data? Perhaps in a distant future, where running LLMs at the edge makes sense, or for specific kinds of workflows where data is read very infrequently.\nOverall, I\u0026rsquo;m grateful to Peyton at Modal for the compute credits. Running a somewhat unconventional experiment like this wouldn\u0026rsquo;t have been possible without full control over the training and inference code, and extremely tedious without the simplicity of running ML infra on Modal! It\u0026rsquo;s truly awesome to be able to just modal deploy and get my own private inference endpoints, or just modal run to prototype some code on the cloud.\nAppendix # Source Code # All of the source code for this experiment, particularly llmfuse and llmencode are open-sourced under MIT.\nllmencode is abstracted into a CLI utility that you can run locally. Inference on 4B models is slow, but entirely possible on consumer hardware. I prototyped most of this code by running on a 2021 MacBook Pro, before productionizing on Modal.\nA fun experiment / party trick to identify how \u0026ldquo;common\u0026rdquo; a certain string is in training data is to look at its llmencode compression ratio!\nSquashFS comparison methodology # The raw .sqsh file is 4096 bytes due to block alignment padding. To find the actual compressed size, I used xxd to inspect the binary and found the last non-zero byte at offset 266 (267 bytes total). Subtracting the fixed 96-byte superblock header gives us 171 bytes of actual gzip-compressed content - everything needed to reconstruct the filesystem.\nCompression as a metric # It\u0026rsquo;s equally interesting to think about compression as a metric. An angle I\u0026rsquo;d considered is doing some kind of RL on the arithmetic coded compression number itself.\nIs that simply equivalent to the pre-training objective (due to the prediction-compression duality)? Or does the \u0026ldquo;sequence-level\u0026rdquo; objective add something more\u0026hellip; interesting to the mix. Please reach out if you have thoughts!\nEDIT:\nAs it turns out - it is indeed equivalent to the pre-training objective!\nPre-training aims to maximize the probability of training data. For a sequence \\(x = (x_1, x_2, \\ldots, x_T)\\), we decompose via the chain rule:\n$$P(x) = \\prod_{t=1}^{T} P(x_t \\mid x_{\u003c t})$$ Taking logarithms (for numerical stability and to convert products to sums):\n$$\\log P(x) = \\sum_{t=1}^{T} \\log P(x_t \\mid x_{\u003c t})$$ Maximizing log-probability is equivalent to minimizing its negative — the negative log-likelihood:\n$$\\mathcal{L}_{\\text{NLL}}(\\theta) = -\\sum_{t=1}^{T} \\log P_\\theta(x_t \\mid x_{\u003c t})$$ Arithmetic coding maps the entire sequence \\(x\\) to an interval on \\([0,1]\\) of width equal to its joint probability \\(P(x)\\). To uniquely specify a point within an interval of width \\(P(x)\\), we require \\(-\\log_2 P(x)\\) bits.\nExpanding this using the chain rule from above gives us the sum:\n$$L_{\\text{compressed}}(x) = -\\log_2 P(x) = -\\log_2 \\prod_{t=1}^{T} P_\\theta(x_t \\mid x_{\u0026lt; t}) = -\\sum_{t=1}^{T} \\log_2 P_\\theta(x_t \\mid x_{\u0026lt; t})$$\nThese differ only by logarithm base. Since \\(\\log_2 P = \\frac{\\ln P}{\\ln 2}\\), and \\(\\frac{1}{\\ln 2} \\approx 1.44\\) is a positive constant:\n$$\\arg\\min_\\theta \\mathcal{L}_{\\text{NLL}} = \\arg\\min_\\theta L_{\\text{compressed}}$$ The same \\(\\theta^*\\) minimizes both. \\(\\square\\)\nBeyond resolving the RL question, there\u0026rsquo;s something quietly beautiful about this framing. In my view, viewing the pre-training objective as compression is a simple way to \u0026ldquo;grok\u0026rdquo; the math.\n","content_html":"\u003cp\u003eEvery systems engineer at some point in their journey yearns to write\na filesystem. This sounds daunting at first - and writing a battle-tested filesystem \u003cem\u003eis\u003c/em\u003e hard - but the minimal surface area for a \u0026ldquo;working\u0026rdquo; FS is surprisingly small, simple, and in-distribution for coding agents.\u003c/p\u003e\n\u003cp\u003eIn fact, one of my smoke tests for new coding models is seeing how good of\na filesystem they can one-shot! At some point, I had quite a few filesystems lying around - and coding models were getting pretty good - which made me wonder if the models were intelligent enough to actually model the filesystem engine itself?\u003c/p\u003e\n\u003cp\u003eA filesystem is the perfect black-box API to model with wacky backends (see \u003ca href=\"http://tom7.org/papers/murphy2022harder.pdf\"\u003e\u0026ldquo;Harder drives\u0026rdquo;\u003c/a\u003e), and besides the joy of training an LLM for fun - there were a few deeper truths about language models that I wanted to explore.\u003c/p\u003e\n\u003ch1 id=\"training-a-filesystem\"\u003eTraining a filesystem \u003ca href=\"#training-a-filesystem\" class=\"hash\"\u003e#\u003c/a\u003e\u003c/h1\u003e\n\u003cp\u003eSo I set upon training a filesystem. Building on top of one of my throwaway\nFUSEs, a few rounds with Claude repurposed it to loopback against the host\nwith added logging, two things I needed to generate reference fine-tuning data:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-py\" data-lang=\"py\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"font-weight:bold\"\u003eclass\u003c/span\u003e \u003cspan style=\"font-weight:bold\"\u003eLoggingLoopbackFS\u003c/span\u003e(LoggingMixIn, Operations):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"font-style:italic\"\u003e\u0026#34;\u0026#34;\u0026#34;\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"font-style:italic\"\u003e    A loopback FUSE filesystem that logs all operations for training data.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"font-style:italic\"\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"font-style:italic\"\u003e    This implementation delegates all filesystem operations to a real directory\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"font-style:italic\"\u003e    on the host filesystem, ensuring perfect semantic correctness while logging\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"font-style:italic\"\u003e    every operation for LLM training data.\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"font-style:italic\"\u003e    \u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eI then wrote a filesystem interaction simulator, which sampled various\noperations against a sandboxed \u003ccode\u003eLoggingLoopbackFS\u003c/code\u003e to generate diverse FUSE\nprompt/completion pairs. Concretely, I captured only the minimal set of \u003ca href=\"https://github.com/rohangpta/llmfuse/blob/main/train/generate_data.py#L246\"\u003eoperations needed\u003c/a\u003e for\nR/W-ish capability (no open, xattrs, fsync etc).\u003c/p\u003e\n\u003cp\u003eAlongside the FUSE operation, I captured the full filesystem state at every\nturn. I experimented with various formats, including an ASCII-art\nrepresentation, but ultimately settled on XML since it enforces prompt\nboundaries clearly and had canonical parsers available.\u003c/p\u003e\n\u003cp\u003eWith prompts including the FUSE operation + XML filesystem tree, the model learned two forms of completions:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eReads (\u0026lt;R\u0026gt;) requested the content / metadata as per the operation\n(\u003ccode\u003egetattr\u003c/code\u003e / \u003ccode\u003ereaddir\u003c/code\u003e / \u003ccode\u003eread\u003c/code\u003e)\u003c/li\u003e\n\u003cli\u003eWrites (\u0026lt;W\u0026gt;) requested the model to output the full filesystem tree state,\nafter modification (\u003ccode\u003eunlink\u003c/code\u003e / \u003ccode\u003echmod\u003c/code\u003e / \u003ccode\u003etruncate\u003c/code\u003e / \u003ccode\u003ewrite\u003c/code\u003e)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eExample prompt (read):\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-fallback\" data-lang=\"fallback\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u0026lt;R\u0026gt;\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eread(\u0026#39;/usr14/log767.rs\u0026#39;, size=4096, offset=0, fh=4) \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e---\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u0026lt;filesystem\u0026gt;\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u0026lt;directory path=\u0026#34;/\u0026#34; name=\u0026#34;/\u0026#34; mode=\u0026#34;755\u0026#34; owner=\u0026#34;root\u0026#34; group=\u0026#34;root\u0026#34;\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003emtime=\u0026#34;2025-01-01T00:00:00\u0026#34;\u0026gt;\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u0026lt;directory path=\u0026#34;usr14\u0026#34; name=\u0026#34;usr14\u0026#34; mode=\u0026#34;755\u0026#34; owner=\u0026#34;root\u0026#34; group=\u0026#34;root\u0026#34;\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003emtime=\u0026#34;2025-01-01T00:00:00\u0026#34;\u0026gt;\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      \u0026lt;file path=\u0026#34;usr14/log767.rs\u0026#34; name=\u0026#34;log767.rs\u0026#34; mode=\u0026#34;644\u0026#34; owner=\u0026#34;root\u0026#34;\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003egroup=\u0026#34;root\u0026#34; mtime=\u0026#34;2025-01-01T00:00:01\u0026#34; size=\u0026#34;276\u0026#34;\u0026gt;\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u0026lt;body\u0026gt;fn main() {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    match process(7) {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        Ok(result) =\u0026amp;gt; println!(\u0026#34;Result: {}\u0026#34;, result),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        Err(e) =\u0026amp;gt; eprintln!(\u0026#34;Error: {}\u0026#34;, e),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    }\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u0026lt;/body\u0026gt;\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      \u0026lt;/file\u0026gt;\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      \u0026lt;file path=\u0026#34;usr14/temp912.sh\u0026#34; name=\u0026#34;temp912.sh\u0026#34; mode=\u0026#34;644\u0026#34; owner=\u0026#34;root\u0026#34;\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003egroup=\u0026#34;root\u0026#34; mtime=\u0026#34;2025-01-01T00:00:01\u0026#34; size=\u0026#34;268\u0026#34;\u0026gt;\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u0026lt;body\u0026gt;#!/bin/bash \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e         echo \u0026#34;temp912\u0026#34; || exit 1\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e       \u0026lt;/body\u0026gt;\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      \u0026lt;/file\u0026gt;\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u0026lt;/directory\u0026gt;\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u0026lt;/directory\u0026gt;\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u0026lt;/filesystem\u0026gt;\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eCompletion:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-rs\" data-lang=\"rs\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"font-weight:bold\"\u003efn\u003c/span\u003e main() {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"font-weight:bold\"\u003ematch\u003c/span\u003e process(7) {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        Ok(result) =\u0026gt; println!(\u003cspan style=\"font-style:italic\"\u003e\u0026#34;Result: \u003c/span\u003e\u003cspan style=\"font-weight:bold;font-style:italic\"\u003e{}\u003c/span\u003e\u003cspan style=\"font-style:italic\"\u003e\u0026#34;\u003c/span\u003e, result),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        Err(e) =\u0026gt; eprintln!(\u003cspan style=\"font-style:italic\"\u003e\u0026#34;Error: \u003c/span\u003e\u003cspan style=\"font-weight:bold;font-style:italic\"\u003e{}\u003c/span\u003e\u003cspan style=\"font-style:italic\"\u003e\u0026#34;\u003c/span\u003e, e),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    }\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e}\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"fine-tuning\"\u003eFine-tuning \u003ca href=\"#fine-tuning\" class=\"hash\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eOnce I had clean, representative, and diverse filesystem simulation data, actually running SFT was pretty straightforward on Modal. Over a few iteration cycles spread across nibbles of spare time, I ended up with ~98% accuracy on a hold-out eval after 8 epochs of SFT on a N=15000 dataset with Qwen3-4b.\u003c/p\u003e\n\u003cp\u003eMost of my time here was spent cleaning generated data and ensuring we represented every FUSE operation sufficiently + generated enough \u0026ldquo;complex\u0026rdquo; trees to learn on.\u003c/p\u003e\n\u003cp\u003eAt this point, I wrote \u0026hellip; possibly the smallest filesystem I\u0026rsquo;ve seen\u0026hellip; to give my model a spin in\nthe real world. Every FUSE operation was a passthrough to the LLM, for example:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-py\" data-lang=\"py\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"font-weight:bold\"\u003eclass\u003c/span\u003e \u003cspan style=\"font-weight:bold\"\u003eLLMFuse\u003c/span\u003e(LoggingMixin, Operations):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    ...\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"font-weight:bold\"\u003edef\u003c/span\u003e chmod(self, path, mode):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"font-style:italic\"\u003e\u0026#34;\u0026#34;\u0026#34;Change file permissions.\u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        response = self._query_llm_for_operation(\u003cspan style=\"font-style:italic\"\u003e\u0026#39;chmod\u0026#39;\u003c/span\u003e, path, mode=oct(mode))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"font-weight:bold\"\u003eif\u003c/span\u003e \u003cspan style=\"font-weight:bold\"\u003enot\u003c/span\u003e self._handle_llm_response(response):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"font-weight:bold\"\u003eraise\u003c/span\u003e FuseOSError(ENOENT)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"font-weight:bold\"\u003ereturn\u003c/span\u003e 0\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    ...\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eNice! I now had a mountable FUSE that was entirely \u0026ldquo;implemented\u0026rdquo; by a language\nmodel. As you can see below, I was able to \u003ccode\u003els\u003c/code\u003e around it, \u003ccode\u003eecho\u003c/code\u003e into files, and \u003ccode\u003ecat\u003c/code\u003e them back out.\u003c/p\u003e\n\u003ccenter\u003e\n\u003cimg src =/images/llmfuse_example.png width=\"800\" height=\"250\"/\u003e\nPoking around a Docker container with a mounted LLMFuse.\n\u003c/center\u003e\n\u003ch1 id=\"compressing-the-filesystem\"\u003eCompressing the filesystem \u003ca href=\"#compressing-the-filesystem\" class=\"hash\"\u003e#\u003c/a\u003e\u003c/h1\u003e\n\u003cp\u003ePerhaps the largest glaring inefficiency in this set up is the sheer verbosity\nof the XML-based representation. I was using many bytes to represent attributes\nand tree structure that could be encoded far more efficiently (~O(bits)) in a standard\nC struct.\u003c/p\u003e\n\u003cp\u003eHowever, as I was fine-tuning on the XML filesystem tree representation, I was\nbaking in this very structure into the weights and probability distributions of my Qwen fork! If only there was a way to leverage this to compress state\u0026hellip;\u003c/p\u003e\n\u003ch2 id=\"two-sides-of-the-same-coin\"\u003eTwo sides of the same coin \u003ca href=\"#two-sides-of-the-same-coin\" class=\"hash\"\u003e#\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eAs it turns out, compression and AI are intimately related. Using LLMs to lossily\ncompress text is one of the most common applications, so it\u0026rsquo;s not entirely\nunintuitive. However, one researcher (Marcus Hutter) claimed back in 2006 that they are \u003cem\u003eequivalent\u003c/em\u003e (and in fact \u003ca href=\"http://prize.hutter1.net/hfaq.htm#about\"\u003ebet $500K on this claim!\u003c/a\u003e).\u003c/p\u003e\n\u003cp\u003ePresciently, Hutter appears to be absolutely right. His \u003ccode\u003eenwik8\u003c/code\u003e and \u003ccode\u003eenwik9\u003c/code\u003e\u0026rsquo;s benchmark datasets are, today, best compressed by a \u003ca href=\"https://bellard.org/ts_zip/\"\u003e169M parameter LLM\u003c/a\u003e (trained by none other than Fabrice Bellard in 2023).\u003c/p\u003e\n\u003cp\u003eThat\u0026rsquo;s a bit perplexing on the first glance. Surely LLM compression isn\u0026rsquo;t reversible? What kind of voodoo magic was going on here?\u003c/p\u003e\n\u003ch3 id=\"arithmetic-coding\"\u003eArithmetic coding \u003ca href=\"#arithmetic-coding\" class=\"hash\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eThe algorithm that enables reversible compression using LLMs is called \u0026ldquo;arithmetic coding\u0026rdquo; and it builds upon a \u003ca href=\"https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf\"\u003e1948 result by Claude Shannon\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eResearchers at DeepMind (including Hutter himself) have \u003ca href=\"https://arxiv.org/pdf/2309.10668\"\u003eexplained the math in\ndetail\u003c/a\u003e, so I\u0026rsquo;ll direct the most inquisitive of you readers there, but for a simplified understanding of what\u0026rsquo;s going on, forget everything you might know about working with LLMs today. There\u0026rsquo;s no prompting involved!\u003c/p\u003e\n\u003ccenter\u003e\n\u003cimg src =/images/ac.png width=\"800\" height=\"300\"/\u003e\n\u003c/center\u003e\n\u003cp\u003eLet\u0026rsquo;s assume the following is true for some predictive model \\(M\\)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eLorem has first-word probability = 0.57.\u003c/li\u003e\n\u003cli\u003eIpsum has second-word conditional probability = 0.67 (joint 0.38).\u003c/li\u003e\n\u003cli\u003eDolor has a third word conditional probability = 0.5 (joint 0.19).\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u0026hellip;\u003c/p\u003e\n\u003cp\u003eso on and so forth until you reach the end of the string you want to compress and you end up with some \u0026ldquo;final interval width\u0026rdquo; \\(P(m)\\) on the real interval \\([0,1]\\) which represents your string.\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s suppose in our example this turns out to be 0.012. We can represent this decimal in roughly \\(- \\log_{2}{P(m)} = 6.4\\) bits, which is our final compression size.\u003c/p\u003e\n\u003cp\u003eThere\u0026rsquo;s a few elegant things about this algorithm:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cem\u003eAny\u003c/em\u003e number within this interval is uniquely determined by tracing the arithmetic coding algorithm through the specific probabilistic model\u0026rsquo;s weights. \u0026ldquo;Decoding\u0026rdquo; is simply a retracing operation (see the line through the probability distributions above)\u003c/li\u003e\n\u003cli\u003eThe inverse log relationship between predictive power \\(P(m)\\) and compression\npushes the burden of the \u0026ldquo;hard compression problem\u0026rdquo; to deep learning machinery which can encode high-dimensional text patterns within model weights, yielding far better compression ratios than deterministic algorithms.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eSounds cool! But \u003cstrong\u003ehow good really\u003c/strong\u003e is this compression? On comparing\narithmetic coding backed by \u003ccode\u003eQwen3-4B\u003c/code\u003e against \u003ccode\u003egzip\u003c/code\u003e for \u003ca href=\"https://www.lipsum.com/\"\u003e\u003ccode\u003elipsum.txt\u003c/code\u003e\u003c/a\u003e,\nwe already see pretty dramatic results:\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eMethod\u003c/th\u003e\n\u003cth\u003eSize (bytes)\u003c/th\u003e\n\u003cth\u003eCompression Impact\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eOriginal (plain)\u003c/td\u003e\n\u003ctd\u003e446\u003c/td\u003e\n\u003ctd\u003e—\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003egzip\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e298\u003c/td\u003e\n\u003ctd\u003e~33% smaller\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003ellmencode\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e13\u003c/td\u003e\n\u003ctd\u003e~97% smaller\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e(note: \u003ca href=\"https://github.com/rohangpta/llmfuse/blob/main/llmencode/llmencode.py\"\u003e\u003ccode\u003ellmencode\u003c/code\u003e\u003c/a\u003e is my implementation of arithmetic coding)\u003c/p\u003e\n\u003cp\u003e22x better compression than \u003ccode\u003egzip\u003c/code\u003e is pretty ridiculous! A caveat here is that \u003ccode\u003elipsum.txt\u003c/code\u003e is heavily represented in training data, but 5-20x efficiency gains broadly hold for all text data that (looks like) it\u0026rsquo;s been on the internet.\u003c/p\u003e\n\u003ch2 id=\"self-compression\"\u003eSelf-compression \u003ca href=\"#self-compression\" class=\"hash\"\u003e#\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eNow, back to our filesystem. The XML overhead we were worried about now can be\n\u0026ldquo;compressed away\u0026rdquo; by the fine-tuned model. Using the same toy filesystem from\nthe Docker container demo above:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-xml\" data-lang=\"xml\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"font-weight:bold\"\u003e\u0026lt;filesystem\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"font-weight:bold\"\u003e\u0026lt;directory\u003c/span\u003e path=\u003cspan style=\"font-style:italic\"\u003e\u0026#34;/\u0026#34;\u003c/span\u003e name=\u003cspan style=\"font-style:italic\"\u003e\u0026#34;/\u0026#34;\u003c/span\u003e mode=\u003cspan style=\"font-style:italic\"\u003e\u0026#34;755\u0026#34;\u003c/span\u003e owner=\u003cspan style=\"font-style:italic\"\u003e\u0026#34;root\u0026#34;\u003c/span\u003e group=\u003cspan style=\"font-style:italic\"\u003e\u0026#34;root\u0026#34;\u003c/span\u003e mtime=\u003cspan style=\"font-style:italic\"\u003e\u0026#34;2025-01-01T00:00:00\u0026#34;\u003c/span\u003e\u003cspan style=\"font-weight:bold\"\u003e\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"font-weight:bold\"\u003e\u0026lt;directory\u003c/span\u003e path=\u003cspan style=\"font-style:italic\"\u003e\u0026#34;testdir\u0026#34;\u003c/span\u003e name=\u003cspan style=\"font-style:italic\"\u003e\u0026#34;testdir\u0026#34;\u003c/span\u003e mode=\u003cspan style=\"font-style:italic\"\u003e\u0026#34;755\u0026#34;\u003c/span\u003e owner=\u003cspan style=\"font-style:italic\"\u003e\u0026#34;root\u0026#34;\u003c/span\u003e group=\u003cspan style=\"font-style:italic\"\u003e\u0026#34;root\u0026#34;\u003c/span\u003e mtime=\u003cspan style=\"font-style:italic\"\u003e\u0026#34;2025-01-01T00:00:00\u0026#34;\u003c/span\u003e \u003cspan style=\"font-weight:bold\"\u003e/\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"font-weight:bold\"\u003e\u0026lt;file\u003c/span\u003e path=\u003cspan style=\"font-style:italic\"\u003e\u0026#34;testfile.txt\u0026#34;\u003c/span\u003e name=\u003cspan style=\"font-style:italic\"\u003e\u0026#34;testfile.txt\u0026#34;\u003c/span\u003e mode=\u003cspan style=\"font-style:italic\"\u003e\u0026#34;644\u0026#34;\u003c/span\u003e owner=\u003cspan style=\"font-style:italic\"\u003e\u0026#34;root\u0026#34;\u003c/span\u003e group=\u003cspan style=\"font-style:italic\"\u003e\u0026#34;root\u0026#34;\u003c/span\u003e mtime=\u003cspan style=\"font-style:italic\"\u003e\u0026#34;2025-01-01T00:00:01\u0026#34;\u003c/span\u003e size=\u003cspan style=\"font-style:italic\"\u003e\u0026#34;14\u0026#34;\u003c/span\u003e\u003cspan style=\"font-weight:bold\"\u003e\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      \u003cspan style=\"font-weight:bold\"\u003e\u0026lt;body\u0026gt;\u003c/span\u003ehello llmfuse\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"font-weight:bold\"\u003e\u0026lt;/body\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"font-weight:bold\"\u003e\u0026lt;/file\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"font-weight:bold\"\u003e\u0026lt;/directory\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"font-weight:bold\"\u003e\u0026lt;/filesystem\u0026gt;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eModel\u003c/th\u003e\n\u003cth\u003eOriginal (bytes)\u003c/th\u003e\n\u003cth\u003eCompressed (bytes)\u003c/th\u003e\n\u003cth\u003eRatio\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eBase Qwen3-4B\u003c/td\u003e\n\u003ctd\u003e394\u003c/td\u003e\n\u003ctd\u003e38\u003c/td\u003e\n\u003ctd\u003e10.4x\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eFine-tuned Qwen3-4B\u003c/td\u003e\n\u003ctd\u003e394\u003c/td\u003e\n\u003ctd\u003e21\u003c/td\u003e\n\u003ctd\u003e18.8x\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eThe fine-tuned model achieves \u003cstrong\u003e44.7% better compression\u003c/strong\u003e on XML filesystem\ntrees - the very format it was trained to predict. This is the \u0026ldquo;self-compression\u0026rdquo;\neffect: by baking the XML structure into the model weights during fine-tuning,\nthe arithmetic coder can represent that structure in fewer bits.\u003c/p\u003e\n\u003cp\u003eSelf-compression in filesystems isn\u0026rsquo;t a novel concept. For example, there exists the\n\u003ca href=\"https://docs.kernel.org/filesystems/squashfs.html\"\u003e\u003ccode\u003esquashfs\u003c/code\u003e\u003c/a\u003e tool (created in 2002) to create R/O compressed filesystems. Squashfs compresses\nfiles, inodes, and directories together, not unlike what we\u0026rsquo;re doing here!\u003c/p\u003e\n\u003cp\u003eUnder the hood, \u003ccode\u003esquashfs\u003c/code\u003e just wraps \u003ccode\u003egzip\u003c/code\u003e/\u003ccode\u003ezstd\u003c/code\u003e/your favourite compression\nalgorithm. So for plain-text data, \u003ccode\u003esquashfs\u003c/code\u003e compression stats pale in the face of \u003ccode\u003ellmfuse\u003c/code\u003e:\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eMethod\u003c/th\u003e\n\u003cth\u003eCompressed Size\u003c/th\u003e\n\u003cth\u003eNotes\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003esquashfs (gzip)\u003c/td\u003e\n\u003ctd\u003e171 bytes\u003c/td\u003e\n\u003ctd\u003egzip-compressed file contents, inodes, directory tables\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003ellmfuse (fine-tuned)\u003c/td\u003e\n\u003ctd\u003e21 bytes\u003c/td\u003e\n\u003ctd\u003eArithmetic coded XML state\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eFor the same filesystem tree (one directory, one 14-byte text file), llmfuse\nachieves \u003cstrong\u003e~8x better compression\u003c/strong\u003e than squashfs (see methodology in appendix).\u003c/p\u003e\n\u003cp\u003eThe difference comes down to \u003ccode\u003ellmencode\u003c/code\u003e being far better than \u003ccode\u003egzip\u003c/code\u003e on\ntext data + XML structure - especially when the model has been fine-tuned on exactly\nthat structure.\u003c/p\u003e\n\u003ch1 id=\"conclusion\"\u003eConclusion \u003ca href=\"#conclusion\" class=\"hash\"\u003e#\u003c/a\u003e\u003c/h1\u003e\n\u003cp\u003eWhat started off as a little experiment mostly to get my hands dirty with\ntraining and inference evolved into a full blown \u003ca href=\"https://xkcd.com/356/\"\u003enerd\nsnipe\u003c/a\u003e and intellectual adventure. Thanks for making it\nthis far!\u003c/p\u003e\n\u003cp\u003eI entirely recognize that this is a \u0026ldquo;toy\u0026rdquo;\nexperiment under a very specific setup; with that said, the numbers above are pretty eye-popping, and the question I\u0026rsquo;ve been trying to answer as I write this up is: does this have any real-world potential?\u003c/p\u003e\n\u003cp\u003eOf course, in the short term, there\u0026rsquo;s a whole host of caveats: you need an\nLLM, likely a GPU, all your data is in the context window (which we know scales\npoorly), and this only works on text data.\u003c/p\u003e\n\u003cp\u003eStill, it\u0026rsquo;s intriguing to wonder whether the very engines that will likely\ndominate all \u0026ldquo;text generation\u0026rdquo; going forward can be used to compress their own\ndata? Perhaps in a distant future, where running LLMs at the edge makes sense, or for specific kinds of workflows where data is read very infrequently.\u003c/p\u003e\n\u003cp\u003eOverall, I\u0026rsquo;m grateful to Peyton at \u003ca href=\"https://modal.com\"\u003eModal\u003c/a\u003e for the compute credits. Running\na somewhat unconventional experiment like this wouldn\u0026rsquo;t have been possible\nwithout full control over the training and inference code, and extremely\ntedious without the simplicity of running ML infra on Modal! It\u0026rsquo;s truly awesome\nto be able to just \u003ccode\u003emodal deploy\u003c/code\u003e and get my own private inference endpoints,\nor just \u003ccode\u003emodal run\u003c/code\u003e to prototype some code on the cloud.\u003c/p\u003e\n\u003ch1 id=\"appendix\"\u003eAppendix \u003ca href=\"#appendix\" class=\"hash\"\u003e#\u003c/a\u003e\u003c/h1\u003e\n\u003ch2 id=\"source-code\"\u003eSource Code \u003ca href=\"#source-code\" class=\"hash\"\u003e#\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eAll of the source code for this experiment, particularly \u003ccode\u003ellmfuse\u003c/code\u003e and\n\u003ccode\u003ellmencode\u003c/code\u003e are \u003ca href=\"https://github.com/rohangpta/llmfuse\"\u003eopen-sourced\u003c/a\u003e under MIT.\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003ellmencode\u003c/code\u003e is abstracted into a CLI utility that you can run locally.\nInference on 4B models is slow, but entirely possible on consumer hardware.\nI prototyped most of this code by running on a 2021 MacBook Pro, before\nproductionizing on Modal.\u003c/p\u003e\n\u003cp\u003eA fun experiment / party trick to identify how \u0026ldquo;common\u0026rdquo; a certain\nstring is in training data is to look at its \u003ccode\u003ellmencode\u003c/code\u003e compression ratio!\u003c/p\u003e\n\u003ch2 id=\"squashfs-comparison-methodology\"\u003eSquashFS comparison methodology \u003ca href=\"#squashfs-comparison-methodology\" class=\"hash\"\u003e#\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eThe raw \u003ccode\u003e.sqsh\u003c/code\u003e file is 4096 bytes due to block alignment padding. To find the\nactual compressed size, I used \u003ccode\u003exxd\u003c/code\u003e to inspect the binary and found the last\nnon-zero byte at offset 266 (267 bytes total). Subtracting the fixed 96-byte\nsuperblock header gives us 171 bytes of actual gzip-compressed content -\neverything needed to reconstruct the filesystem.\u003c/p\u003e\n\u003ch2 id=\"compression-as-a-metric\"\u003eCompression as a metric \u003ca href=\"#compression-as-a-metric\" class=\"hash\"\u003e#\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eIt\u0026rsquo;s equally interesting to think about compression as a metric. An angle I\u0026rsquo;d\nconsidered is doing some kind of RL on the arithmetic coded compression number itself.\u003c/p\u003e\n\u003cp\u003e\u003cdel\u003eIs that simply equivalent to the pre-training objective (due to the prediction-compression duality)? Or does the \u0026ldquo;sequence-level\u0026rdquo; objective add something more\u0026hellip; interesting to the mix. Please reach out if you have thoughts!\u003c/del\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eEDIT:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eAs it turns out - it is indeed equivalent to the pre-training objective!\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003ePre-training\u003c/strong\u003e aims to maximize the probability of training data. For a sequence \\(x = (x_1, x_2, \\ldots, x_T)\\), we decompose via the chain rule:\u003c/p\u003e\n\u003cdiv\u003e\n$$P(x) = \\prod_{t=1}^{T} P(x_t \\mid x_{\u003c t})$$\n\u003c/div\u003e\n\u003cp\u003eTaking logarithms (for numerical stability and to convert products to sums):\u003c/p\u003e\n\u003cdiv\u003e\n$$\\log P(x) = \\sum_{t=1}^{T} \\log P(x_t \\mid x_{\u003c t})$$\n\u003c/div\u003e\n\u003cp\u003eMaximizing log-probability is equivalent to minimizing its negative — the \u003cstrong\u003enegative log-likelihood\u003c/strong\u003e:\u003c/p\u003e\n\u003cdiv\u003e\n$$\\mathcal{L}_{\\text{NLL}}(\\theta) = -\\sum_{t=1}^{T} \\log P_\\theta(x_t \\mid x_{\u003c t})$$\n\u003c/div\u003e\n\u003cp\u003e\u003cstrong\u003eArithmetic coding\u003c/strong\u003e maps the entire sequence \\(x\\) to an interval on \\([0,1]\\) of width equal to its joint probability \\(P(x)\\). To uniquely specify a point within an interval of width \\(P(x)\\), we require \\(-\\log_2 P(x)\\) bits.\u003c/p\u003e\n\u003cp\u003eExpanding this using the chain rule from above gives us the sum:\u003c/p\u003e\n\u003cp\u003e$$L_{\\text{compressed}}(x) = -\\log_2 P(x) = -\\log_2 \\prod_{t=1}^{T} P_\\theta(x_t \\mid x_{\u0026lt; t}) = -\\sum_{t=1}^{T} \\log_2 P_\\theta(x_t \\mid x_{\u0026lt; t})$$\u003c/p\u003e\n\u003cp\u003eThese differ only by logarithm base. Since \\(\\log_2 P = \\frac{\\ln P}{\\ln 2}\\), and \\(\\frac{1}{\\ln 2} \\approx 1.44\\) is a positive constant:\u003c/p\u003e\n\u003cdiv\u003e\n$$\\arg\\min_\\theta \\mathcal{L}_{\\text{NLL}} = \\arg\\min_\\theta L_{\\text{compressed}}$$\n\u003c/div\u003e\n\u003cp\u003eThe same \\(\\theta^*\\) minimizes both. \\(\\square\\)\u003c/p\u003e\n\u003cp\u003eBeyond resolving the RL question, there\u0026rsquo;s something quietly beautiful about this framing. In my view, viewing the pre-training objective as compression is a simple way to \u0026ldquo;grok\u0026rdquo; the math.\u003c/p\u003e\n","url":"https://grohan.co/2025/11/25/llmfuse/","image":"https://grohan.cophotos/<no value>","banner_image":"https://grohan.cophotos/<no value>","date_published":"25116-25-09T110:2525:00+00:00","date_modified":"25116-25-09T110:2525:00+00:00","author":{"name":"Ronalds Vilcins","url":"https://ronaldsvilcins.com/"}},{"id":"3d97b6cddc1764f4d41ae30637dd1ac7dc9acaaf","title":"Technical Debt is Entropy In Software","summary":"Entropy","content_text":"Entropy # Entropy is the ultimate boss battle [1]. As the reason why ice melts, why tires burst, and why ink diffuses \u0026ndash; thermodynamic entropy is a fact of the physical world, sharply following the arrow of time [2].\nThe Second Law of Thermodynamics states\nFor an isolated system, entropy will either increase or remain the same over time; decreases require exporting entropy to the environment.\nThere\u0026rsquo;s something about inevitability that I think is fascinating, especially when you can see it unfold in front of you \u0026ndash; my favourite visualisation is below (thanks Gemini).\nThis picture displays the three stages of entropy:\nInfancy: when the ink enters the water Expansion: when the ink diffuses through water Maturity: when the ink and water have fully merged So, how does entropy grow? Academics and practitioners alike believe that entropy follows an S-curve [4], with its three stages likened to those of ink diffusing in water.\nThis is strikingly similar to the business lifecycle curve above! Indeed, prior art agrees that business and technology lifecycles are overlaid entropy curves [5].\nEntropy in business is largely a representation of diffusion of a particular product, driven by the forces of supply and demand. While entropy is often likened to \u0026ldquo;disorder\u0026rdquo;, I like to use \u0026ldquo;disruption\u0026rdquo; - permeation of new (ink) into old (water). It is neither good nor bad, simply inevitable.\nProperties of Entropy # Statistical Entropy # Since we are talking about software and not atoms, let\u0026rsquo;s turn to information theory to understand the information contained in software programs.\nIn information theory, Shannon\u0026rsquo;s entropy is, for a random variable \\( X \\) distributed according to \\( p: (x \\in \\mathcal{X}) \\rightarrow [0, 1] \\):\n$$ H(X) = - \\sum_{x \\in \\mathcal{X}} p(x)\\log(p(x)) $$\nWhile this may look obscure at first, the formulation begets two properties:\nProperty 1. The number of possible states that a system can have is generally proportional to the total entropy in a system. A dice with 6 sides has more entropy than one with 4 sides.\nProperty 2. Higher entropy is correlated with a higher presence of tail events. Gaussians and exponentials are maximum entropy distributions (under certain statistical conditions [3]), but they are light-tailed. Many real-world systems are heavier-tailed (Taleb-style), so tail risk is larger than Gaussian intuition suggests [16].\nComplexity # Complexity is entropy\u0026rsquo;s first cousin. Formally we\u0026rsquo;ll use Kolmogorov complexity:\n\\(K(o)\\) for an object \\(o\\) is the length of the shortest program that produces the object as output.\nIn other words, it measures how \u0026ldquo;compressible\u0026rdquo; something is.\nNow - how does entropy relate to complexity? Modis posits the following relation [4][15]:\nComplexity is the time derivative of entropy. Given that entropy is an S-curve, complexity is roughly normally distributed.\nWith time, complexity increases until a peak and decreases after.\nTo see this: let\u0026rsquo;s consider Scott Aaronson\u0026rsquo;s lucid example around cream dissolving into coffee [6].\nHis research empirically calculated a complexity score using the gzip compression file size of pictures of cream melting in the coffee.\nThe picture above represents snapshots of the coffee cup at the three phases of entropy. As the cream and coffee begin to mix, complexity increases until a maximum before decreasing as the mixture becomes saturated and homogeneous.\nIt is simple to see why the first and last image can be more easily compressed (i.e are less complex) compared to the image in the middle.\nWhere Does Software Fit Into This? # Let\u0026rsquo;s start by quantifying where software is today in its business lifecycle / entropy curve.\nAs in the graph above, I think we are around (X,Y) today.\nWhy?\nSoftware is still custom-made / productised. SaaS \u0026ldquo;exploded\u0026rdquo; but hasn\u0026rsquo;t permeated all industries. Digital modernisation efforts continue to be in higher demand than available supply, indicating superlinear business growth or relative convexity \u0026ndash; we are close to the central inflection point.\nSoftware complexity is close to peaking. For starters, the cloud has driven the successful commoditisation of infrastructure. We now have the ability to run software cheaply and easily.\nWhat\u0026rsquo;s left is to reduce the complexity of the specification of software (language and layers of abstraction) that runs on said infrastructure. Prediction: LLMs are statistical program compression tools [12] and will do exactly this.\nTech Debt as Complexity # For a particular business, technological modernisation is largely bottlenecked by iteration speed to a desired solution. I view tech debt as the maintenance and complexity related resistance to change that causes this bottleneck.\nTo be clear, every problem that software solves has some theoretical baseline level of \u0026ldquo;complexity\u0026rdquo; needed to meet its specification. Technical debt is simply the add-on difference in complexity between the ideal and real world solutions.\nSo what causes complexity? The sources of complexity that I\u0026rsquo;ve found to be the most concise yet accurate are below [7]:\nComplexity via obscurity # Poorly designed software abstractions (obscurity) generate more complexity - \u0026ldquo;garbage in, garbage out\u0026rdquo;.\nThe core issue with \u0026ldquo;obscure\u0026rdquo; abstractions is that they are uncompressed representations of state. Their interfaces are inherently complex, where they should instead be simple and hide deep complexity [11].\nOften times, obscurity appears due to constraints imposed on systems (\u0026ldquo;make this code accept XYZ data format\u0026rdquo;). Local optimisations lead to global API changes that introduce obscurity and state bloat [17]. Refactors / complexity regulation measures don\u0026rsquo;t fit into deadlines and this bloat almost always compounds (Property 1).\nComplexity via dense dependency graphs # Software that depends on A LOT of other software is more prone to bugs, vulnerabilities and maintenance overhead. This kind of technical debt introduces some obscurity but also \u0026ldquo;tail-risk\u0026rdquo; around software (Property 2).\nThe number of failures due to weaknesses in the open source parts of a software supply chain increased by 650% between 2020 and 2021 [8]. At the same time, OSS adoption has been growing 70% YoY [9], bringing with it increasingly public vulnerabilities like in log4j, xz, OpenSSH etc.\nDependencies can also reflect a level of \u0026ldquo;obscurity\u0026rdquo; in software that may not justify their added risk. Leftpad is a great example here! [18]\nWhat isn\u0026rsquo;t captured by mere probabilities is how this \u0026ldquo;tail-risk\u0026rdquo; can manifest as devastatingly high-impact Black Swan events [14]: massive cybercrimes affecting data protection and financial security. Software outage affecting the global economy. We\u0026rsquo;ve all seen them play out.\nConclusion # To recap, entropy in software tracks the expansion of software with time and complexity is the derivative of entropy. Due to the S-curve shape of entropy and thus \u0026ldquo;normal\u0026rdquo; shape of its derivative, \u0026ldquo;complexity begets complexity\u0026rdquo; until a certain point in time, after which complexity reduces.\nI argue that due to market conditions: namely the state of SaaS as a product / custom-made offering and the commoditisation of infrastructure (cloud) that we are at the middle of the S-curve. i.e., we are at the peak of software complexity / tech debt growth. Intuitively, in the business “experimental -\u0026gt; custom -\u0026gt; product -\u0026gt; commodity” lifecycle, software is at the custom/product intersection, which is somewhere in the middle.\nWhile the total amount of code in the world will keep growing, in order to sustain healthy growth, demand for \u0026ldquo;software regulators\u0026rdquo; will rise to address growing maintenance and lifecycle management costs. In order to get from product to commodity and therefore fully permeate society, operating software needs to be far simpler and cheaper.\nEconomically, McKinsey estimates that tech debt accounts for 40% of IT balance sheets and up to 50% of developer time [13]. It shows up as a vicious cycle that organisations increasingly have a harder time escaping from (remember: complexity begets complexity).\nA report on software quality in 2022 attaches a $2.4 trillion price tag to technical debt in the form of poor quality and overly complex software [8]. However, if you interchange loosely with \u0026ldquo;technological opportunity cost\u0026rdquo;, the real business value is of course far higher:\nThe total technological debt includes entirely \u0026ldquo;untapped\u0026rdquo; digital transformation in industries (??T) + the canonical \u0026ldquo;poorly tapped\u0026rdquo; form of tech debt (2.4T).\nA lot of words to say: there\u0026rsquo;s a whole lot left to do here.\nFootnotes and References # [1] https://x.com/elonmusk/status/1090689205586472960\n[2] https://en.wikipedia.org/wiki/Entropy_as_an_arrow_of_time\n[3] https://pillowlab.princeton.edu/teaching/statneuro2018/slides/notes08_infotheory.pdf\n[4] https://arxiv.org/abs/2410.10844\n[5] http://www.growth-dynamics.com/articles/Forecasting_Complexity.pdf\n[6] https://arxiv.org/abs/1405.6903\n[7] https://books.google.com/books/about/A_Philosophy_of_Software_Design.html?id=hkfEzgEACAAJ\u0026amp;source=kp_book_description\n[8] https://www.it-cisq.org/wp-content/uploads/sites/6/2022/11/CPSQ-Report-Nov-22-2.pdf\n[9] https://www.sonatype.com/blog/the-scale-of-open-source-growth-challenges-and-key-insights\n[10] https://en.wikipedia.org/wiki/Lindy_effect\n[11] https://en.wikipedia.org/wiki/Everything_is_a_file\n[12] https://arxiv.org/abs/2309.10668\n[13] https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/breaking-technical-debts-vicious-cycle-to-modernize-your-business\n[14] https://en.wikipedia.org/wiki/Black_swan_theory\n[15] Theodore Modis presents an excellent information-theoretic analysis of the relation between complexity and entropy: his claim held true over a 20 year prediction (2002-2022) [4] (a Lindy trend [10]).\n[16] https://arxiv.org/pdf/1412.7647\n[17] https://www.hillelwayne.com/post/complexity-constraints/\n[18] https://en.wikipedia.org/wiki/Npm_left-pad_incident\nAppendix: Three Hard Problems # I\u0026rsquo;ve spent a lot of words talking about a problem and tying it to theory and predictions. What does taming complexity look like? How do we roll the ball down the peak of the hill?\nAt the risk of being tongue-in-cheek, I\u0026rsquo;ll start by saying I think the solution will involve solving the \u0026ldquo;three hardest problems\u0026rdquo; in computer science.\nNaming # Naming standards solve the literal naming problem. Today, standards work - they just suffer from enforcement and distribution problems. Because of this, the shapes of software abstractions haven\u0026rsquo;t been globally standardised. It\u0026rsquo;s a problem famous enough to warrant its own obligatory XKCD.\nBecause of where we are on the software S-curve, I suspect the move from product to commodity will entail hiding the naming problem with higher-order abstractions. As people move up an abstraction layer, user-defined names will simply matter less.\nCaching # What\u0026rsquo;s hard about caching isn\u0026rsquo;t maintaining caching infrastructure, but rather defining the correct cache key and invalidation policy. This isn\u0026rsquo;t impossible - it again just requires precise and well-defined behaviour. The CPU caches are a great success story here - they just needed maturity in the CPU/memory interface.\nWith cacheability, you get reproducibility, determinism, and more general fungibility of building blocks. With fungibility of building blocks - you get commodity-like properties! Declarative specifications of components are closely tied to cacheability - a statement which traces the entire history of software infrastructure growth and commoditisation (Kubernetes, Docker, Terraform, Nix, Bazel etc). Off by one errors # These simply represent human hallucinations. Execution driven feedback. i.e unit test cases encapsulating abstractions \u0026ldquo;solve\u0026rdquo; this well.\nProve it? # I won\u0026rsquo;t elaborate too much on \u0026ldquo;solutions\u0026rdquo; since I have various hypotheses-in-testing that need iteration (as opposed to more ideation).\nIf you made it this far, clearly some of this was interesting to you. Let\u0026rsquo;s chat! Email X\n","content_html":"\u003ch1 id=\"entropy\"\u003eEntropy \u003ca href=\"#entropy\" class=\"hash\"\u003e#\u003c/a\u003e\u003c/h1\u003e\n\u003cp\u003eEntropy is the ultimate boss battle [1]. As the reason why ice melts, why tires\nburst, and why ink diffuses \u0026ndash; thermodynamic entropy is a fact of the\nphysical world, sharply following the arrow of time [2].\u003c/p\u003e\n\u003cp\u003eThe Second Law of Thermodynamics states\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eFor an isolated system, entropy will either increase or remain the same over time; decreases require exporting entropy to the environment.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eThere\u0026rsquo;s something about inevitability that I think is fascinating, especially\nwhen you can see it unfold in front of you \u0026ndash; my\nfavourite visualisation is below (thanks Gemini).\u003c/p\u003e\n\u003ccenter\u003e\u003cimg src =/images/diffusion.png width=\"550\" height=\"400\"/\u003e\u003c/center\u003e\n\u003cp\u003eThis picture displays the three stages of entropy:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eInfancy\u003c/strong\u003e: when the ink enters the water\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eExpansion\u003c/strong\u003e: when the ink diffuses through water\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMaturity\u003c/strong\u003e: when the ink and water have fully merged\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eSo, how does entropy grow? Academics and practitioners alike believe that\nentropy follows an S-curve [4], with its three stages likened to those of ink diffusing in water.\u003c/p\u003e\n\u003ccenter\u003e\u003cimg src =/images/scurve.png width=\"550\" height=\"400\"/\u003e \u003c/center\u003e\n\u003cp\u003eThis is strikingly similar to the business lifecycle curve above! Indeed, prior\nart agrees that \u003cstrong\u003ebusiness and technology lifecycles are overlaid entropy curves\u003c/strong\u003e\n[5].\u003c/p\u003e\n\u003cp\u003eEntropy in business is largely a representation of diffusion of a\nparticular product, driven by the forces of supply and demand. While entropy is\noften likened to \u0026ldquo;disorder\u0026rdquo;, I like to use \u0026ldquo;disruption\u0026rdquo; - permeation of new (ink)\ninto old (water). It is neither good nor bad, simply inevitable.\u003c/p\u003e\n\u003ch2 id=\"properties-of-entropy\"\u003eProperties of Entropy \u003ca href=\"#properties-of-entropy\" class=\"hash\"\u003e#\u003c/a\u003e\u003c/h2\u003e\n\u003ch3 id=\"statistical-entropy\"\u003eStatistical Entropy \u003ca href=\"#statistical-entropy\" class=\"hash\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eSince we are talking about software and not atoms, let\u0026rsquo;s turn to information theory to\nunderstand the information contained in software programs.\u003c/p\u003e\n\u003cp\u003eIn information theory, Shannon\u0026rsquo;s entropy is, for a random\nvariable  \\( X \\) distributed according to \\( p: (x \\in \\mathcal{X})\n\\rightarrow [0, 1] \\):\u003c/p\u003e\n\u003cp\u003e$$ H(X) = - \\sum_{x \\in \\mathcal{X}} p(x)\\log(p(x)) $$\u003c/p\u003e\n\u003cp\u003eWhile this may look obscure at first, the formulation begets two properties:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eProperty 1.\u003c/strong\u003e The number of possible states that a system can have is\ngenerally proportional to the total entropy in a system. A dice with 6 sides has\nmore entropy than one with 4 sides.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eProperty 2.\u003c/strong\u003e Higher entropy is correlated with a higher presence of tail events.\nGaussians and exponentials are maximum entropy distributions (under\ncertain statistical conditions [3]), but they are light-tailed.\nMany real-world systems are heavier-tailed (Taleb-style), so tail risk\nis larger than Gaussian intuition suggests [16].\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"complexity\"\u003eComplexity \u003ca href=\"#complexity\" class=\"hash\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eComplexity is entropy\u0026rsquo;s first cousin. Formally we\u0026rsquo;ll use\nKolmogorov complexity:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\\(K(o)\\) for an object \\(o\\) is the length of the shortest program that produces the\nobject as output.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eIn other words, it measures how \u0026ldquo;compressible\u0026rdquo; something is.\u003c/p\u003e\n\u003cp\u003eNow - how does entropy relate to complexity? Modis posits the following relation [4][15]:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eComplexity is the time derivative of entropy. Given that entropy is an\nS-curve, complexity is roughly normally distributed.\u003c/p\u003e\n\u003cp\u003eWith time, complexity increases until a peak and decreases after.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eTo see this: let\u0026rsquo;s consider Scott Aaronson\u0026rsquo;s lucid example around cream dissolving\ninto coffee [6].\u003c/p\u003e\n\u003cp\u003eHis research empirically calculated a complexity score using the \u003ccode\u003egzip\u003c/code\u003e compression\nfile size of pictures of cream melting in the coffee.\u003c/p\u003e\n\u003ccenter\u003e\u003cimg src =/images/coffee_complexity.png width=\"413\" height=\"300\"/\u003e\u003c/center\u003e\n\u003cp\u003eThe picture above represents snapshots of the coffee cup at the three phases of entropy. As the cream and coffee begin to mix, complexity increases until a maximum\nbefore decreasing as the mixture becomes saturated and homogeneous.\u003c/p\u003e\n\u003cp\u003eIt is simple to see why the first and\nlast image can be more easily compressed (i.e are less complex) compared to the\nimage in the middle.\u003c/p\u003e\n\u003ch1 id=\"where-does-software-fit-into-this\"\u003eWhere Does Software Fit Into This? \u003ca href=\"#where-does-software-fit-into-this\" class=\"hash\"\u003e#\u003c/a\u003e\u003c/h1\u003e\n\u003cp\u003eLet\u0026rsquo;s start by quantifying where software is today in its business lifecycle /\nentropy curve.\u003c/p\u003e\n\u003ccenter\u003e\u003cimg src =/images/software_today.png width=\"550\" height=\"400\"/\u003e\u003c/center\u003e\n\u003cp\u003eAs in the graph above, I think we are around (X,Y) today.\u003c/p\u003e\n\u003cp\u003eWhy?\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eSoftware is still custom-made / productised. SaaS \u0026ldquo;exploded\u0026rdquo; but hasn\u0026rsquo;t permeated all\nindustries. Digital modernisation efforts continue to be in higher\ndemand than available supply, indicating superlinear business growth or relative\nconvexity \u0026ndash; \u003cem\u003ewe are close to the central inflection point\u003c/em\u003e.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eSoftware complexity is close to peaking. For starters, the\ncloud has driven the successful commoditisation of infrastructure. We now have\nthe ability to \u003cem\u003erun software\u003c/em\u003e cheaply and easily.\u003c/p\u003e\n\u003cp\u003eWhat\u0026rsquo;s left is to reduce the complexity of\nthe specification of software (language and layers of\nabstraction) that runs on said infrastructure. \u003cem\u003ePrediction\u003c/em\u003e: LLMs are statistical\nprogram compression tools [12] and will do exactly this.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"tech-debt-as-complexity\"\u003eTech Debt as Complexity \u003ca href=\"#tech-debt-as-complexity\" class=\"hash\"\u003e#\u003c/a\u003e\u003c/h1\u003e\n\u003cp\u003eFor a particular business, technological modernisation is largely bottlenecked by iteration speed to a desired solution.\nI view tech debt as the maintenance and complexity related resistance to change that\ncauses this bottleneck.\u003c/p\u003e\n\u003cp\u003eTo be clear, every problem that software solves has some theoretical baseline level of\n\u0026ldquo;complexity\u0026rdquo; needed to meet its specification. Technical debt is simply the add-on\ndifference in complexity between the ideal and real world solutions.\u003c/p\u003e\n\u003cp\u003eSo what causes complexity? The sources of complexity that I\u0026rsquo;ve found to\nbe the most concise yet accurate are below [7]:\u003c/p\u003e\n\u003ch3 id=\"complexity-via-obscurity\"\u003eComplexity via obscurity \u003ca href=\"#complexity-via-obscurity\" class=\"hash\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003ePoorly designed software abstractions (obscurity) generate more complexity - \u0026ldquo;garbage in, garbage\nout\u0026rdquo;.\u003c/p\u003e\n\u003cp\u003eThe core issue with \u0026ldquo;obscure\u0026rdquo; abstractions is that they are uncompressed representations of state.\nTheir interfaces are inherently complex, where they should instead be\nsimple and hide deep complexity [11].\u003c/p\u003e\n\u003cp\u003eOften times, obscurity appears due to constraints imposed on systems (\u0026ldquo;make this\ncode accept XYZ data format\u0026rdquo;). Local\noptimisations lead to global API changes that introduce obscurity and state bloat\n[17]. Refactors / complexity regulation measures don\u0026rsquo;t fit into deadlines and this bloat\nalmost always compounds (Property 1).\u003c/p\u003e\n\u003ch3 id=\"complexity-via-dense-dependency-graphs\"\u003eComplexity via dense dependency graphs \u003ca href=\"#complexity-via-dense-dependency-graphs\" class=\"hash\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eSoftware that depends on A LOT of other software is more prone to bugs,\nvulnerabilities and maintenance overhead. This kind of\ntechnical debt introduces some obscurity but also \u0026ldquo;tail-risk\u0026rdquo; around software (Property\n2).\u003c/p\u003e\n\u003cp\u003eThe number of failures due to weaknesses\nin the open source parts of a software supply chain increased by 650% between\n2020 and 2021 [8]. At the same time, OSS adoption has been growing 70% YoY [9], bringing with it\nincreasingly public vulnerabilities like in \u003ccode\u003elog4j\u003c/code\u003e, \u003ccode\u003exz\u003c/code\u003e, \u003ccode\u003eOpenSSH\u003c/code\u003e etc.\u003c/p\u003e\n\u003cp\u003eDependencies can also reflect a level of \u0026ldquo;obscurity\u0026rdquo; in software that may not\njustify their added risk. Leftpad is a great example here! [18]\u003c/p\u003e\n\u003cp\u003eWhat isn\u0026rsquo;t captured by mere probabilities is how this \u0026ldquo;tail-risk\u0026rdquo; can\nmanifest as devastatingly high-impact Black Swan events [14]: massive cybercrimes affecting data\nprotection and financial security. Software outage affecting the\nglobal economy. We\u0026rsquo;ve all seen them play out.\u003c/p\u003e\n\u003ch1 id=\"conclusion\"\u003eConclusion \u003ca href=\"#conclusion\" class=\"hash\"\u003e#\u003c/a\u003e\u003c/h1\u003e\n\u003ccenter\u003e\u003cimg src =/images/complexity_entropy.png width=\"550\" height=\"400\"/\u003e\u003c/center\u003e\n\u003cp\u003eTo recap, entropy in software tracks the expansion of software with time and complexity\nis the derivative of entropy. Due to the S-curve shape of entropy\nand thus \u0026ldquo;normal\u0026rdquo; shape of its derivative, \u0026ldquo;complexity begets\ncomplexity\u0026rdquo; until a certain point in time, after which complexity reduces.\u003c/p\u003e\n\u003cp\u003eI argue that due to market conditions: namely the state of SaaS as a product /\ncustom-made offering and the commoditisation of infrastructure (cloud) that we are at the middle of the S-curve. i.e., we are at the \u003cstrong\u003epeak\nof software complexity / tech debt growth\u003c/strong\u003e. Intuitively, in the business “experimental -\u0026gt; custom -\u0026gt; product -\u0026gt; commodity” lifecycle,\nsoftware is at the custom/product intersection, which is somewhere in the middle.\u003c/p\u003e\n\u003cp\u003eWhile the \u003cem\u003etotal amount\u003c/em\u003e of code in the world will keep growing, in order to sustain healthy\ngrowth, demand for \u0026ldquo;software regulators\u0026rdquo; will rise to address growing\nmaintenance and lifecycle management costs. In order to get from product to\ncommodity and therefore fully permeate society, operating software needs to be\nfar simpler and\ncheaper.\u003c/p\u003e\n\u003cp\u003eEconomically, McKinsey estimates that tech debt accounts for 40% of IT balance\nsheets and up to 50% of developer time [13]. It shows up as a vicious cycle that\norganisations increasingly have a harder time escaping from (remember:\ncomplexity begets complexity).\u003c/p\u003e\n\u003ccenter\u003e\u003cimg src =/images/complexity_cycle.svgz width=\"550\" height=\"400\"/\u003e\u003c/center\u003e\n\u003cp\u003eA report on software quality in 2022 attaches a $2.4 trillion price tag\nto technical debt in the form of poor quality and overly complex software [8].\nHowever, if you interchange loosely with \u0026ldquo;technological opportunity cost\u0026rdquo;, the\nreal business value is of course far\nhigher:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThe total technological debt includes entirely \u0026ldquo;untapped\u0026rdquo; digital\ntransformation in industries (??T) + the canonical \u0026ldquo;poorly tapped\u0026rdquo; form of\ntech debt (2.4T).\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eA lot of words to say: there\u0026rsquo;s a whole lot left to do here.\u003c/p\u003e\n\u003ch1 id=\"footnotes-and-references\"\u003eFootnotes and References \u003ca href=\"#footnotes-and-references\" class=\"hash\"\u003e#\u003c/a\u003e\u003c/h1\u003e\n\u003cp\u003e[1] \u003ca href=\"https://x.com/elonmusk/status/1090689205586472960\"\u003ehttps://x.com/elonmusk/status/1090689205586472960\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e[2] \u003ca href=\"https://en.wikipedia.org/wiki/Entropy_as_an_arrow_of_time\"\u003ehttps://en.wikipedia.org/wiki/Entropy_as_an_arrow_of_time\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e[3] \u003ca href=\"https://pillowlab.princeton.edu/teaching/statneuro2018/slides/notes08_infotheory.pdf\"\u003ehttps://pillowlab.princeton.edu/teaching/statneuro2018/slides/notes08_infotheory.pdf\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e[4] \u003ca href=\"https://arxiv.org/abs/2410.10844\"\u003ehttps://arxiv.org/abs/2410.10844\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e[5] \u003ca href=\"http://www.growth-dynamics.com/articles/Forecasting_Complexity.pdf\"\u003ehttp://www.growth-dynamics.com/articles/Forecasting_Complexity.pdf\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e[6] \u003ca href=\"https://arxiv.org/abs/1405.6903\"\u003ehttps://arxiv.org/abs/1405.6903\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e[7] \u003ca href=\"https://books.google.com/books/about/A_Philosophy_of_Software_Design.html?id=hkfEzgEACAAJ\u0026amp;source=kp_book_description\"\u003ehttps://books.google.com/books/about/A_Philosophy_of_Software_Design.html?id=hkfEzgEACAAJ\u0026amp;source=kp_book_description\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e[8] \u003ca href=\"https://www.it-cisq.org/wp-content/uploads/sites/6/2022/11/CPSQ-Report-Nov-22-2.pdf\"\u003ehttps://www.it-cisq.org/wp-content/uploads/sites/6/2022/11/CPSQ-Report-Nov-22-2.pdf\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e[9] \u003ca href=\"https://www.sonatype.com/blog/the-scale-of-open-source-growth-challenges-and-key-insights\"\u003ehttps://www.sonatype.com/blog/the-scale-of-open-source-growth-challenges-and-key-insights\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e[10] \u003ca href=\"https://en.wikipedia.org/wiki/Lindy_effect\"\u003ehttps://en.wikipedia.org/wiki/Lindy_effect\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e[11] \u003ca href=\"https://en.wikipedia.org/wiki/Everything_is_a_file\"\u003ehttps://en.wikipedia.org/wiki/Everything_is_a_file\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e[12] \u003ca href=\"https://arxiv.org/abs/2309.10668\"\u003ehttps://arxiv.org/abs/2309.10668\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e[13] \u003ca href=\"https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/breaking-technical-debts-vicious-cycle-to-modernize-your-business\"\u003ehttps://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/breaking-technical-debts-vicious-cycle-to-modernize-your-business\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e[14] \u003ca href=\"https://en.wikipedia.org/wiki/Black_swan_theory\"\u003ehttps://en.wikipedia.org/wiki/Black_swan_theory\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e[15] Theodore Modis presents an excellent information-theoretic analysis of the relation between complexity\nand entropy: his claim held true over a 20 year prediction (2002-2022) [4] (\u003cem\u003ea Lindy trend\u003c/em\u003e [10]).\u003c/p\u003e\n\u003cp\u003e[16] \u003ca href=\"https://arxiv.org/pdf/1412.7647\"\u003ehttps://arxiv.org/pdf/1412.7647\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e[17] \u003ca href=\"https://www.hillelwayne.com/post/complexity-constraints/\"\u003ehttps://www.hillelwayne.com/post/complexity-constraints/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e[18] \u003ca href=\"https://en.wikipedia.org/wiki/Npm_left-pad_incident\"\u003ehttps://en.wikipedia.org/wiki/Npm_left-pad_incident\u003c/a\u003e\u003c/p\u003e\n\u003ch1 id=\"appendix-three-hard-problems\"\u003eAppendix: Three Hard Problems \u003ca href=\"#appendix-three-hard-problems\" class=\"hash\"\u003e#\u003c/a\u003e\u003c/h1\u003e\n\u003cp\u003eI\u0026rsquo;ve spent a lot of words talking about a problem and tying it to theory and predictions. What does taming complexity look like? How do we roll the ball down\nthe peak of the hill?\u003c/p\u003e\n\u003cp\u003eAt the risk of being tongue-in-cheek,\nI\u0026rsquo;ll start by saying I think the solution\nwill involve solving the \u0026ldquo;three hardest problems\u0026rdquo; in computer science.\u003c/p\u003e\n\u003ch3 id=\"naming\"\u003eNaming \u003ca href=\"#naming\" class=\"hash\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eNaming standards solve the literal naming problem. Today, standards\nwork - they just suffer from enforcement and distribution problems. Because of this, the shapes of\nsoftware abstractions haven\u0026rsquo;t been globally standardised. It\u0026rsquo;s a problem famous\nenough to warrant its own \u003ca href=\"https://xkcd.com/927/\"\u003eobligatory XKCD\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eBecause of where we are on the software S-curve, I suspect the move from product to commodity will\nentail hiding the naming problem with higher-order abstractions. As people move\nup an abstraction layer, user-defined names will simply matter less.\u003c/p\u003e\n\u003ch3 id=\"caching\"\u003eCaching \u003ca href=\"#caching\" class=\"hash\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eWhat\u0026rsquo;s hard about caching isn\u0026rsquo;t maintaining caching\ninfrastructure, but rather defining the correct cache key and invalidation\npolicy. This isn\u0026rsquo;t impossible - it again just requires precise and\nwell-defined behaviour. The CPU caches are a great success story here - they\njust needed maturity in the CPU/memory interface.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eWith cacheability, you get reproducibility, determinism, and more general\nfungibility of building blocks. With fungibility of building blocks - you\nget commodity-like properties!\u003c/li\u003e\n\u003cli\u003eDeclarative specifications of components are closely tied to cacheability -\na statement which traces the entire history of software infrastructure\ngrowth and commoditisation (Kubernetes, Docker, Terraform, Nix, Bazel etc).\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"off-by-one-errors\"\u003eOff by one errors \u003ca href=\"#off-by-one-errors\" class=\"hash\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eThese simply represent human hallucinations. Execution\ndriven feedback. i.e unit test cases encapsulating abstractions \u0026ldquo;solve\u0026rdquo; this well.\u003c/p\u003e\n\u003ch3 id=\"prove-it\"\u003eProve it? \u003ca href=\"#prove-it\" class=\"hash\"\u003e#\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eI won\u0026rsquo;t elaborate too much on \u0026ldquo;solutions\u0026rdquo; since I have various\nhypotheses-in-testing that need iteration (as opposed to more ideation).\u003c/p\u003e\n\u003cp\u003eIf you made it this far, clearly some of this was interesting to you. Let\u0026rsquo;s chat! \u003ca href=\"mailto:rohangupta883@gmail.com\"\u003eEmail\u003c/a\u003e \u003ca href=\"https://x.com/rohangupta_\"\u003eX\u003c/a\u003e\u003c/p\u003e\n","url":"https://grohan.co/2024/12/27/entropy/","image":"https://grohan.cophotos/<no value>","banner_image":"https://grohan.cophotos/<no value>","date_published":"27126-27-09T120:2727:00+00:00","date_modified":"27126-27-09T120:2727:00+00:00","author":{"name":"Ronalds Vilcins","url":"https://ronaldsvilcins.com/"}},{"id":"46ef1ef941af4eccb00a5fe986a02f0b51a08a78","title":"Factorials \u0026 Fun with Vim","summary":"Vim trix and more","content_text":"I recently hit a somewhat important milestone in my life: one year as a Vim user. Despite its steep learning curve, I used to think that mastering Vim was mostly about habit formation. I swore by the cheatsheets and believed I would be golden. After all, a text editor couldn\u0026rsquo;t be too terribly complex, right?\nHowever, I soon realised how little I knew about Vim and how much there really was to know! In writing this post, I\u0026rsquo;d like to demonstrate (by way of example) some of the features of Vim that I found interesting; this is by no means a comprehensive guide to Vim\u0026rsquo;s capabilities (I would be lying if I said I knew half of what Vim has to offer), but perhaps a newfound taste of power for the green, fresh-off-VS Code individual looking to dabble with the dark arts.\nComputing Factorials # This post has a simple goal: computing the factorial. A classic exercise for any new programmer \u0026ndash; simple to understand, but broad in terms of concepts covered. More formally, we\u0026rsquo;ll define our problem as writing a Vim routine that takes an input (ex. 5!) and spits out the factorial (120 in our example).\nAs a preface, I\u0026rsquo;ll be referencing a variety of Vim features in this post, but I\u0026rsquo;m explicitly choosing not to explain their usage in detail. Instead, I\u0026rsquo;ve tried to link resources that explain syntax and functionality better than I can. This is not a tutorial as much as it is a showcase.\nState Transitions and Math # To break our problem down a bit, we need a way to multiply (and decrement) numbers in Vim, store the results somewhere, and then repeat that process N times. Sounds simple enough?\nThankfully, we don\u0026rsquo;t have to implement multiplication from scratch. Vim\u0026rsquo;s got us covered here with the expression register, noted by \u0026ldquo;=\u0026rdquo;. You can access this in insert mode using \u0026lt;C-r\u0026gt;= and enter any expression you\u0026rsquo;d like. The expression register is clever enough to do basic math operations (addition, subtraction, multiplication, division), but not the factorial \u0026ndash; unfortunately our life isn\u0026rsquo;t that easy.\nAs an example, you could enter insert mode and input \u0026lt;C-r\u0026gt;=10*7 and the text under your cursor will be filled with 70, neat enough for simple calculations on the fly.\nSo we\u0026rsquo;ve got a way to multiply numbers, now we need to do this recursively as well as store intermediate state somewhere. Let\u0026rsquo;s tackle the state problem first; for the sake of demonstration, we\u0026rsquo;ll do the most naive thing I can think of to store our intermediate state: write to text.\nConsider a program with the following states\n7! 6!7 5!42 4!210 ... 5040 The contents after the ! store the intermediate state of the program, while leaving the contents before valid for recursion. We now effectively have program memory equal to the size of the text buffer we\u0026rsquo;re working with!\nWith our states defined, we\u0026rsquo;ve now got to think about how exactly we transition between these states. This is where we can introduce the Vim substitute command, which matches against patterns and replaces them according to a set of rules.\nThe command below matches on a number a followed by ! and replaces it with (a-1)!a. This is our base case for the recursion. We use the . operator to concatenate our expressions (using the expression register) together. We\u0026rsquo;re also able to use submatch(0) which simply matches against everything captured.\n:s/\\d\\+!/\\=submatch(0) - 1 . \u0026#34;!\u0026#34; . submatch(0)/ Next, we need our \u0026ldquo;recursive case\u0026rdquo;, matching on a!b and replacing with a-1!b*a. Note the introduction of capturing groups below! Previously, we only matched on one set of characters but now we have 2 distinct operations, which is also why we use indices 1 and 2 for submatch.\n:s/\\(\\d\\+\\)!\\(\\d\\+\\)/\\=submatch(1) - 1 . \u0026#34;!\u0026#34; . submatch(1) \\* submatch(2)/ Fantastic! This will take 6!7 and map it to 5!42, and so on.\nOrchestration # We\u0026rsquo;ve got all the state transformations we need now, but we still need a way to somehow orchestrate them together in a single command. This includes finding a way to programmatically recurse on our substitution expression a variable number of times. Sounds like a lot, but Vim\u0026rsquo;s again got us covered with an idiomatic tool: the macro, which is essentially a way to record sequences of edits and apply them in one shot.\nOur script now looks like this (we record the macro in the @b register):\nV :s/\\d\\+!/\\=submatch(0) - 1 . \u0026#34;!\u0026#34; . submatch(0)/ qb V :s/\\(\\d\\+\\)!\\(\\d\\+\\)/\\=submatch(1) - 1 . \u0026#34;!\u0026#34; . submatch(1) \\* submatch(2)/ q Nearly done, now all we need is a way to find the number of times to execute the macro. Until now, we\u0026rsquo;ve been on a purist edit-only streak, but let\u0026rsquo;s instead introduce some real programming concepts (variables!?).\nlet i = matchstr(getline(\u0026#39;.\u0026#39;), \u0026#39;\\d\\+\u0026#39;) execute \u0026#34;normal! \u0026#34; . i . \u0026#34;@b\u0026#34; These two lines will execute our macro i times, where i is simply the value of the number we want to compute the factorial of.\nFinally, this will leave us with 1!5040 (in the case of 7!). We need to perform just a bit of cleanup, since we have some extra information in the line (corresponding to our state), which we can do with the following substitution logic:\n:s/.\\*!\\(\\d\\+\\)/\\1/ or alternatively, using globals (note the use of the Vim exclusive look-ahead \\ze)\n:g/.\\*!\\ze/normal! df! And that\u0026rsquo;s it! Combining all of these expressions together gives us a general factorial machine.\nNote: I\u0026rsquo;m certainly aware of neater/shorter/better ways to do the same thing but I wanted to go through as many different \u0026ldquo;Vim trix\u0026rdquo; as I could in a post. If you\u0026rsquo;re looking to optimise for neatness and flex your Vim muscles, you might want to check out Vim Golf. I\u0026rsquo;m convinced some of the folks on there are not human.\nMeans to an End # The purpose of this post is not to encourage the reader to leave their trusted calculator behind in favour of Vim, but instead to view a multi-stage programming problem under the lens of powerful, expressive edits.\nVim, like many other programming tools, is primarily a means to an end. It simply happens to be powerful enough to do almost anything under the sun (if you are determined and/or crazy enough).\nWhat\u0026rsquo;s also remarkable to me is how Vim (circa 1991) has squarely stood its ground in the everchanging landscape of software development tools. Perhaps this is a testament to the timeless quality of design decisions made \u0026ndash; a seemingly rare feat in the fast-paced modern world of software. There\u0026rsquo;s a long way to go before I consider myself an expert at Vim, and I\u0026rsquo;m not sure if there truly is an end in sight; but perhaps that\u0026rsquo;s what makes it a fun tool to use?\nP.S.: I write fondly about Vim but I should mention that I\u0026rsquo;m actually an Emacs user. Plot twist, I know. My editor of choice is Doom Emacs which is a smooth, Space-centric Emacs config with Vim emulation. Why fight over Emacs vs Vim when you can have both?\nFurther Reading # I realise as I write this post that a lot of what I\u0026rsquo;ve written about involves Vim regex. Here\u0026rsquo;s an article I found online that does a good job explaining it.\nI\u0026rsquo;m often reminded of this legendary answer which captures the essence of what I think makes Vim so enduringly good: powerful expressivity combined with a simplistic grammar.\nLastly, here is a well-written epitaph to Bram Moolenar (the creator of Vim).\n","content_html":"\u003cp\u003eI recently hit a somewhat important milestone in my life: one year as a Vim\nuser. Despite its steep learning curve, I used to think that mastering Vim\nwas mostly about habit formation. I swore by the\ncheatsheets and believed I would be golden. After all, a\ntext editor couldn\u0026rsquo;t be too terribly complex, right?\u003c/p\u003e\n\u003cp\u003eHowever, I soon realised how \u003cem\u003elittle\u003c/em\u003e I knew about Vim and how much there\nreally was to know! In writing this post, I\u0026rsquo;d like to demonstrate (by\nway of example) some of\nthe features of Vim that I found interesting; this is by no means a\ncomprehensive guide to Vim\u0026rsquo;s capabilities (I would be lying if I said I knew half of what Vim\nhas to offer), but perhaps a newfound \u003ca href=\"https://xkcd.com/378/\"\u003etaste of power\u003c/a\u003e\nfor the green, fresh-off-VS Code individual looking to dabble with the dark arts.\u003c/p\u003e\n\u003ch1 id=\"computing-factorials\"\u003eComputing Factorials \u003ca href=\"#computing-factorials\" class=\"hash\"\u003e#\u003c/a\u003e\u003c/h1\u003e\n\u003cp\u003eThis post has a simple goal: computing the factorial. A classic exercise for any\nnew programmer \u0026ndash; simple to understand, but broad in terms of concepts covered.\nMore formally, we\u0026rsquo;ll define our problem as writing a Vim routine that takes an input (ex. 5!) and spits out the factorial (120 in our example).\u003c/p\u003e\n\u003cp\u003eAs a preface, I\u0026rsquo;ll be referencing a variety of Vim features in this post, but I\u0026rsquo;m explicitly choosing not to explain their usage in detail. Instead, I\u0026rsquo;ve tried to link resources that explain syntax and functionality better than I can. This is not a tutorial as much as it is a showcase.\u003c/p\u003e\n\u003ch2 id=\"state-transitions-and-math\"\u003eState Transitions and Math \u003ca href=\"#state-transitions-and-math\" class=\"hash\"\u003e#\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eTo break our problem down a bit, we need a way to multiply (and decrement) numbers in Vim, store the results somewhere, and then repeat that process N times. Sounds simple enough?\u003c/p\u003e\n\u003cp\u003eThankfully, we don\u0026rsquo;t have to implement multiplication from scratch. Vim\u0026rsquo;s got us covered here with the \u003ca href=\"https://stackoverflow.com/questions/7027741/what-is-the-purpose-of-the-expression-register\"\u003eexpression register\u003c/a\u003e, noted by \u0026ldquo;=\u0026rdquo;. You can access this in insert mode using \u003ccode\u003e\u0026lt;C-r\u0026gt;=\u003c/code\u003e and enter any expression you\u0026rsquo;d like. The expression register is clever enough to do basic math operations (addition, subtraction, multiplication, division), but not the factorial \u0026ndash; unfortunately\nour life isn\u0026rsquo;t \u003cem\u003ethat\u003c/em\u003e easy.\u003c/p\u003e\n\u003cp\u003eAs an example, you could enter insert mode and input \u003ccode\u003e\u0026lt;C-r\u0026gt;=10*7\u003c/code\u003e and the text under your cursor will be filled with 70, neat enough for simple calculations on the fly.\u003c/p\u003e\n\u003cp\u003eSo we\u0026rsquo;ve got a way to multiply numbers, now we need to do this recursively as\nwell as store intermediate state somewhere. Let\u0026rsquo;s tackle the state problem\nfirst; for the sake of demonstration, we\u0026rsquo;ll do the most naive thing I can think\nof to store our intermediate state: write to text.\u003c/p\u003e\n\u003cp\u003eConsider a program with the following states\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-fallback\" data-lang=\"fallback\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e7!\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e6!7\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e5!42\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e4!210\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e...\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e5040\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThe contents after the ! store the intermediate state of the program, while leaving the contents before valid for recursion. We now effectively have program memory equal to the size of the text buffer we\u0026rsquo;re working with!\u003c/p\u003e\n\u003cp\u003eWith our states defined, we\u0026rsquo;ve now got to think about how exactly we transition between these states. This is where we can introduce the Vim \u003ca href=\"https://vim.fandom.com/wiki/Search_and_replace\"\u003esubstitute\u003c/a\u003e command, which matches against patterns and replaces them according to a set of rules.\u003c/p\u003e\n\u003cp\u003eThe command below matches on a number \u003ccode\u003ea\u003c/code\u003e followed by ! and replaces it with \u003ccode\u003e(a-1)!a\u003c/code\u003e. This is our base case for the recursion. We use the \u003ccode\u003e.\u003c/code\u003e operator to concatenate our expressions (using the expression register) together. We\u0026rsquo;re also able to use \u003ccode\u003esubmatch(0)\u003c/code\u003e which simply matches against everything captured.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-fallback\" data-lang=\"fallback\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e:s/\\d\\+!/\\=submatch(0) - 1 . \u0026#34;!\u0026#34; . submatch(0)/\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eNext, we need our \u0026ldquo;recursive case\u0026rdquo;, matching on \u003ccode\u003ea!b\u003c/code\u003e and replacing with \u003ccode\u003ea-1!b*a\u003c/code\u003e. Note the introduction of capturing groups below! Previously, we only matched on one set of characters but now we have 2 distinct operations, which is also why we use indices 1 and 2 for \u003ccode\u003esubmatch\u003c/code\u003e.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-fallback\" data-lang=\"fallback\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e:s/\\(\\d\\+\\)!\\(\\d\\+\\)/\\=submatch(1) - 1 . \u0026#34;!\u0026#34; . submatch(1) \\* submatch(2)/\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eFantastic! This will take 6!7 and map it to 5!42, and so on.\u003c/p\u003e\n\u003ch2 id=\"orchestration\"\u003eOrchestration \u003ca href=\"#orchestration\" class=\"hash\"\u003e#\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eWe\u0026rsquo;ve got all the state transformations we need now, but we still need a way to somehow orchestrate them together in a single command. This includes finding a way to programmatically recurse on our substitution expression a variable number of times. Sounds like a lot, but Vim\u0026rsquo;s again got us covered with an idiomatic tool: \u003ca href=\"https://vim.fandom.com/wiki/Macros\"\u003ethe macro\u003c/a\u003e, which is essentially a way to record sequences of edits and apply them in one shot.\u003c/p\u003e\n\u003cp\u003eOur script now looks like this (we record the macro in the \u003ccode\u003e@b\u003c/code\u003e register):\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-fallback\" data-lang=\"fallback\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eV\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e:s/\\d\\+!/\\=submatch(0) - 1 . \u0026#34;!\u0026#34; . submatch(0)/\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eqb\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eV\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e:s/\\(\\d\\+\\)!\\(\\d\\+\\)/\\=submatch(1) - 1 . \u0026#34;!\u0026#34; . submatch(1) \\* submatch(2)/\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eq\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eNearly done, now all we need is a way to find the number of times to execute the macro. Until now, we\u0026rsquo;ve been on a purist edit-only streak, but let\u0026rsquo;s instead introduce some \u003cem\u003ereal\u003c/em\u003e programming concepts (variables!?).\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-fallback\" data-lang=\"fallback\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003elet i = matchstr(getline(\u0026#39;.\u0026#39;), \u0026#39;\\d\\+\u0026#39;)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eexecute \u0026#34;normal! \u0026#34; . i . \u0026#34;@b\u0026#34;\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThese two lines will execute our macro \u003ccode\u003ei\u003c/code\u003e times, where \u003ccode\u003ei\u003c/code\u003e is simply the value of the number we want to compute the factorial of.\u003c/p\u003e\n\u003cp\u003eFinally, this will leave us with 1!5040 (in the case of 7!). We need to perform just a bit of cleanup, since we have some extra information in the line (corresponding to our state), which we can do with the following substitution logic:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-fallback\" data-lang=\"fallback\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e:s/.\\*!\\(\\d\\+\\)/\\1/\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eor alternatively, using \u003ca href=\"https://vim.fandom.com/wiki/Power_of_g\"\u003eglobals\u003c/a\u003e (note the use of the Vim exclusive look-ahead \u003ccode\u003e\\ze\u003c/code\u003e)\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-fallback\" data-lang=\"fallback\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e:g/.\\*!\\ze/normal! df!\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eAnd that\u0026rsquo;s it! Combining all of these expressions together gives us a general factorial machine.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eNote\u003c/strong\u003e: I\u0026rsquo;m certainly aware of neater/shorter/better ways to do the same thing\nbut I wanted to go through as many different \u0026ldquo;Vim trix\u0026rdquo; as I could in a post. If\nyou\u0026rsquo;re looking to optimise for neatness and flex your Vim muscles, you might want to check out \u003ca href=\"https://vimgolf.com\"\u003eVim Golf\u003c/a\u003e. I\u0026rsquo;m convinced some of the folks on there are not human.\u003c/p\u003e\n\u003ch2 id=\"means-to-an-end\"\u003eMeans to an End \u003ca href=\"#means-to-an-end\" class=\"hash\"\u003e#\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eThe purpose of this post is not to encourage the reader to leave\ntheir trusted calculator behind in favour of Vim, but instead to view a\nmulti-stage programming problem under the lens of powerful, expressive edits.\u003c/p\u003e\n\u003cp\u003eVim, like many other programming tools, is primarily a means to an end. It\nsimply happens to be powerful enough to do almost anything under the sun (if you are determined and/or\ncrazy enough).\u003c/p\u003e\n\u003cp\u003eWhat\u0026rsquo;s also remarkable to me is how Vim (circa 1991) has squarely stood its ground in the everchanging landscape of software development tools. Perhaps this is a testament to the timeless quality of design\ndecisions made \u0026ndash; a seemingly rare feat in the fast-paced modern world of\nsoftware. There\u0026rsquo;s a long way to go before I consider myself an expert at Vim, and I\u0026rsquo;m not\nsure if there truly is an end in sight; but perhaps that\u0026rsquo;s what makes it a fun\ntool to use?\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eP.S.\u003c/em\u003e: I write fondly about Vim but I should mention that I\u0026rsquo;m actually\nan Emacs user. Plot twist, I know. My editor of choice is \u003ca href=\"https://github.com/doomemacs/doomemacs\"\u003eDoom Emacs\u003c/a\u003e which\nis a smooth, Space-centric Emacs config with Vim emulation. Why fight over\nEmacs vs Vim when you can have both?\u003c/p\u003e\n\u003ch2 id=\"further-reading\"\u003eFurther Reading \u003ca href=\"#further-reading\" class=\"hash\"\u003e#\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eI realise as I write this post that a lot of what I\u0026rsquo;ve written about involves Vim regex.\nHere\u0026rsquo;s an \u003ca href=\"https://dev.to/iggredible/learning-vim-regex-26ep\"\u003earticle\u003c/a\u003e I found\nonline that does a good job explaining it.\u003c/p\u003e\n\u003cp\u003eI\u0026rsquo;m often reminded of \u003ca href=\"https://stackoverflow.com/questions/1218390/what-is-your-most-productive-shortcut-with-vim/1220118#1220118\"\u003ethis legendary\nanswer\u003c/a\u003e\nwhich captures the essence of what I think makes Vim so enduringly good:\npowerful expressivity combined with a simplistic grammar.\u003c/p\u003e\n\u003cp\u003eLastly, here is a \u003ca href=\"https://j11g.com/2023/08/07/the-legacy-of-bram-moolenaar/\"\u003ewell-written epitaph\u003c/a\u003e to Bram Moolenar (the creator of Vim).\u003c/p\u003e\n","url":"https://grohan.co/2023/08/10/vim/","image":"https://grohan.cophotos/<no value>","banner_image":"https://grohan.cophotos/<no value>","date_published":"10086-10-09T80:1010:00+00:00","date_modified":"10086-10-09T80:1010:00+00:00","author":{"name":"Ronalds Vilcins","url":"https://ronaldsvilcins.com/"}},{"id":"a77c2c5d4d187c889fa2442bb6b1bd728e96248a","title":"Halfway There","summary":"Reflection on first 2 years of college","content_text":"College. Times of fun, frolic, and of course, learning. For a couple days now, I\u0026rsquo;ve been thinking about the past 2 years of my life, focusing on the most meaningful aspects. I needed a place to write my reflections down, so I thought perhaps this would make for a fitting first blog post. So here\u0026rsquo;s a couple shoddily jotted down thoughts on what I think have been my most significant takeaways from school, so far.\nSchool still sucks, but less # I used to absolutely despise highschool. Part of it was because I was a lazy slob who wanted to play RuneScape every waking hour but mostly because I didn\u0026rsquo;t like what I was meant to do in school in order to do well. For example, the Computer Science curriculum tested us on some pretty jank stuff. We\u0026rsquo;re talking definitions, case studies, and the most basic OOP. I\u0026rsquo;d spend hours the morning of the exam rote learning the names of 7 layers of the OSI model, but not what they actually were useful for.\nWhen I got to college, I was hoping that I would be, for the most part, doing things that I enjoyed, and those things would directly correlate to how well I was doing. This turned out to be initially true! I felt pretty academically satisfied after my first year of school, but (to my unfortunate realization) it just happened to be that intro classes at Penn are run especially well. Not only do they teach you real CS fundamentals, but they also have fun, knowledgeable folks on staff and run a great curriculum that kept me engaged with the material. (aside: I was pleasantly surprised to learn early freshman year that CIS 1xx staff are pretty tight-knit. They even do BYOs \u0026ndash; I know, not what you\u0026rsquo;d think when I mention CS course staff)\nAs I progressed to some of the upper division classes, I was still learning interesting things, but it felt like I was increasingly falling into the habit of autodidactism. For example, I took CIS 521: Artificial Intelligence sophomore fall. The class was, by all metrics, good, but there was no sense of community or engagement as in the intro classes. Crucially, it could be done async and had annoying assignments. The in-person equivalent just didn\u0026rsquo;t seem valuable enough to go to. I could just Google any questions I had and, well, kind of figure things out as I go. This left me thinking \u0026ndash; if I really cared about the material, I could have just as easily put my head down and grinded out the class (which was basically a bunch of videos) in a couple weeks, skipping over the poorly designed bits. I ended up attending zero (0) live lectures for the class \u0026ndash; I don\u0026rsquo;t even know what room it was in.\nIs that the way I like learning? I\u0026rsquo;m not sure. But one thing is for sure: it\u0026rsquo;s convenient. Since Fall 2021, I\u0026rsquo;ve been going to almost no classes, because it is simply more convenient to learn material at home, at my own pace. There\u0026rsquo;s probably some joke out there about 1.5x lectures being the new norm, but at this point it\u0026rsquo;s reality. I have been living on 1.5x and lecture notes.\nWhile the convenience is certainly nice, it\u0026rsquo;s unfulfilling. I had hoped college would be more of the interactive, community-oriented experience I had in my intro classes. For the few upper division classes that do offer this, great. But for the others, if someone were to download the Canvas page and send it over to me to cherry-pick modules, I think I\u0026rsquo;d have a better time than having to go through administrative nonsense, annoying homeworks, and even more annoying exams \u0026ndash; I\u0026rsquo;m taking most of this upper level stuff for fun anyways.\nFor the most part, school isn\u0026rsquo;t the one \u0026ldquo;teaching\u0026rdquo; me, I\u0026rsquo;m learning things myself. But with the added overhead of whatever the course staff decides is going to make up my grade. Let me do this stuff on my own, or run classes better and make people want to show up!\nI actually like what I do now # I don\u0026rsquo;t like the question: what are your hobbies? A hobby sounds far too casual for me. I\u0026rsquo;m the type of person who finds one thing and then does it obsessively. In highschool, I played a lot of RuneScape. Like a lot. ~300 full days worth of RuneScape. You can do the math yourself, but that\u0026rsquo;s a large number of hours. (fun fact: I only told my mother this number after I got into college, she still wasn\u0026rsquo;t amused). RuneScape was, truly, what I did \u0026ndash; the thing I would think about on the hour long bus ride back from school, and often sneak late at night to do instead of whatever homework I had due.\nBut it wasn\u0026rsquo;t the most practical or interesting (to most people). Indeed, it\u0026rsquo;s pretty difficult to sit down at dinner with someone and explain why a game that\u0026rsquo;s older than I am and not very well known was so captivating to me. So, well, if someone asked me my hobbies were or what I did, I\u0026rsquo;d look them in the eye, lie through my teeth, and say that I was into Computer Science. But, at the time (in highschool) that wasn\u0026rsquo;t really true.\nFor context, my first legitimate introduction to real-world programming was the summer after sophomore year. I was unquestioningly forced to go make use of myself at my mother\u0026rsquo;s hospital\u0026rsquo;s IT department, where I was tasked with building some nonsense webapp using everyone\u0026rsquo;s favourite web framework: Django. I didn\u0026rsquo;t know the first thing about web programming so I began prowling through Django documentation. I didn\u0026rsquo;t really understand everything, but I understood enough to combine with code from GitHub and StackOverFlow and piece together something that just about worked.\nThe problem was: I didn\u0026rsquo;t really care about understanding more, or learning about the things I was working with. For example, I didn\u0026rsquo;t know what HTTP was, just that you could do a bunch of operations with it, like GET and POST, and those operations did exactly what you\u0026rsquo;d think they did. Instead, I wanted to play RuneScape, or laze around, or watch some show, or eat Indian McDonald\u0026rsquo;s Mexican Cheesy Fries (I miss this a lot). Coding was alright, I could do it if I was really asked to, and I wasn\u0026rsquo;t terrible at it, but it didn\u0026rsquo;t really make me tick.\nFlash-forward to freshman fall at Penn. I come in as a Cognitive Science major. I mean, brains seemed cool. I half-assed a Coursera course on ML sometime senior year of highschool, so AI also seemed cool. I\u0026rsquo;d also worked with computers. It seemed natural to study Cognitive Science, but I wasn\u0026rsquo;t the guy with a plan. On a whim, I saw that Penn Labs used Django, and it seemed somewhat interesting, so I thought I\u0026rsquo;d give it a shot. By some miracle, things work out and I get in. In a week\u0026rsquo;s time, I realize that I\u0026rsquo;ve been lying to myself.\nEveryone at the club seemed so genuinely passionate about computers, it was nuts \u0026ndash; they actually cared about how things worked. The folks I worked with actually knew what HTTP was, and that was the least of the things they knew about. Strange words like DevOps, Docker, Kubernetes would float around in Slack, and I\u0026rsquo;d have no idea what they meant. Debugging wasn\u0026rsquo;t just Googling and copypasting on loop, but actually trying things meaningfully. I felt pretty out of place \u0026ndash; a small fish in a big pond \u0026ndash; but I liked it!\nNot only was I learning a lot, but the enthusiasm for programming was rubbing off on me. Experiencing this alongside the intro CS classes I talked about above, I actually began to enjoy computer science. I found myself Googling random things about Django in my free time, searching up about tools like pipenv, and doing things that the younger me would never have done. I was experiencing a shift in the thing I did: from RuneScape to coding.\nThings end up panning out after: I transfer to NETS, get more involved with Labs, learn about those strange words I mentioned above through CIS 188, and generally immerse myself in the programming world. However, looking back, it\u0026rsquo;s funny how things work out sometimes \u0026ndash; I\u0026rsquo;ve often reflected on the fact that the 10 person IT department in a hospital in Bangalore, India just happened to use Django as their backend, which is perhaps the only reason I thought I\u0026rsquo;d have a shot at Penn Labs, and consequently end up growing as much as I did.\nTo conclude, I\u0026rsquo;d like to note that the title is a little misleading, I\u0026rsquo;ve always liked what I really did, but I definitely did fake it for a bit. Now, when I\u0026rsquo;m at a dinner, and I\u0026rsquo;m prompted about my hobbies, it\u0026rsquo;s a little more gratifying to say that I like programming \u0026ndash; because I know it is for real.\n","content_html":"\u003cp\u003eCollege. Times of fun, frolic, and of course, learning. For a couple days now,\nI\u0026rsquo;ve been thinking about the past 2 years of my life, focusing on the most\nmeaningful aspects. I needed a place to write my reflections down, so\nI thought perhaps this would make for a fitting first blog post. So here\u0026rsquo;s a\ncouple shoddily jotted down thoughts on what I think have been my most\nsignificant takeaways from school, so far.\u003c/p\u003e\n\u003ch1 id=\"school-still-sucks-but-less\"\u003eSchool still sucks, but less \u003ca href=\"#school-still-sucks-but-less\" class=\"hash\"\u003e#\u003c/a\u003e\u003c/h1\u003e\n\u003cp\u003eI used to absolutely despise highschool. Part of it was because I was a lazy\nslob who wanted to play RuneScape every waking hour but mostly because I didn\u0026rsquo;t\nlike what I was meant to do in school in order to do well. For example, the\nComputer Science curriculum tested us on some pretty jank stuff. We\u0026rsquo;re talking\ndefinitions, case studies, and the most basic OOP. I\u0026rsquo;d spend hours the morning\nof the exam rote learning the names of 7 layers of the OSI model, but not what\nthey \u003cem\u003eactually\u003c/em\u003e were useful for.\u003c/p\u003e\n\u003cp\u003eWhen I got to college, I was hoping that I would be, for the most part, doing\nthings that I enjoyed, and those things would directly correlate to how well I\nwas doing. This turned out to be initially true! I felt pretty academically\nsatisfied after my first year of school, but (to my unfortunate realization) it\njust happened to be that intro classes at Penn are run especially well. Not only\ndo they teach you real CS fundamentals, but they also have fun, knowledgeable\nfolks on staff and run a great curriculum that kept me engaged with the\nmaterial. (aside: I was pleasantly surprised to learn early freshman year that\nCIS 1xx staff are pretty tight-knit. They even do BYOs \u0026ndash; I know, not what you\u0026rsquo;d\nthink when I mention CS course staff)\u003c/p\u003e\n\u003cp\u003eAs I progressed to some of the upper division classes, I was still learning\ninteresting things, but it felt like I was increasingly falling into the habit\nof autodidactism. For example, I took \u003cstrong\u003eCIS 521: Artificial Intelligence\u003c/strong\u003e\nsophomore fall. The class was, by all metrics, good, but there was no sense of\ncommunity or engagement as in the intro classes. Crucially, it \u003cem\u003ecould be done\nasync and had annoying assignments\u003c/em\u003e. The in-person equivalent just didn\u0026rsquo;t seem\nvaluable enough to go to. I could just Google any questions I had and, well,\nkind of figure things out as I go. This left me thinking \u0026ndash; if I really cared\nabout the material, I could have just as easily put my head down and grinded out\nthe class (which was basically a bunch of videos) in a couple weeks, skipping\nover the poorly designed bits. I ended up attending zero (0) live lectures for\nthe class \u0026ndash; I don\u0026rsquo;t even know what room it was in.\u003c/p\u003e\n\u003cp\u003eIs that the way I like learning? I\u0026rsquo;m not sure. But one thing is for sure: it\u0026rsquo;s\nconvenient. Since Fall 2021, I\u0026rsquo;ve been going to almost no classes, because it is\nsimply more convenient to learn material at home, at my own pace. There\u0026rsquo;s\nprobably some joke out there about 1.5x lectures being the new norm, but at this\npoint it\u0026rsquo;s reality. I have been \u003cem\u003eliving\u003c/em\u003e on 1.5x and lecture notes.\u003c/p\u003e\n\u003cp\u003eWhile the convenience is certainly nice, it\u0026rsquo;s unfulfilling. I had hoped college\nwould be more of the interactive, community-oriented experience I had in my\nintro classes. For the few upper division classes that do offer this, great. But\nfor the others, if someone were to download the Canvas page and send it over to\nme to cherry-pick modules, I think I\u0026rsquo;d have a better time than having to go\nthrough administrative nonsense, annoying homeworks, and even more annoying\nexams \u0026ndash; I\u0026rsquo;m taking most of this upper level stuff for fun anyways.\u003c/p\u003e\n\u003cp\u003eFor the most part, school isn\u0026rsquo;t the one \u0026ldquo;teaching\u0026rdquo; me, I\u0026rsquo;m learning things\nmyself. But with the added overhead of whatever the course staff decides is\ngoing to make up my grade. Let me do this stuff on my own, or run classes better\nand make people want to show up!\u003c/p\u003e\n\u003c!--- markdown auto fill mode ---\u003e\n\u003ch1 id=\"i-actually-like-what-i-_do_-now\"\u003eI actually like what I \u003cem\u003edo\u003c/em\u003e now \u003ca href=\"#i-actually-like-what-i-_do_-now\" class=\"hash\"\u003e#\u003c/a\u003e\u003c/h1\u003e\n\u003cp\u003eI don\u0026rsquo;t like the question: what are your hobbies? A hobby sounds far too casual\nfor me. I\u0026rsquo;m the type of person who finds one thing and then \u003cem\u003edoes\u003c/em\u003e it\nobsessively. In highschool, I played a lot of RuneScape. Like a lot. ~300 full\ndays worth of RuneScape. You can do the math yourself, but that\u0026rsquo;s a large number\nof hours. (fun fact: I only told my mother this number after I got\ninto college, she still wasn\u0026rsquo;t amused). RuneScape was, truly, what I \u003cem\u003edid\u003c/em\u003e \u0026ndash; the thing\nI would think about on the hour long bus ride back from school, and often sneak\nlate at night to do instead of whatever homework I had due.\u003c/p\u003e\n\u003cp\u003eBut it wasn\u0026rsquo;t the most practical or interesting (to most people). Indeed, it\u0026rsquo;s\npretty difficult to sit down at dinner with someone and explain why a game\nthat\u0026rsquo;s older than I am and not very well known was so captivating to me. So,\nwell, if someone asked me my hobbies were or what I \u003cem\u003edid\u003c/em\u003e, I\u0026rsquo;d look them in the\neye, lie through my teeth, and say that I was into Computer Science. But, at\nthe time (in highschool) that wasn\u0026rsquo;t really true.\u003c/p\u003e\n\u003cp\u003eFor context, my first legitimate introduction to real-world programming was the summer\nafter sophomore year. I was unquestioningly forced to go make use of myself at\nmy mother\u0026rsquo;s hospital\u0026rsquo;s IT department, where I was tasked with building some\nnonsense webapp using everyone\u0026rsquo;s favourite web framework: Django. I didn\u0026rsquo;t know\nthe first thing about web programming so I began prowling through Django\ndocumentation. I didn\u0026rsquo;t really understand everything, but I understood enough to\ncombine with code from GitHub and StackOverFlow and piece together something\nthat just about worked.\u003c/p\u003e\n\u003cp\u003eThe problem was: I didn\u0026rsquo;t really \u003cem\u003ecare\u003c/em\u003e about understanding more, or learning\nabout the things I was working with. For example, I didn\u0026rsquo;t know what \u003ccode\u003eHTTP\u003c/code\u003e was,\njust that you could do a bunch of operations with it, like \u003ccode\u003eGET\u003c/code\u003e and \u003ccode\u003ePOST\u003c/code\u003e, and\nthose operations did exactly what you\u0026rsquo;d think they did. Instead, I wanted to\nplay RuneScape, or laze around, or watch some show, or eat Indian McDonald\u0026rsquo;s\n\u003ca href=\"https://mcdonaldsblog.in/wp-content/uploads/2016/11/mexican-fries.jpg\"\u003eMexican Cheesy\nFries\u003c/a\u003e (I\nmiss this a lot). Coding was alright, I could do it if I was really asked to,\nand I wasn\u0026rsquo;t terrible at it, but it didn\u0026rsquo;t really make me tick.\u003c/p\u003e\n\u003cp\u003eFlash-forward to freshman fall at Penn. I come in as a Cognitive Science major.\nI mean, brains seemed cool. I half-assed a Coursera course on ML sometime senior\nyear of highschool, so AI also seemed cool. I\u0026rsquo;d also worked with computers. It\nseemed natural to study Cognitive Science, but I wasn\u0026rsquo;t the guy with a plan. On\na whim, I saw that \u003ca href=\"https://pennlabs.org\"\u003ePenn Labs\u003c/a\u003e used Django, and it seemed\nsomewhat interesting, so I thought I\u0026rsquo;d give it a shot. By some miracle, things\nwork out and I get in. In a week\u0026rsquo;s time, I realize that I\u0026rsquo;ve been lying to\nmyself.\u003c/p\u003e\n\u003cp\u003eEveryone at the club seemed so genuinely passionate about computers, it was nuts\n\u0026ndash; they actually cared about how things worked. The folks I worked with actually knew\nwhat \u003ccode\u003eHTTP\u003c/code\u003e was, and that was the least of the things they knew about. Strange\nwords like \u003ccode\u003eDevOps, Docker, Kubernetes\u003c/code\u003e would float around in Slack, and I\u0026rsquo;d\nhave no idea what they meant. Debugging wasn\u0026rsquo;t just Googling and copypasting on\nloop, but actually trying things meaningfully. I felt pretty out of place \u0026ndash; a\nsmall fish in a big pond \u0026ndash; but I liked it!\u003c/p\u003e\n\u003cp\u003eNot only was I learning a lot, but the enthusiasm for programming was rubbing\noff on me. Experiencing this alongside the intro CS classes I talked about\nabove, I actually began to enjoy computer science. I found myself Googling\nrandom things about Django in my free time, searching up about tools like\n\u003ccode\u003epipenv\u003c/code\u003e, and doing things that the younger me would never have done. I was\nexperiencing a shift in the thing I \u003cem\u003edid\u003c/em\u003e: from RuneScape to coding.\u003c/p\u003e\n\u003cp\u003eThings end up panning out after: I transfer to \u003ca href=\"https://www.nets.upenn.edu/\"\u003eNETS\u003c/a\u003e, get\nmore involved with Labs, learn about those strange words I mentioned above through\n\u003ca href=\"https://cis188.org/\"\u003eCIS 188\u003c/a\u003e, and generally immerse myself in the programming\nworld. However, looking back, it\u0026rsquo;s funny how things work out sometimes \u0026ndash; I\u0026rsquo;ve often reflected on the fact\nthat the 10 person IT department in a hospital in Bangalore, India just happened\nto use Django as their backend, which is perhaps the only reason I thought I\u0026rsquo;d\nhave a shot at Penn Labs, and consequently end up growing as much as I did.\u003c/p\u003e\n\u003cp\u003eTo conclude, I\u0026rsquo;d like to note that the title is a little misleading, I\u0026rsquo;ve always\nliked what I \u003cstrong\u003ereally\u003c/strong\u003e did, but I definitely did fake it for a bit. Now, when\nI\u0026rsquo;m at a dinner, and I\u0026rsquo;m prompted about my hobbies, it\u0026rsquo;s a little more\ngratifying to say that I like programming \u0026ndash; because I know it is for real.\u003c/p\u003e\n","url":"https://grohan.co/2022/06/15/halfway/","image":"https://grohan.cophotos/<no value>","banner_image":"https://grohan.cophotos/<no value>","date_published":"15066-15-09T60:1515:00+00:00","date_modified":"15066-15-09T60:1515:00+00:00","author":{"name":"Ronalds Vilcins","url":"https://ronaldsvilcins.com/"}}]}